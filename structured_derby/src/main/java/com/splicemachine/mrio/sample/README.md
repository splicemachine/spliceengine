Map Reduce Integration with Splice DB

#### Preparation
1. define table name that you want to retrieve data from (That table should exist in Splice Machine DB)
> String inputTableName = "WIKIDATA";<br>
  String outputTableName = "USERTEST"<br>
  //table names can be specified as schemaName.tableName<br>
  //if given the tableName as WIKIDATA, then it will use the default schema.

2. new instance of Job
> Configuration config = HBaseConfiguration.create();
> Job job = new Job(config, "MRTest2");

3. init mapper job
> TableMapReduceUtil.initTableMapperJob(
			    tableName, &nbsp;&nbsp;&nbsp;&nbsp;// input Splice table name<br>
			    scan, &nbsp;&nbsp;&nbsp;&nbsp;// Scan instance to control CF and attribute selection<br>
			    MyMapper.class, &nbsp;&nbsp;&nbsp;&nbsp;// mapper<br>
			    Text.class, &nbsp;&nbsp;&nbsp;&nbsp;// mapper output key<br>
			    IntWritable.class,  &nbsp;&nbsp;&nbsp;&nbsp;// mapper output value<br>
			    job,<br>
			    true,<br>
			    SpliceInputFormat.class);<br>

#### Retrieve value within map function
1. implement your map function
> public void map(ImmutableBytesWritable row, Result value, Context context)
{
&nbsp;&nbsp;&nbsp;&nbsp;<br>// use 'value' to retreive a single row. 
}

2. retreive and parse single row with specified columns
> DataValueDescriptor dvd[] = value.getRowArray();

3. print it out
> see the WordCount.decode();

#### Manipulate and Save value within reduce function
1. create ExecRow <br>
2. fill in the row with value: execRow.setRowArray(DataValueDescriptors...)<br>

#### Commit your job if it is successful
> job.commit()

#### Rollback your job if it fails
> job.rollback()

#### Run the example
1. make sure your SpliceDB is on.

2. put sample/mediawiki.dic.txt under some path. example: /home/ubuntu/mediawiki.dic.txt <br>

3. run ImportData.java
> args: <br>
  1. tableName
  2. create table statement
  3. inputFile path
  4. jdbc connection string<br>

> run the script, for example: <br>
  java -cp splice_machine-FUJI.0-SNAPSHOT-cloudera-cdh4.5.0_complete.jar&nbsp;com.splicemachine.mrio.sample.ImportData&nbsp;&nbsp;&nbsp;&nbsp;
  WIKIDATA &nbsp;&nbsp;&nbsp;&nbsp;'create table WIKIDATA(WORD varchar(100))'&nbsp;&nbsp;&nbsp;/home/ubuntu/mediawiki.dic.txt&nbsp;&nbsp;&nbsp;,&nbsp;&nbsp;&nbsp;'jdbc:splice://localhost:1527/splicedb;user=splice;password=admin'<br>
> this will load data in mediawiki.dic.txt into WIKIDATA table in Splice

4. run CreateOutputTable.java
> args:<br>
  1. tableName
  2. create table statement
  3. jdbc connection string<br>

> run the script, for example:<br>
  java -cp splice_machine-FUJI.0-SNAPSHOT-cloudera-cdh4.5.0_complete.jar com.splicemachine.mrio.sample.CreateOutputTable &nbsp;&nbsp;&nbsp;&nbsp;
  USERTEST&nbsp;&nbsp;&nbsp;&nbsp;'create table USERTEST (WORD varchar(100), COUNT integer)'&nbsp;&nbsp;&nbsp;&nbsp;'jdbc:derby://localhost:1527/splicedb;user=splice;password=adminâ€™<br>
> this will create the output table for reducer.

5. when the map-reduce finished running, you can checkout the result in Splice, data will be saved in the output table you created. 
