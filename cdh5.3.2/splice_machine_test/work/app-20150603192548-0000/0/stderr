15/06/03 19:25:53 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
15/06/03 19:26:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/06/03 19:26:04 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/03 19:26:04 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/03 19:26:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/03 19:26:05 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/06/03 19:26:05 INFO Remoting: Starting remoting
15/06/03 19:26:05 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.1.65:65274]
15/06/03 19:26:05 INFO util.Utils: Successfully started service 'driverPropsFetcher' on port 65274.
15/06/03 19:26:05 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/03 19:26:05 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/03 19:26:05 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/03 19:26:05 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/06/03 19:26:05 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/06/03 19:26:05 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/06/03 19:26:05 INFO Remoting: Starting remoting
15/06/03 19:26:05 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.1.65:65276]
15/06/03 19:26:05 INFO util.Utils: Successfully started service 'sparkExecutor' on port 65276.
15/06/03 19:26:05 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/06/03 19:26:05 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@10.0.1.65:65270/user/CoarseGrainedScheduler
15/06/03 19:26:05 INFO worker.WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@10.0.1.65:65051/user/Worker
15/06/03 19:26:05 INFO worker.WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@10.0.1.65:65051/user/Worker
15/06/03 19:26:05 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
15/06/03 19:26:05 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/03 19:26:05 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/03 19:26:05 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/03 19:26:05 INFO util.AkkaUtils: Connecting to MapOutputTracker: akka.tcp://sparkDriver@10.0.1.65:65270/user/MapOutputTracker
15/06/03 19:26:05 INFO util.AkkaUtils: Connecting to BlockManagerMaster: akka.tcp://sparkDriver@10.0.1.65:65270/user/BlockManagerMaster
15/06/03 19:26:05 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20150603192605-9cff
15/06/03 19:26:05 INFO storage.MemoryStore: MemoryStore started with capacity 706.6 MB
15/06/03 19:26:06 INFO netty.NettyBlockTransferService: Server created on 65279
15/06/03 19:26:06 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/06/03 19:26:06 INFO storage.BlockManagerMaster: Registered BlockManager
15/06/03 19:26:06 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@10.0.1.65:65270/user/HeartbeatReceiver
15/06/03 19:26:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
15/06/03 19:26:06 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/06/03 19:26:06 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
15/06/03 19:26:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
15/06/03 19:26:06 INFO storage.MemoryStore: ensureFreeSpace(8149) called with curMem=0, maxMem=740960501
15/06/03 19:26:06 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KB, free 706.6 MB)
15/06/03 19:26:06 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/06/03 19:26:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 220 ms
15/06/03 19:26:06 INFO storage.MemoryStore: ensureFreeSpace(18824) called with curMem=8149, maxMem=740960501
15/06/03 19:26:06 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.4 KB, free 706.6 MB)
15/06/03 19:26:07 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x3e3c4870 connecting to ZooKeeper ensemble=localhost:2181
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 02/24/2015 20:43 GMT
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:host.name=10.0.1.65
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:java.version=1.7.0_71
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.7.0_71.jdk/Contents/Home/jre
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/Users/jleach/.m2/repository/com/splicemachine/splice_machine_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_machine-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_si_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_si_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_si-cdh5.3.2/1.1.1-SNAPSHOT/splice_si-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_constants-cdh5.3.2/1.1.1-SNAPSHOT/splice_constants-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/googlecode/concurrentlinkedhashmap/concurrentlinkedhashmap-lru/1.4.2/concurrentlinkedhashmap-lru-1.4.2.jar:/Users/jleach/.m2/repository/joda-time/joda-time/2.3/joda-time-2.3.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-client/2.5.0-cdh5.3.2/hadoop-client-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.5.0-cdh5.3.2/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.5.0-cdh5.3.2/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.5.0-cdh5.3.2/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.5.0-cdh5.3.2/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.5.0-cdh5.3.2/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.5.0-cdh5.3.2/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.5.0-cdh5.3.2/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.5.0-cdh5.3.2/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.5.0-cdh5.3.2/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-aws/2.5.0-cdh5.3.2/hadoop-aws-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.2.3/jackson-databind-2.2.3.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.2.3/jackson-core-2.2.3.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.2.3/jackson-annotations-2.2.3.jar:/Users/jleach/.m2/repository/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-annotations/2.5.0-cdh5.3.2/hadoop-annotations-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-common/2.5.0-cdh5.3.2/hadoop-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/Users/jleach/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/jleach/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/jleach/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/Users/jleach/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/jleach/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/jleach/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/jleach/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/Users/jleach/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/Users/jleach/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/jleach/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/jleach/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/jleach/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/jleach/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/jleach/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/jleach/.m2/repository/org/apache/avro/avro/1.7.6-cdh5.3.2/avro-1.7.6-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/jleach/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-auth/2.5.0-cdh5.3.2/hadoop-auth-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/jleach/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/jleach/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/jleach/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/Users/jleach/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/jleach/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.5.0-cdh5.3.2/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-client/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-client-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-common/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-common-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/cloudera/htrace/htrace-core/2.04/htrace-core-2.04.jar:/Users/jleach/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-common/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-common-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-core/2.5.0-mr1-cdh5.3.2/hadoop-core-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/jleach/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-protocol/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-protocol-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-server/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-server-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-prefix-tree/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-prefix-tree-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop2-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/yammer/metrics/metrics-core/2.1.2/metrics-core-2.1.2.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/jleach/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.5.0-cdh5.3.2/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-testing-util/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-testing-util-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-server/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-server-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop2-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-minicluster/2.5.0-mr1-cdh5.3.2/hadoop-minicluster-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-test/2.5.0-mr1-cdh5.3.2/hadoop-test-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftplet-api/1.0.0/ftplet-api-1.0.0.jar:/Users/jleach/.m2/repository/org/apache/mina/mina-core/2.0.0-M5/mina-core-2.0.0-M5.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftpserver-core/1.0.0/ftpserver-core-1.0.0.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftpserver-deprecated/1.0.0-M2/ftpserver-deprecated-1.0.0-M2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-common/2.5.0-cdh5.3.2/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-beeline/0.13.1-cdh5.3.2/hive-beeline-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-metastore/0.13.1-cdh5.3.2/hive-metastore-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/jleach/.m2/repository/org/apache/derby/derby/10.10.1.1/derby-10.10.1.1.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/Users/jleach/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/jleach/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/Users/jleach/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/Users/jleach/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/Users/jleach/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/Users/jleach/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/Users/jleach/.m2/repository/org/apache/thrift/libthrift/0.9.0-cdh5-2/libthrift-0.9.0-cdh5-2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-cli/0.13.1-cdh5.3.2/hive-cli-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-serde/0.13.1-cdh5.3.2/hive-serde-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-exec/0.13.1-cdh5.3.2/hive-exec-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-ant/0.13.1-cdh5.3.2/hive-ant-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/Users/jleach/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/Users/jleach/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/jleach/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/Users/jleach/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/Users/jleach/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-common/0.13.1-cdh5.3.2/hive-common-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-contrib/0.13.1-cdh5.3.2/hive-contrib-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-hbase-handler/0.13.1-cdh5.3.2/hive-hbase-handler-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-hwi/0.13.1-cdh5.3.2/hive-hwi-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-jdbc/0.13.1-cdh5.3.2/hive-jdbc-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/jleach/.m2/repository/org/apache/httpcomponents/httpcore/4.2.5/httpcore-4.2.5.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-service/0.13.1-cdh5.3.2/hive-service-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/Users/jleach/.m2/repository/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar:/Users/jleach/.m2/repository/javax/mail/mail/1.4.1/mail-1.4.1.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar:/Users/jleach/.m2/repository/asm/asm-commons/3.1/asm-commons-3.1.jar:/Users/jleach/.m2/repository/asm/asm-tree/3.1/asm-tree-3.1.jar:/Users/jleach/.m2/repository/org/apache/thrift/libfb303/0.9.0/libfb303-0.9.0.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-shims/0.13.1-cdh5.3.2/hive-shims-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-common/0.13.1-cdh5.3.2/hive-shims-common-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-common-secure/0.13.1-cdh5.3.2/hive-shims-common-secure-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-0.23/0.13.1-cdh5.3.2/hive-shims-0.23-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-scheduler/0.13.1-cdh5.3.2/hive-shims-scheduler-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.3.2/zookeeper-3.4.5-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/spark/spark-assembly-hadoop2.5.0-cdh5.3.2/1.2.0/spark-assembly-hadoop2.5.0-cdh5.3.2-1.2.0.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_protocol-cdh5.3.2/1.1.1-SNAPSHOT/splice_protocol-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/asynchbase/1.5.5/asynchbase-1.5.5.jar:/Users/jleach/.m2/repository/com/stumbleupon/async/1.4.0/async-1.4.0.jar:/Users/jleach/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_machine_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/splicetest/sqlj/sqlj-it-procs/1.0.2-SNAPSHOT/sqlj-it-procs-1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/jleach/.m2/repository/com/carrotsearch/hppc/0.5.2/hppc-0.5.2.jar:/Users/jleach/.m2/repository/com/carrotsearch/java-sizeof/0.0.4/java-sizeof-0.0.4.jar:/Users/jleach/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/jleach/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/jleach/.m2/repository/org/ow2/asm/asm/4.0/asm-4.0.jar:/Users/jleach/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/jleach/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/jleach/.m2/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/jleach/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/jleach/.m2/repository/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar:/Users/jleach/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/Users/jleach/.m2/repository/com/lmax/disruptor/3.2.1/disruptor-3.2.1.jar:/Users/jleach/.m2/repository/com/splicemachine/utilities/1.1.2-SNAPSHOT/utilities-1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/stats/1.0.0-SNAPSHOT/stats-1.0.0-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice-web/1.1.0-SNAPSHOT/splice-web-1.1.0-SNAPSHOT.tar.gz:/Users/jleach/.m2/repository/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar:/Users/jleach/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/jleach/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/jleach/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/jleach/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/jleach/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/jleach/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.7.1/jackson-jaxrs-1.7.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar:/Users/jleach/.m2/repository/asm/asm/3.1/asm-3.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/jleach/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/jleach/.m2/repository/commons-dbutils/commons-dbutils/1.5/commons-dbutils-1.5.jar:/Users/jleach/.m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar:/Users/jleach/.m2/repository/commons-lang/commons-lang/2.5/commons-lang-2.5.jar:/Users/jleach/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:/Users/jleach/.m2/repository/de/javakaffee/kryo-serializers/0.26/kryo-serializers-0.26.jar:/Users/jleach/.m2/repository/io/netty/netty/3.6.6.Final/netty-3.6.6.Final.jar:/Users/jleach/.m2/repository/net/sf/ehcache/ehcache-core/2.6.6/ehcache-core-2.6.6.jar:/Users/jleach/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/jleach/.m2/repository/net/sf/supercsv/super-csv/2.3.2-SNAPSHOT-splice/super-csv-2.3.2-SNAPSHOT-splice.jar:/Users/jleach/.m2/repository/com/splicemachine/db/1.1.1.2-SNAPSHOT/db-1.1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/dbclient/1.1.1.2-SNAPSHOT/dbclient-1.1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/lucene/lucene-core/4.3.1/lucene-core-4.3.1.jar:/Users/jleach/.m2/repository/org/apache/mrunit/mrunit/1.0.0/mrunit-1.0.0-hadoop2.jar:/Users/jleach/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.1/uncommons-maths-1.2.1.jar:/Users/jleach/.m2/repository/org/uncommons/watchmaker/watchmaker-framework/0.7.1/watchmaker-framework-0.7.1.jar:/Users/jleach/.m2/repository/junit/junit/4.11/junit-4.11.jar:/Users/jleach/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/jleach/.m2/repository/org/mockito/mockito-all/1.9.5/mockito-all-1.9.5.jar:/Users/jleach/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/jleach/Documents/workspace/spliceengine/cdh5.3.2/splice_machine_test/target/classes
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:java.library.path=:/Users/jleach/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/var/folders/01/td1fd0hx6z72c6g58vqc1tnr0000gp/T/
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:os.name=Mac OS X
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:os.arch=x86_64
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:os.version=10.10.3
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:user.name=jleach
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:user.home=/Users/jleach
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Client environment:user.dir=/Users/jleach/Documents/workspace/spliceengine/cdh5.3.2/splice_machine_test/work/app-20150603192548-0000/0
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x3e3c4870, quorum=localhost:2181, baseZNode=/hbase
15/06/03 19:26:07 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:07 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:07 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f762000f, negotiated timeout = 60000
15/06/03 19:26:07 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7ae6712 connecting to ZooKeeper ensemble=localhost:2181
15/06/03 19:26:07 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x7ae6712, quorum=localhost:2181, baseZNode=/hbase
15/06/03 19:26:07 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:07 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 19:26:07 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:07 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:07 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620010, negotiated timeout = 60000
15/06/03 19:26:07 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
15/06/03 19:26:07 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/03 19:26:08 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0xd80d1d connecting to ZooKeeper ensemble=localhost:2181
15/06/03 19:26:08 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0xd80d1d, quorum=localhost:2181, baseZNode=/hbase
15/06/03 19:26:08 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:08 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:08 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620011, negotiated timeout = 60000
15/06/03 19:26:08 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x3222590b connecting to ZooKeeper ensemble=localhost:2181
15/06/03 19:26:08 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x3222590b, quorum=localhost:2181, baseZNode=/hbase
15/06/03 19:26:08 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:08 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 19:26:08 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
15/06/03 19:26:08 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/03 19:26:08 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:08 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:08 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620012, negotiated timeout = 60000
15/06/03 19:26:09 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x12056a52 connecting to ZooKeeper ensemble=localhost:2181
15/06/03 19:26:09 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x12056a52, quorum=localhost:2181, baseZNode=/hbase
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620013, negotiated timeout = 60000
15/06/03 19:26:09 INFO temp.TempTable: Temp Table initial bucket count: 16
15/06/03 19:26:09 INFO zookeeper.RecoverableZooKeeper: Process identifier=spliceconnection connecting to ZooKeeper ensemble=localhost:2181
15/06/03 19:26:09 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=spliceconnection, quorum=localhost:2181, baseZNode=/hbase
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:09 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 19:26:09 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7809015 connecting to ZooKeeper ensemble=localhost:2181
15/06/03 19:26:09 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x7809015, quorum=localhost:2181, baseZNode=/hbase
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620014, negotiated timeout = 60000
15/06/03 19:26:09 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1c081b06 connecting to ZooKeeper ensemble=localhost:2181
15/06/03 19:26:09 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x1c081b06, quorum=localhost:2181, baseZNode=/hbase
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:09 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620015, negotiated timeout = 60000
15/06/03 19:26:09 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
15/06/03 19:26:09 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:09 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620016, negotiated timeout = 60000
15/06/03 19:26:10 ERROR utils.PipelineConstants: No Native Snappy Installed: Splice Machine's Write Pipeline will not compress data over the wire.
15/06/03 19:26:10 INFO hbase.SpliceDriver: Booting the SpliceDriver
15/06/03 19:26:10 INFO zookeeper.RecoverableZooKeeper: Node /ddl already exists and this is not a retry
15/06/03 19:26:10 INFO zookeeper.RecoverableZooKeeper: Node /ddl/ongoingChanges already exists and this is not a retry
15/06/03 19:26:10 INFO zookeeper.RecoverableZooKeeper: Node /ddl/activeServers already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /spliceTasks already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /spliceJobs already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates/__CONGLOM_SEQUENCE already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /derbyPropertyPath already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /transactions already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /transactions/maxReservedTimestamp already exists and this is not a retry
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Node /transactions/minimum already exists and this is not a retry
15/06/03 19:26:11 INFO db.SpliceDatabase: Booting the Splice Machine database
15/06/03 19:26:11 INFO txn.SpliceTimestampSource: Creating the TimestampClient...
15/06/03 19:26:11 INFO timestamp.TimestampClient: TimestampClient on region server successfully registered with JMX
15/06/03 19:26:11 INFO timestamp.TimestampClient: Attempting to connect to server (host localhost, port 60012)
15/06/03 19:26:11 INFO timestamp.TimestampClient: Successfully connected to server
15/06/03 19:26:11 INFO catalog.SpliceDataDictionary: Splice Software Version = 1.1.1
15/06/03 19:26:11 INFO catalog.SpliceDataDictionary: Splice Catalog Version = 1.1.1
15/06/03 19:26:11 INFO hbase.SpliceDriver: Splice Engine is Running, Enabling Services
15/06/03 19:26:11 INFO hbase.SpliceDriver: Splice Machine Release = 1.1.1-SNAPSHOT
15/06/03 19:26:11 INFO hbase.SpliceDriver: Splice Machine Version Hash = 08c20a115b
15/06/03 19:26:11 INFO hbase.SpliceDriver: Splice Machine Build Time = 2015-06-03 17:28 +0000
15/06/03 19:26:11 INFO hbase.SpliceDriver: Splice Machine URL = http://www.splicemachine.com
15/06/03 19:26:11 INFO hbase.SpliceDriver: Services successfully started, enabling JDBC connections...
15/06/03 19:26:11 INFO rdd.NewHadoopRDD: Input split: localhost:\x01,\xF1\xC1\xC1
15/06/03 19:26:11 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
15/06/03 19:26:11 INFO storage.MemoryStore: ensureFreeSpace(24584) called with curMem=26973, maxMem=740960501
15/06/03 19:26:11 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KB, free 706.6 MB)
15/06/03 19:26:11 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/06/03 19:26:11 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 11 ms
15/06/03 19:26:11 INFO hbase.SpliceDriver: Ready to accept JDBC connections on 0.0.0.0:1527
15/06/03 19:26:11 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=51557, maxMem=740960501
15/06/03 19:26:11 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 380.7 KB, free 706.2 MB)
15/06/03 19:26:11 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x5f1589d6 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 19:26:11 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x5f1589d6, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 19:26:11 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:11 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:11 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620017, negotiated timeout = 60000
15/06/03 19:26:12 INFO hfile.CacheConfig: Allocating LruBlockCache with maximum size 1.9 G
15/06/03 19:26:12 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:12 INFO util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
15/06/03 19:26:12 INFO util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
15/06/03 19:26:12 INFO regionserver.HRegion: Onlined 790b5e2020bc448c107762790b7552e9; next sequenceid=15
15/06/03 19:26:12 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:12 INFO regionserver.HRegion: Onlined 790b5e2020bc448c107762790b7552e9; next sequenceid=15
15/06/03 19:26:12 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 19:26:12 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbbf2f7620017
15/06/03 19:26:12 INFO zookeeper.ZooKeeper: Session: 0x14dbbf2f7620017 closed
15/06/03 19:26:12 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 19:26:12 INFO regionserver.HStore: Closed V
15/06/03 19:26:12 INFO regionserver.HRegion: Closed 209,,1433352600310.790b5e2020bc448c107762790b7552e9.
15/06/03 19:26:12 WARN regionserver.HRegion: Region 209,,1433352600310.790b5e2020bc448c107762790b7552e9. already closed
15/06/03 19:26:12 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2249 bytes result sent to driver
15/06/03 19:26:41 INFO storage.BlockManager: Removing broadcast 1
15/06/03 19:26:41 INFO storage.BlockManager: Removing block broadcast_1_piece0
15/06/03 19:26:41 INFO storage.MemoryStore: Block broadcast_1_piece0 of size 8149 dropped from memory (free 740527298)
15/06/03 19:26:41 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/06/03 19:26:41 INFO storage.BlockManager: Removing block broadcast_1
15/06/03 19:26:41 INFO storage.MemoryStore: Block broadcast_1 of size 18824 dropped from memory (free 740546122)
15/06/03 19:26:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
15/06/03 19:26:42 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
15/06/03 19:26:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
15/06/03 19:26:42 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
15/06/03 19:26:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
15/06/03 19:26:42 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
15/06/03 19:26:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
15/06/03 19:26:42 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
15/06/03 19:26:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
15/06/03 19:26:42 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 4)
15/06/03 19:26:42 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 5)
15/06/03 19:26:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
15/06/03 19:26:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
15/06/03 19:26:42 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 6)
15/06/03 19:26:42 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 7)
15/06/03 19:26:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
15/06/03 19:26:42 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 8)
15/06/03 19:26:42 INFO storage.MemoryStore: ensureFreeSpace(8790) called with curMem=414379, maxMem=740960501
15/06/03 19:26:42 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.6 KB, free 706.2 MB)
15/06/03 19:26:42 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/06/03 19:26:42 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 16 ms
15/06/03 19:26:42 INFO storage.MemoryStore: ensureFreeSpace(19952) called with curMem=423169, maxMem=740960501
15/06/03 19:26:42 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.5 KB, free 706.2 MB)
15/06/03 19:26:42 INFO rdd.NewHadoopRDD: Input split: localhost:,\xE4\x0B\xAA\xC1\x00\x84
15/06/03 19:26:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE46\xDE!\x00\x83,\xE4=\xD3\xE7
15/06/03 19:26:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4I_'\x00\x84,\xE4W\xCF\x00\x00\x84
15/06/03 19:26:42 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
15/06/03 19:26:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x1A\x15\x07,\xE4(\x7F\xE1\x00\x85
15/06/03 19:26:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4W\xCF\x00\x00\x84,\xE4f9\xC6
15/06/03 19:26:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x0B\xAA\xC1\x00\x84,\xE4\x1A\x15\x07
15/06/03 19:26:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4(\x7F\xE1\x00\x85,\xE46\xDE!\x00\x83
15/06/03 19:26:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4=\xD3\xE7,\xE4I_'\x00\x84
15/06/03 19:26:42 INFO storage.MemoryStore: ensureFreeSpace(24731) called with curMem=443121, maxMem=740960501
15/06/03 19:26:42 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.2 KB, free 706.2 MB)
15/06/03 19:26:42 INFO storage.BlockManagerMaster: Updated info of block broadcast_2_piece0
15/06/03 19:26:42 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 10 ms
15/06/03 19:26:42 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=467852, maxMem=740960501
15/06/03 19:26:42 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 380.7 KB, free 705.8 MB)
15/06/03 19:26:42 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x31626345 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 19:26:42 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x31626345, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 19:26:42 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:26:42 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:26:42 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f7620019, negotiated timeout = 60000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:26:42 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:46 INFO regionserver.HStore: Closed V
15/06/03 19:26:46 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:26:46 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:26:46 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 5). 2020 bytes result sent to driver
15/06/03 19:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
15/06/03 19:26:46 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 9)
15/06/03 19:26:46 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4f9\xC6,\xE4t\xA2B\x00\x82
15/06/03 19:26:46 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:46 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:46 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:46 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:47 INFO regionserver.HStore: Closed V
15/06/03 19:26:47 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:26:47 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:26:47 INFO regionserver.HStore: Closed V
15/06/03 19:26:47 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:26:47 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:26:47 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2020 bytes result sent to driver
15/06/03 19:26:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
15/06/03 19:26:47 INFO executor.Executor: Running task 9.0 in stage 1.0 (TID 10)
15/06/03 19:26:47 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 6). 2019 bytes result sent to driver
15/06/03 19:26:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
15/06/03 19:26:47 INFO executor.Executor: Running task 10.0 in stage 1.0 (TID 11)
15/06/03 19:26:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4t\xA2B\x00\x82,\xE4{\xA2C\x00\x83
15/06/03 19:26:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4{\xA2C\x00\x83,\xE4\x80\xCB\xE7\x00\x83
15/06/03 19:26:47 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:47 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 19:26:47 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:26:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:47 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 19:26:48 INFO regionserver.HStore: Closed V
15/06/03 19:26:48 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:26:48 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:26:48 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 8). 2020 bytes result sent to driver
15/06/03 19:26:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
15/06/03 19:26:48 INFO executor.Executor: Running task 11.0 in stage 1.0 (TID 12)
15/06/03 19:26:48 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x80\xCB\xE7\x00\x83,\xE4\x8CK\xE5\x00\x86
15/06/03 19:26:48 INFO regionserver.HStore: Closed V
15/06/03 19:26:48 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:26:48 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:26:48 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 2020 bytes result sent to driver
15/06/03 19:26:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
15/06/03 19:26:48 INFO executor.Executor: Running task 12.0 in stage 1.0 (TID 13)
15/06/03 19:26:48 INFO regionserver.HStore: Closed V
15/06/03 19:26:48 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:26:48 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:26:48 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x8CK\xE5\x00\x86,\xE4\x9A\xB2c\x00\x85
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 2020 bytes result sent to driver
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:26:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
15/06/03 19:26:48 INFO executor.Executor: Running task 13.0 in stage 1.0 (TID 14)
15/06/03 19:26:48 INFO regionserver.HStore: Closed V
15/06/03 19:26:48 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:26:48 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:26:48 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x9A\xB2c\x00\x85,\xE4\xA7.\xA7\x00\x82
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:26:48 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 4). 2020 bytes result sent to driver
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
15/06/03 19:26:48 INFO executor.Executor: Running task 14.0 in stage 1.0 (TID 15)
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:26:48 INFO regionserver.HStore: Closed V
15/06/03 19:26:48 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:26:48 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:26:48 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xA7.\xA7\x00\x82,\xE4\xB2\xBAC\x00\x82
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 7). 2020 bytes result sent to driver
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
15/06/03 19:26:48 INFO executor.Executor: Running task 15.0 in stage 1.0 (TID 16)
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:48 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xB2\xBAC\x00\x82,\xE4\xC1(\x85\x00\x85
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:48 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:49 INFO regionserver.HStore: Closed V
15/06/03 19:26:49 INFO regionserver.HRegion: Closed 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72.
15/06/03 19:26:49 WARN regionserver.HRegion: Region 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72. already closed
15/06/03 19:26:49 INFO executor.Executor: Finished task 10.0 in stage 1.0 (TID 11). 1856 bytes result sent to driver
15/06/03 19:26:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
15/06/03 19:26:49 INFO executor.Executor: Running task 16.0 in stage 1.0 (TID 17)
15/06/03 19:26:49 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xC1(\x85\x00\x85,\xE4\xCF\x8E\xE5\x00\x83
15/06/03 19:26:49 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:49 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:49 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:49 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:49 INFO regionserver.HStore: Closed V
15/06/03 19:26:49 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:26:49 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:26:49 INFO executor.Executor: Finished task 9.0 in stage 1.0 (TID 10). 1856 bytes result sent to driver
15/06/03 19:26:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
15/06/03 19:26:49 INFO executor.Executor: Running task 17.0 in stage 1.0 (TID 18)
15/06/03 19:26:49 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xCF\x8E\xE5\x00\x83,\xE4\xDD\xF5\xC2\x00\x85
15/06/03 19:26:49 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:49 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:49 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:49 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:50 INFO regionserver.HStore: Closed V
15/06/03 19:26:50 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:26:50 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:26:50 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 9). 1857 bytes result sent to driver
15/06/03 19:26:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
15/06/03 19:26:50 INFO executor.Executor: Running task 18.0 in stage 1.0 (TID 19)
15/06/03 19:26:50 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xDD\xF5\xC2\x00\x85,\xE4\xECc\xA7
15/06/03 19:26:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:50 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:50 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:52 INFO regionserver.HStore: Closed V
15/06/03 19:26:52 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:26:52 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:26:52 INFO executor.Executor: Finished task 11.0 in stage 1.0 (TID 12). 1857 bytes result sent to driver
15/06/03 19:26:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
15/06/03 19:26:52 INFO executor.Executor: Running task 19.0 in stage 1.0 (TID 20)
15/06/03 19:26:52 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xECc\xA7,\xE4\xF7k\x86\x00\x85
15/06/03 19:26:52 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:52 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:52 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:52 INFO regionserver.HStore: Closed V
15/06/03 19:26:52 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:26:52 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:26:52 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:26:52 INFO executor.Executor: Finished task 14.0 in stage 1.0 (TID 15). 1857 bytes result sent to driver
15/06/03 19:26:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
15/06/03 19:26:52 INFO executor.Executor: Running task 20.0 in stage 1.0 (TID 21)
15/06/03 19:26:52 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xF7k\x86\x00\x85,\xE5\x02\xF4\x83\x00\x82
15/06/03 19:26:52 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:52 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:52 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:52 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:52 INFO regionserver.HStore: Closed V
15/06/03 19:26:52 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:26:52 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:26:52 INFO executor.Executor: Finished task 13.0 in stage 1.0 (TID 14). 1857 bytes result sent to driver
15/06/03 19:26:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
15/06/03 19:26:52 INFO executor.Executor: Running task 21.0 in stage 1.0 (TID 22)
15/06/03 19:26:52 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x02\xF4\x83\x00\x82,\xE5\x11ae
15/06/03 19:26:52 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:52 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:52 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:52 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:52 INFO regionserver.HStore: Closed V
15/06/03 19:26:52 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:26:52 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:26:53 INFO executor.Executor: Finished task 12.0 in stage 1.0 (TID 13). 1857 bytes result sent to driver
15/06/03 19:26:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
15/06/03 19:26:53 INFO executor.Executor: Running task 22.0 in stage 1.0 (TID 23)
15/06/03 19:26:53 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x11ae,\xE5\x1F\xCB\xA1\x00\x83
15/06/03 19:26:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:53 INFO regionserver.HStore: Closed V
15/06/03 19:26:53 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:26:53 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:26:53 INFO executor.Executor: Finished task 15.0 in stage 1.0 (TID 16). 1856 bytes result sent to driver
15/06/03 19:26:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
15/06/03 19:26:53 INFO executor.Executor: Running task 23.0 in stage 1.0 (TID 24)
15/06/03 19:26:53 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x1F\xCB\xA1\x00\x83,\xE5.:\x08
15/06/03 19:26:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:53 INFO regionserver.HStore: Closed V
15/06/03 19:26:53 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:26:53 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:26:53 INFO executor.Executor: Finished task 16.0 in stage 1.0 (TID 17). 1857 bytes result sent to driver
15/06/03 19:26:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
15/06/03 19:26:53 INFO executor.Executor: Running task 24.0 in stage 1.0 (TID 25)
15/06/03 19:26:53 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5.:\x08,\xE5<\xA2D\x00\x85
15/06/03 19:26:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:54 INFO regionserver.HStore: Closed V
15/06/03 19:26:54 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:26:54 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:26:54 INFO executor.Executor: Finished task 17.0 in stage 1.0 (TID 18). 1857 bytes result sent to driver
15/06/03 19:26:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
15/06/03 19:26:54 INFO executor.Executor: Running task 25.0 in stage 1.0 (TID 26)
15/06/03 19:26:54 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5<\xA2D\x00\x85,\xE5CNa\x00\x85
15/06/03 19:26:54 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:54 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:54 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:54 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:26:54 INFO regionserver.HStore: Closed V
15/06/03 19:26:54 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:26:54 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:26:54 INFO executor.Executor: Finished task 19.0 in stage 1.0 (TID 20). 1856 bytes result sent to driver
15/06/03 19:26:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27
15/06/03 19:26:54 INFO executor.Executor: Running task 26.0 in stage 1.0 (TID 27)
15/06/03 19:26:54 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5CNa\x00\x85,\xE5N\xD3\x85\x00\x85
15/06/03 19:26:54 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:54 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:54 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:54 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:54 INFO regionserver.HStore: Closed V
15/06/03 19:26:54 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:26:54 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:26:55 INFO executor.Executor: Finished task 18.0 in stage 1.0 (TID 19). 1856 bytes result sent to driver
15/06/03 19:26:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28
15/06/03 19:26:55 INFO executor.Executor: Running task 27.0 in stage 1.0 (TID 28)
15/06/03 19:26:55 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5N\xD3\x85\x00\x85,\xE5]:\xC7\x00\x82
15/06/03 19:26:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:55 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:55 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:56 INFO regionserver.HStore: Closed V
15/06/03 19:26:56 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:26:56 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:26:56 INFO executor.Executor: Finished task 20.0 in stage 1.0 (TID 21). 1857 bytes result sent to driver
15/06/03 19:26:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29
15/06/03 19:26:56 INFO executor.Executor: Running task 28.0 in stage 1.0 (TID 29)
15/06/03 19:26:56 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5]:\xC7\x00\x82,\xE5k\xA2\xE1\x00\x83
15/06/03 19:26:56 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:56 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:56 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:56 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:56 INFO regionserver.HStore: Closed V
15/06/03 19:26:56 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:26:56 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:26:56 INFO executor.Executor: Finished task 25.0 in stage 1.0 (TID 26). 2020 bytes result sent to driver
15/06/03 19:26:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30
15/06/03 19:26:56 INFO executor.Executor: Running task 29.0 in stage 1.0 (TID 30)
15/06/03 19:26:56 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5k\xA2\xE1\x00\x83,\xE5z\x00G\x00\x81
15/06/03 19:26:56 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:56 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:56 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:56 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:57 INFO regionserver.HStore: Closed V
15/06/03 19:26:57 INFO regionserver.HStore: Closed V
15/06/03 19:26:57 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:26:57 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:26:57 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:26:57 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:26:57 INFO executor.Executor: Finished task 22.0 in stage 1.0 (TID 23). 2020 bytes result sent to driver
15/06/03 19:26:57 INFO executor.Executor: Finished task 21.0 in stage 1.0 (TID 22). 2020 bytes result sent to driver
15/06/03 19:26:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31
15/06/03 19:26:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32
15/06/03 19:26:57 INFO executor.Executor: Running task 30.0 in stage 1.0 (TID 31)
15/06/03 19:26:57 INFO executor.Executor: Running task 31.0 in stage 1.0 (TID 32)
15/06/03 19:26:57 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x86\x8B\x06,\xE5\x92\x0Cc\x00\x82
15/06/03 19:26:57 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:57 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:57 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5z\x00G\x00\x81,\xE5\x86\x8B\x06
15/06/03 19:26:57 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:57 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:57 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:57 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:57 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:57 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:26:57 INFO regionserver.HStore: Closed V
15/06/03 19:26:57 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:26:57 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:26:57 INFO executor.Executor: Finished task 23.0 in stage 1.0 (TID 24). 2020 bytes result sent to driver
15/06/03 19:26:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33
15/06/03 19:26:57 INFO executor.Executor: Running task 32.0 in stage 1.0 (TID 33)
15/06/03 19:26:57 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x92\x0Cc\x00\x82,\xE5\xA0q\x06\x00\x86
15/06/03 19:26:57 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:57 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:57 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:57 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:58 INFO regionserver.HStore: Closed V
15/06/03 19:26:58 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:26:58 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:26:58 INFO executor.Executor: Finished task 24.0 in stage 1.0 (TID 25). 2020 bytes result sent to driver
15/06/03 19:26:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34
15/06/03 19:26:58 INFO executor.Executor: Running task 33.0 in stage 1.0 (TID 34)
15/06/03 19:26:58 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xA0q\x06\x00\x86,\xE5\xAE\xD6B\x00\x82
15/06/03 19:26:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:58 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:58 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:58 INFO regionserver.HStore: Closed V
15/06/03 19:26:58 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:26:58 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:26:58 INFO executor.Executor: Finished task 26.0 in stage 1.0 (TID 27). 2020 bytes result sent to driver
15/06/03 19:26:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35
15/06/03 19:26:58 INFO executor.Executor: Running task 34.0 in stage 1.0 (TID 35)
15/06/03 19:26:58 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xAE\xD6B\x00\x82,\xE5\xBDA`\x00\x85
15/06/03 19:26:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:58 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:58 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:59 INFO regionserver.HStore: Closed V
15/06/03 19:26:59 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:26:59 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:26:59 INFO executor.Executor: Finished task 27.0 in stage 1.0 (TID 28). 2019 bytes result sent to driver
15/06/03 19:26:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36
15/06/03 19:26:59 INFO executor.Executor: Running task 35.0 in stage 1.0 (TID 36)
15/06/03 19:26:59 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xBDA`\x00\x85,
15/06/03 19:26:59 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:59 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:26:59 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:26:59 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:00 INFO regionserver.HStore: Closed V
15/06/03 19:27:00 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:27:00 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:27:00 INFO executor.Executor: Finished task 28.0 in stage 1.0 (TID 29). 1857 bytes result sent to driver
15/06/03 19:27:00 INFO regionserver.HStore: Closed V
15/06/03 19:27:00 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:27:00 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:27:00 INFO executor.Executor: Finished task 29.0 in stage 1.0 (TID 30). 1857 bytes result sent to driver
15/06/03 19:27:01 INFO regionserver.HStore: Closed V
15/06/03 19:27:01 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:01 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:01 INFO regionserver.HStore: Closed V
15/06/03 19:27:01 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:27:01 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:27:01 INFO executor.Executor: Finished task 31.0 in stage 1.0 (TID 32). 1856 bytes result sent to driver
15/06/03 19:27:01 INFO executor.Executor: Finished task 30.0 in stage 1.0 (TID 31). 1857 bytes result sent to driver
15/06/03 19:27:02 INFO regionserver.HStore: Closed V
15/06/03 19:27:02 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:02 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:02 INFO executor.Executor: Finished task 32.0 in stage 1.0 (TID 33). 1856 bytes result sent to driver
15/06/03 19:27:02 INFO regionserver.HStore: Closed V
15/06/03 19:27:02 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:02 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:02 INFO executor.Executor: Finished task 34.0 in stage 1.0 (TID 35). 1856 bytes result sent to driver
15/06/03 19:27:02 INFO regionserver.HStore: Closed V
15/06/03 19:27:02 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:02 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:02 INFO executor.Executor: Finished task 33.0 in stage 1.0 (TID 34). 1857 bytes result sent to driver
15/06/03 19:27:02 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 19:27:02 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbbf2f7620019
15/06/03 19:27:02 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 19:27:02 INFO zookeeper.ZooKeeper: Session: 0x14dbbf2f7620019 closed
15/06/03 19:27:02 INFO regionserver.HStore: Closed V
15/06/03 19:27:02 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:02 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:02 INFO executor.Executor: Finished task 35.0 in stage 1.0 (TID 36). 1856 bytes result sent to driver
15/06/03 19:27:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37
15/06/03 19:27:02 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 37)
15/06/03 19:27:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
15/06/03 19:27:02 INFO storage.MemoryStore: ensureFreeSpace(8019) called with curMem=857647, maxMem=740960501
15/06/03 19:27:02 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.8 KB, free 705.8 MB)
15/06/03 19:27:02 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/06/03 19:27:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 10 ms
15/06/03 19:27:02 INFO storage.MemoryStore: ensureFreeSpace(16536) called with curMem=865666, maxMem=740960501
15/06/03 19:27:02 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.1 KB, free 705.8 MB)
15/06/03 19:27:02 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 37). 721 bytes result sent to driver
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38
15/06/03 19:27:03 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 38)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39
15/06/03 19:27:03 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 39)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40
15/06/03 19:27:03 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 40)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41
15/06/03 19:27:03 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 41)
15/06/03 19:27:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
15/06/03 19:27:03 INFO storage.MemoryStore: ensureFreeSpace(15356) called with curMem=882202, maxMem=740960501
15/06/03 19:27:03 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.0 KB, free 705.8 MB)
15/06/03 19:27:03 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
15/06/03 19:27:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 8 ms
15/06/03 19:27:03 INFO storage.MemoryStore: ensureFreeSpace(62992) called with curMem=897558, maxMem=740960501
15/06/03 19:27:03 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 61.5 KB, free 705.7 MB)
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 3.0 in stage 3.0 (TID 41)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 0.0 in stage 3.0 (TID 38)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 1.0 in stage 3.0 (TID 39)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 2.0 in stage 3.0 (TID 40)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43
15/06/03 19:27:03 INFO executor.Executor: Running task 3.1 in stage 3.0 (TID 42)
15/06/03 19:27:03 INFO executor.Executor: Running task 0.1 in stage 3.0 (TID 43)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44
15/06/03 19:27:03 INFO executor.Executor: Running task 1.1 in stage 3.0 (TID 44)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45
15/06/03 19:27:03 INFO executor.Executor: Running task 2.1 in stage 3.0 (TID 45)
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 1.1 in stage 3.0 (TID 44)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 3.1 in stage 3.0 (TID 42)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 2.1 in stage 3.0 (TID 45)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46
15/06/03 19:27:03 INFO executor.Executor: Running task 1.2 in stage 3.0 (TID 46)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47
15/06/03 19:27:03 INFO executor.Executor: Running task 3.2 in stage 3.0 (TID 47)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48
15/06/03 19:27:03 INFO executor.Executor: Running task 2.2 in stage 3.0 (TID 48)
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 0.1 in stage 3.0 (TID 43)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49
15/06/03 19:27:03 INFO executor.Executor: Running task 0.2 in stage 3.0 (TID 49)
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 1.2 in stage 3.0 (TID 46)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 2.2 in stage 3.0 (TID 48)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50
15/06/03 19:27:03 INFO executor.Executor: Running task 1.3 in stage 3.0 (TID 50)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 3.2 in stage 3.0 (TID 47)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51
15/06/03 19:27:03 INFO executor.Executor: Running task 2.3 in stage 3.0 (TID 51)
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52
15/06/03 19:27:03 INFO executor.Executor: Running task 3.3 in stage 3.0 (TID 52)
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 0.2 in stage 3.0 (TID 49)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 1.3 in stage 3.0 (TID 50)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53
15/06/03 19:27:03 INFO executor.Executor: Running task 0.3 in stage 3.0 (TID 53)
15/06/03 19:27:03 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:03 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:03 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:03 ERROR executor.Executor: Exception in task 3.3 in stage 3.0 (TID 52)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:03 INFO executor.Executor: Executor is trying to kill task 2.3 in stage 3.0 (TID 51)
15/06/03 19:27:03 INFO executor.Executor: Executor is trying to kill task 0.3 in stage 3.0 (TID 53)
15/06/03 19:27:03 INFO executor.Executor: Executor killed task 2.3 in stage 3.0 (TID 51)
15/06/03 19:27:03 INFO executor.Executor: Executor killed task 0.3 in stage 3.0 (TID 53)
15/06/03 19:27:26 INFO storage.BlockManager: Removing broadcast 4
15/06/03 19:27:26 INFO storage.BlockManager: Removing block broadcast_4
15/06/03 19:27:26 INFO storage.MemoryStore: Block broadcast_4 of size 16536 dropped from memory (free 740016487)
15/06/03 19:27:26 INFO storage.BlockManager: Removing block broadcast_4_piece0
15/06/03 19:27:26 INFO storage.MemoryStore: Block broadcast_4_piece0 of size 8019 dropped from memory (free 740024506)
15/06/03 19:27:26 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/06/03 19:27:26 INFO storage.BlockManager: Removing broadcast 5
15/06/03 19:27:26 INFO storage.BlockManager: Removing block broadcast_5_piece0
15/06/03 19:27:26 INFO storage.MemoryStore: Block broadcast_5_piece0 of size 15356 dropped from memory (free 740039862)
15/06/03 19:27:26 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
15/06/03 19:27:26 INFO storage.BlockManager: Removing block broadcast_5
15/06/03 19:27:26 INFO storage.MemoryStore: Block broadcast_5 of size 62992 dropped from memory (free 740102854)
15/06/03 19:27:26 INFO storage.BlockManager: Removing broadcast 2
15/06/03 19:27:26 INFO storage.BlockManager: Removing block broadcast_2
15/06/03 19:27:26 INFO storage.MemoryStore: Block broadcast_2 of size 389795 dropped from memory (free 740492649)
15/06/03 19:27:26 INFO storage.BlockManager: Removing block broadcast_2_piece0
15/06/03 19:27:26 INFO storage.MemoryStore: Block broadcast_2_piece0 of size 24731 dropped from memory (free 740517380)
15/06/03 19:27:26 INFO storage.BlockManagerMaster: Updated info of block broadcast_2_piece0
15/06/03 19:27:26 INFO storage.BlockManager: Removing broadcast 3
15/06/03 19:27:26 INFO storage.BlockManager: Removing block broadcast_3_piece0
15/06/03 19:27:26 INFO storage.MemoryStore: Block broadcast_3_piece0 of size 8790 dropped from memory (free 740526170)
15/06/03 19:27:26 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/06/03 19:27:26 INFO storage.BlockManager: Removing block broadcast_3
15/06/03 19:27:26 INFO storage.MemoryStore: Block broadcast_3 of size 19952 dropped from memory (free 740546122)
15/06/03 19:27:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54
15/06/03 19:27:32 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 54)
15/06/03 19:27:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55
15/06/03 19:27:32 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 55)
15/06/03 19:27:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
15/06/03 19:27:32 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 56)
15/06/03 19:27:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57
15/06/03 19:27:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
15/06/03 19:27:32 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 57)
15/06/03 19:27:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58
15/06/03 19:27:32 INFO executor.Executor: Running task 4.0 in stage 4.0 (TID 58)
15/06/03 19:27:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59
15/06/03 19:27:32 INFO executor.Executor: Running task 5.0 in stage 4.0 (TID 59)
15/06/03 19:27:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60
15/06/03 19:27:32 INFO executor.Executor: Running task 6.0 in stage 4.0 (TID 60)
15/06/03 19:27:32 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61
15/06/03 19:27:32 INFO executor.Executor: Running task 7.0 in stage 4.0 (TID 61)
15/06/03 19:27:32 INFO storage.MemoryStore: ensureFreeSpace(8790) called with curMem=414379, maxMem=740960501
15/06/03 19:27:32 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.6 KB, free 706.2 MB)
15/06/03 19:27:32 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
15/06/03 19:27:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 13 ms
15/06/03 19:27:32 INFO storage.MemoryStore: ensureFreeSpace(19952) called with curMem=423169, maxMem=740960501
15/06/03 19:27:32 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.5 KB, free 706.2 MB)
15/06/03 19:27:32 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4=\xD3\xE7,\xE4I_'\x00\x84
15/06/03 19:27:32 INFO rdd.NewHadoopRDD: Input split: localhost:\xE46\xDE!\x00\x83,\xE4=\xD3\xE7
15/06/03 19:27:32 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
15/06/03 19:27:32 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4W\xCF\x00\x00\x84,\xE4f9\xC6
15/06/03 19:27:32 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x1A\x15\x07,\xE4(\x7F\xE1\x00\x85
15/06/03 19:27:32 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x0B\xAA\xC1\x00\x84,\xE4\x1A\x15\x07
15/06/03 19:27:32 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4(\x7F\xE1\x00\x85,\xE46\xDE!\x00\x83
15/06/03 19:27:32 INFO rdd.NewHadoopRDD: Input split: localhost:,\xE4\x0B\xAA\xC1\x00\x84
15/06/03 19:27:32 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4I_'\x00\x84,\xE4W\xCF\x00\x00\x84
15/06/03 19:27:32 INFO storage.MemoryStore: ensureFreeSpace(24731) called with curMem=443121, maxMem=740960501
15/06/03 19:27:32 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.2 KB, free 706.2 MB)
15/06/03 19:27:32 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
15/06/03 19:27:32 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 8 ms
15/06/03 19:27:32 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=467852, maxMem=740960501
15/06/03 19:27:32 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 380.7 KB, free 705.8 MB)
15/06/03 19:27:32 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1ccca413 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 19:27:32 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x1ccca413, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 19:27:32 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:27:32 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:27:32 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f762001a, negotiated timeout = 60000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:32 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:32 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:27:34 INFO regionserver.HStore: Closed V
15/06/03 19:27:34 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:27:34 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:27:34 INFO executor.Executor: Finished task 4.0 in stage 4.0 (TID 58). 1857 bytes result sent to driver
15/06/03 19:27:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62
15/06/03 19:27:34 INFO executor.Executor: Running task 8.0 in stage 4.0 (TID 62)
15/06/03 19:27:34 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4f9\xC6,\xE4t\xA2B\x00\x82
15/06/03 19:27:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:34 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:34 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:36 INFO regionserver.HStore: Closed V
15/06/03 19:27:36 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:27:36 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:27:36 INFO regionserver.HStore: Closed V
15/06/03 19:27:36 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:27:36 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:27:36 INFO executor.Executor: Finished task 5.0 in stage 4.0 (TID 59). 1856 bytes result sent to driver
15/06/03 19:27:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63
15/06/03 19:27:36 INFO executor.Executor: Running task 9.0 in stage 4.0 (TID 63)
15/06/03 19:27:36 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 54). 1857 bytes result sent to driver
15/06/03 19:27:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64
15/06/03 19:27:36 INFO executor.Executor: Running task 10.0 in stage 4.0 (TID 64)
15/06/03 19:27:36 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4t\xA2B\x00\x82,\xE4{\xA2C\x00\x83
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:36 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4{\xA2C\x00\x83,\xE4\x80\xCB\xE7\x00\x83
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 19:27:36 INFO regionserver.HStore: Closed V
15/06/03 19:27:36 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:27:36 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:27:36 INFO regionserver.HStore: Closed V
15/06/03 19:27:36 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:27:36 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:27:36 INFO executor.Executor: Finished task 6.0 in stage 4.0 (TID 60). 2020 bytes result sent to driver
15/06/03 19:27:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65
15/06/03 19:27:36 INFO executor.Executor: Running task 11.0 in stage 4.0 (TID 65)
15/06/03 19:27:36 INFO executor.Executor: Finished task 7.0 in stage 4.0 (TID 61). 2020 bytes result sent to driver
15/06/03 19:27:36 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x80\xCB\xE7\x00\x83,\xE4\x8CK\xE5\x00\x86
15/06/03 19:27:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66
15/06/03 19:27:36 INFO executor.Executor: Running task 12.0 in stage 4.0 (TID 66)
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:27:36 INFO regionserver.HStore: Closed V
15/06/03 19:27:36 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:27:36 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:27:36 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x8CK\xE5\x00\x86,\xE4\x9A\xB2c\x00\x85
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:27:36 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 56). 2020 bytes result sent to driver
15/06/03 19:27:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67
15/06/03 19:27:36 INFO executor.Executor: Running task 13.0 in stage 4.0 (TID 67)
15/06/03 19:27:36 INFO regionserver.HStore: Closed V
15/06/03 19:27:36 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:27:36 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:27:36 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x9A\xB2c\x00\x85,\xE4\xA7.\xA7\x00\x82
15/06/03 19:27:36 INFO regionserver.HStore: Closed V
15/06/03 19:27:36 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:27:36 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:27:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:36 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:27:36 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 57). 2020 bytes result sent to driver
15/06/03 19:27:37 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 55). 2020 bytes result sent to driver
15/06/03 19:27:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68
15/06/03 19:27:37 INFO executor.Executor: Running task 14.0 in stage 4.0 (TID 68)
15/06/03 19:27:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
15/06/03 19:27:37 INFO executor.Executor: Running task 15.0 in stage 4.0 (TID 69)
15/06/03 19:27:37 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xB2\xBAC\x00\x82,\xE4\xC1(\x85\x00\x85
15/06/03 19:27:37 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xA7.\xA7\x00\x82,\xE4\xB2\xBAC\x00\x82
15/06/03 19:27:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:37 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:37 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:37 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:37 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:37 INFO regionserver.HStore: Closed V
15/06/03 19:27:37 INFO regionserver.HRegion: Closed 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72.
15/06/03 19:27:37 WARN regionserver.HRegion: Region 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72. already closed
15/06/03 19:27:37 INFO executor.Executor: Finished task 10.0 in stage 4.0 (TID 64). 1856 bytes result sent to driver
15/06/03 19:27:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70
15/06/03 19:27:37 INFO executor.Executor: Running task 16.0 in stage 4.0 (TID 70)
15/06/03 19:27:37 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xC1(\x85\x00\x85,\xE4\xCF\x8E\xE5\x00\x83
15/06/03 19:27:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:37 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:37 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:37 INFO regionserver.HStore: Closed V
15/06/03 19:27:37 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:27:37 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:27:37 INFO executor.Executor: Finished task 9.0 in stage 4.0 (TID 63). 1856 bytes result sent to driver
15/06/03 19:27:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71
15/06/03 19:27:37 INFO executor.Executor: Running task 17.0 in stage 4.0 (TID 71)
15/06/03 19:27:37 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xCF\x8E\xE5\x00\x83,\xE4\xDD\xF5\xC2\x00\x85
15/06/03 19:27:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:38 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:38 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:38 INFO regionserver.HStore: Closed V
15/06/03 19:27:38 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:27:38 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:27:38 INFO executor.Executor: Finished task 8.0 in stage 4.0 (TID 62). 2020 bytes result sent to driver
15/06/03 19:27:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72
15/06/03 19:27:38 INFO executor.Executor: Running task 18.0 in stage 4.0 (TID 72)
15/06/03 19:27:38 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xDD\xF5\xC2\x00\x85,\xE4\xECc\xA7
15/06/03 19:27:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:39 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:39 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:40 INFO regionserver.HStore: Closed V
15/06/03 19:27:40 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:27:40 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:27:40 INFO executor.Executor: Finished task 11.0 in stage 4.0 (TID 65). 1857 bytes result sent to driver
15/06/03 19:27:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73
15/06/03 19:27:40 INFO executor.Executor: Running task 19.0 in stage 4.0 (TID 73)
15/06/03 19:27:40 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xECc\xA7,\xE4\xF7k\x86\x00\x85
15/06/03 19:27:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:40 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:40 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:27:40 INFO regionserver.HStore: Closed V
15/06/03 19:27:40 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:27:40 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:27:40 INFO executor.Executor: Finished task 14.0 in stage 4.0 (TID 68). 1857 bytes result sent to driver
15/06/03 19:27:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74
15/06/03 19:27:40 INFO executor.Executor: Running task 20.0 in stage 4.0 (TID 74)
15/06/03 19:27:40 INFO regionserver.HStore: Closed V
15/06/03 19:27:40 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:27:40 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:27:40 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xF7k\x86\x00\x85,\xE5\x02\xF4\x83\x00\x82
15/06/03 19:27:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:40 INFO executor.Executor: Finished task 13.0 in stage 4.0 (TID 67). 1857 bytes result sent to driver
15/06/03 19:27:40 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75
15/06/03 19:27:40 INFO executor.Executor: Running task 21.0 in stage 4.0 (TID 75)
15/06/03 19:27:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:40 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:40 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x02\xF4\x83\x00\x82,\xE5\x11ae
15/06/03 19:27:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:40 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:40 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:40 INFO regionserver.HStore: Closed V
15/06/03 19:27:40 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:27:40 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:27:40 INFO executor.Executor: Finished task 12.0 in stage 4.0 (TID 66). 1857 bytes result sent to driver
15/06/03 19:27:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76
15/06/03 19:27:40 INFO executor.Executor: Running task 22.0 in stage 4.0 (TID 76)
15/06/03 19:27:40 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x11ae,\xE5\x1F\xCB\xA1\x00\x83
15/06/03 19:27:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:40 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:40 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:41 INFO regionserver.HStore: Closed V
15/06/03 19:27:41 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:27:41 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:27:41 INFO executor.Executor: Finished task 15.0 in stage 4.0 (TID 69). 1856 bytes result sent to driver
15/06/03 19:27:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77
15/06/03 19:27:41 INFO executor.Executor: Running task 23.0 in stage 4.0 (TID 77)
15/06/03 19:27:41 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x1F\xCB\xA1\x00\x83,\xE5.:\x08
15/06/03 19:27:41 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:41 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:41 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:41 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:41 INFO regionserver.HStore: Closed V
15/06/03 19:27:41 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:27:41 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:27:41 INFO executor.Executor: Finished task 16.0 in stage 4.0 (TID 70). 1857 bytes result sent to driver
15/06/03 19:27:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
15/06/03 19:27:41 INFO executor.Executor: Running task 24.0 in stage 4.0 (TID 78)
15/06/03 19:27:41 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5.:\x08,\xE5<\xA2D\x00\x85
15/06/03 19:27:41 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:41 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:41 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:41 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:42 INFO regionserver.HStore: Closed V
15/06/03 19:27:42 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:27:42 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:27:42 INFO executor.Executor: Finished task 17.0 in stage 4.0 (TID 71). 1857 bytes result sent to driver
15/06/03 19:27:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
15/06/03 19:27:42 INFO executor.Executor: Running task 25.0 in stage 4.0 (TID 79)
15/06/03 19:27:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5<\xA2D\x00\x85,\xE5CNa\x00\x85
15/06/03 19:27:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:42 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:42 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:27:42 INFO regionserver.HStore: Closed V
15/06/03 19:27:42 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:27:42 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:27:42 INFO executor.Executor: Finished task 19.0 in stage 4.0 (TID 73). 1856 bytes result sent to driver
15/06/03 19:27:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80
15/06/03 19:27:42 INFO executor.Executor: Running task 26.0 in stage 4.0 (TID 80)
15/06/03 19:27:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5CNa\x00\x85,\xE5N\xD3\x85\x00\x85
15/06/03 19:27:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:42 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:42 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:42 INFO regionserver.HStore: Closed V
15/06/03 19:27:42 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:27:42 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:27:43 INFO executor.Executor: Finished task 18.0 in stage 4.0 (TID 72). 1856 bytes result sent to driver
15/06/03 19:27:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81
15/06/03 19:27:43 INFO executor.Executor: Running task 27.0 in stage 4.0 (TID 81)
15/06/03 19:27:43 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5N\xD3\x85\x00\x85,\xE5]:\xC7\x00\x82
15/06/03 19:27:43 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:43 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:43 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:43 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:44 INFO regionserver.HStore: Closed V
15/06/03 19:27:44 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:27:44 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:27:44 INFO executor.Executor: Finished task 20.0 in stage 4.0 (TID 74). 1857 bytes result sent to driver
15/06/03 19:27:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82
15/06/03 19:27:44 INFO executor.Executor: Running task 28.0 in stage 4.0 (TID 82)
15/06/03 19:27:44 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5]:\xC7\x00\x82,\xE5k\xA2\xE1\x00\x83
15/06/03 19:27:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:44 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:44 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:44 INFO regionserver.HStore: Closed V
15/06/03 19:27:44 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:27:44 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:27:44 INFO executor.Executor: Finished task 25.0 in stage 4.0 (TID 79). 1857 bytes result sent to driver
15/06/03 19:27:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83
15/06/03 19:27:44 INFO executor.Executor: Running task 29.0 in stage 4.0 (TID 83)
15/06/03 19:27:44 INFO regionserver.HStore: Closed V
15/06/03 19:27:44 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:27:44 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:27:44 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5k\xA2\xE1\x00\x83,\xE5z\x00G\x00\x81
15/06/03 19:27:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:44 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:44 INFO executor.Executor: Finished task 22.0 in stage 4.0 (TID 76). 1857 bytes result sent to driver
15/06/03 19:27:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84
15/06/03 19:27:44 INFO executor.Executor: Running task 30.0 in stage 4.0 (TID 84)
15/06/03 19:27:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:44 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:44 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5z\x00G\x00\x81,\xE5\x86\x8B\x06
15/06/03 19:27:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:44 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:44 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:27:44 INFO regionserver.HStore: Closed V
15/06/03 19:27:45 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:27:45 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:27:45 INFO executor.Executor: Finished task 21.0 in stage 4.0 (TID 75). 1857 bytes result sent to driver
15/06/03 19:27:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85
15/06/03 19:27:45 INFO executor.Executor: Running task 31.0 in stage 4.0 (TID 85)
15/06/03 19:27:45 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x86\x8B\x06,\xE5\x92\x0Cc\x00\x82
15/06/03 19:27:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:45 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:45 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:45 INFO regionserver.HStore: Closed V
15/06/03 19:27:45 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:27:45 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:27:45 INFO regionserver.HStore: Closed V
15/06/03 19:27:45 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:27:45 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:27:45 INFO executor.Executor: Finished task 24.0 in stage 4.0 (TID 78). 1857 bytes result sent to driver
15/06/03 19:27:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86
15/06/03 19:27:45 INFO executor.Executor: Running task 32.0 in stage 4.0 (TID 86)
15/06/03 19:27:45 INFO executor.Executor: Finished task 23.0 in stage 4.0 (TID 77). 1857 bytes result sent to driver
15/06/03 19:27:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87
15/06/03 19:27:45 INFO executor.Executor: Running task 33.0 in stage 4.0 (TID 87)
15/06/03 19:27:45 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x92\x0Cc\x00\x82,\xE5\xA0q\x06\x00\x86
15/06/03 19:27:45 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xA0q\x06\x00\x86,\xE5\xAE\xD6B\x00\x82
15/06/03 19:27:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:45 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:45 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:45 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:45 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:46 INFO regionserver.HStore: Closed V
15/06/03 19:27:46 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:27:46 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:27:46 INFO executor.Executor: Finished task 26.0 in stage 4.0 (TID 80). 2020 bytes result sent to driver
15/06/03 19:27:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88
15/06/03 19:27:46 INFO executor.Executor: Running task 34.0 in stage 4.0 (TID 88)
15/06/03 19:27:46 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xAE\xD6B\x00\x82,\xE5\xBDA`\x00\x85
15/06/03 19:27:46 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:46 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:46 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:46 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:47 INFO regionserver.HStore: Closed V
15/06/03 19:27:47 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:27:47 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:27:47 INFO executor.Executor: Finished task 27.0 in stage 4.0 (TID 81). 2019 bytes result sent to driver
15/06/03 19:27:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89
15/06/03 19:27:47 INFO executor.Executor: Running task 35.0 in stage 4.0 (TID 89)
15/06/03 19:27:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xBDA`\x00\x85,
15/06/03 19:27:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:47 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:27:47 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:27:47 INFO regionserver.HStore: Closed V
15/06/03 19:27:47 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:27:47 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:27:47 INFO executor.Executor: Finished task 28.0 in stage 4.0 (TID 82). 2020 bytes result sent to driver
15/06/03 19:27:48 INFO regionserver.HStore: Closed V
15/06/03 19:27:48 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:27:48 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:27:48 INFO executor.Executor: Finished task 30.0 in stage 4.0 (TID 84). 2020 bytes result sent to driver
15/06/03 19:27:48 INFO regionserver.HStore: Closed V
15/06/03 19:27:48 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:27:48 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:27:48 INFO executor.Executor: Finished task 29.0 in stage 4.0 (TID 83). 2020 bytes result sent to driver
15/06/03 19:27:48 INFO regionserver.HStore: Closed V
15/06/03 19:27:48 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:48 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:48 INFO executor.Executor: Finished task 31.0 in stage 4.0 (TID 85). 2019 bytes result sent to driver
15/06/03 19:27:49 INFO regionserver.HStore: Closed V
15/06/03 19:27:49 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:49 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:49 INFO regionserver.HStore: Closed V
15/06/03 19:27:49 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:49 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:49 INFO executor.Executor: Finished task 33.0 in stage 4.0 (TID 87). 2020 bytes result sent to driver
15/06/03 19:27:49 INFO executor.Executor: Finished task 32.0 in stage 4.0 (TID 86). 2019 bytes result sent to driver
15/06/03 19:27:49 INFO regionserver.HStore: Closed V
15/06/03 19:27:49 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:49 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:49 INFO executor.Executor: Finished task 34.0 in stage 4.0 (TID 88). 1856 bytes result sent to driver
15/06/03 19:27:49 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 19:27:49 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbbf2f762001a
15/06/03 19:27:49 INFO zookeeper.ZooKeeper: Session: 0x14dbbf2f762001a closed
15/06/03 19:27:49 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 19:27:49 INFO regionserver.HStore: Closed V
15/06/03 19:27:49 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:27:49 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:27:50 INFO executor.Executor: Finished task 35.0 in stage 4.0 (TID 89). 1856 bytes result sent to driver
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 90
15/06/03 19:27:50 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 90)
15/06/03 19:27:50 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
15/06/03 19:27:50 INFO storage.MemoryStore: ensureFreeSpace(8015) called with curMem=857647, maxMem=740960501
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.8 KB, free 705.8 MB)
15/06/03 19:27:50 INFO storage.BlockManagerMaster: Updated info of block broadcast_8_piece0
15/06/03 19:27:50 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 8 ms
15/06/03 19:27:50 INFO storage.MemoryStore: ensureFreeSpace(16536) called with curMem=865662, maxMem=740960501
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.1 KB, free 705.8 MB)
15/06/03 19:27:50 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 90). 721 bytes result sent to driver
15/06/03 19:27:50 INFO storage.BlockManager: Removing broadcast 6
15/06/03 19:27:50 INFO storage.BlockManager: Removing block broadcast_6_piece0
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_6_piece0 of size 24731 dropped from memory (free 740103034)
15/06/03 19:27:50 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
15/06/03 19:27:50 INFO storage.BlockManager: Removing block broadcast_6
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_6 of size 389795 dropped from memory (free 740492829)
15/06/03 19:27:50 INFO storage.BlockManager: Removing broadcast 7
15/06/03 19:27:50 INFO storage.BlockManager: Removing block broadcast_7
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_7 of size 19952 dropped from memory (free 740512781)
15/06/03 19:27:50 INFO storage.BlockManager: Removing block broadcast_7_piece0
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_7_piece0 of size 8790 dropped from memory (free 740521571)
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 91
15/06/03 19:27:50 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 91)
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 92
15/06/03 19:27:50 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 92)
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 93
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 94
15/06/03 19:27:50 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 93)
15/06/03 19:27:50 INFO executor.Executor: Running task 3.0 in stage 6.0 (TID 94)
15/06/03 19:27:50 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
15/06/03 19:27:50 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
15/06/03 19:27:50 INFO storage.BlockManager: Removing broadcast 8
15/06/03 19:27:50 INFO storage.BlockManager: Removing block broadcast_8
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_8 of size 16536 dropped from memory (free 740538107)
15/06/03 19:27:50 INFO storage.BlockManager: Removing block broadcast_8_piece0
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_8_piece0 of size 8015 dropped from memory (free 740546122)
15/06/03 19:27:50 INFO storage.MemoryStore: ensureFreeSpace(15360) called with curMem=414379, maxMem=740960501
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 15.0 KB, free 706.2 MB)
15/06/03 19:27:50 INFO storage.BlockManagerMaster: Updated info of block broadcast_8_piece0
15/06/03 19:27:50 INFO storage.BlockManagerMaster: Updated info of block broadcast_9_piece0
15/06/03 19:27:50 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 12 ms
15/06/03 19:27:50 INFO storage.MemoryStore: ensureFreeSpace(62992) called with curMem=429739, maxMem=740960501
15/06/03 19:27:50 INFO storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 61.5 KB, free 706.2 MB)
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 1.0 in stage 6.0 (TID 92)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 3.0 in stage 6.0 (TID 94)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 2.0 in stage 6.0 (TID 93)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 0.0 in stage 6.0 (TID 91)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 95
15/06/03 19:27:50 INFO executor.Executor: Running task 1.1 in stage 6.0 (TID 95)
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 96
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 97
15/06/03 19:27:50 INFO executor.Executor: Running task 2.1 in stage 6.0 (TID 96)
15/06/03 19:27:50 INFO executor.Executor: Running task 3.1 in stage 6.0 (TID 97)
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 98
15/06/03 19:27:50 INFO executor.Executor: Running task 0.1 in stage 6.0 (TID 98)
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 1.1 in stage 6.0 (TID 95)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 3.1 in stage 6.0 (TID 97)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 2.1 in stage 6.0 (TID 96)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 99
15/06/03 19:27:50 INFO executor.Executor: Running task 1.2 in stage 6.0 (TID 99)
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 100
15/06/03 19:27:50 INFO executor.Executor: Running task 3.2 in stage 6.0 (TID 100)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 0.1 in stage 6.0 (TID 98)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 101
15/06/03 19:27:50 INFO executor.Executor: Running task 2.2 in stage 6.0 (TID 101)
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 102
15/06/03 19:27:50 INFO executor.Executor: Running task 0.2 in stage 6.0 (TID 102)
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 1.2 in stage 6.0 (TID 99)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 3.2 in stage 6.0 (TID 100)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 2.2 in stage 6.0 (TID 101)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 103
15/06/03 19:27:50 INFO executor.Executor: Running task 1.3 in stage 6.0 (TID 103)
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 104
15/06/03 19:27:50 INFO executor.Executor: Running task 3.3 in stage 6.0 (TID 104)
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 0.2 in stage 6.0 (TID 102)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 105
15/06/03 19:27:50 INFO executor.Executor: Running task 2.3 in stage 6.0 (TID 105)
15/06/03 19:27:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 106
15/06/03 19:27:50 INFO executor.Executor: Running task 0.3 in stage 6.0 (TID 106)
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 1.3 in stage 6.0 (TID 103)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 19:27:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 19:27:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 19:27:50 ERROR executor.Executor: Exception in task 3.3 in stage 6.0 (TID 104)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 19:27:50 INFO executor.Executor: Executor is trying to kill task 2.3 in stage 6.0 (TID 105)
15/06/03 19:27:50 INFO executor.Executor: Executor killed task 2.3 in stage 6.0 (TID 105)
15/06/03 19:27:50 INFO executor.Executor: Executor is trying to kill task 0.3 in stage 6.0 (TID 106)
15/06/03 19:27:50 INFO executor.Executor: Executor killed task 0.3 in stage 6.0 (TID 106)
15/06/03 19:28:47 INFO storage.BlockManager: Removing broadcast 9
15/06/03 19:28:47 INFO storage.BlockManager: Removing block broadcast_9
15/06/03 19:28:47 INFO storage.MemoryStore: Block broadcast_9 of size 62992 dropped from memory (free 740530762)
15/06/03 19:28:47 INFO storage.BlockManager: Removing block broadcast_9_piece0
15/06/03 19:28:47 INFO storage.MemoryStore: Block broadcast_9_piece0 of size 15360 dropped from memory (free 740546122)
15/06/03 19:28:47 INFO storage.BlockManagerMaster: Updated info of block broadcast_9_piece0
15/06/03 19:28:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 107
15/06/03 19:28:47 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 107)
15/06/03 19:28:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 108
15/06/03 19:28:47 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 108)
15/06/03 19:28:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 109
15/06/03 19:28:47 INFO executor.Executor: Running task 2.0 in stage 7.0 (TID 109)
15/06/03 19:28:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 110
15/06/03 19:28:47 INFO executor.Executor: Running task 3.0 in stage 7.0 (TID 110)
15/06/03 19:28:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 111
15/06/03 19:28:47 INFO executor.Executor: Running task 4.0 in stage 7.0 (TID 111)
15/06/03 19:28:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 112
15/06/03 19:28:47 INFO executor.Executor: Running task 5.0 in stage 7.0 (TID 112)
15/06/03 19:28:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 113
15/06/03 19:28:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 114
15/06/03 19:28:47 INFO executor.Executor: Running task 6.0 in stage 7.0 (TID 113)
15/06/03 19:28:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12
15/06/03 19:28:47 INFO executor.Executor: Running task 7.0 in stage 7.0 (TID 114)
15/06/03 19:28:47 INFO storage.MemoryStore: ensureFreeSpace(9387) called with curMem=414379, maxMem=740960501
15/06/03 19:28:47 INFO storage.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 9.2 KB, free 706.2 MB)
15/06/03 19:28:47 INFO storage.BlockManagerMaster: Updated info of block broadcast_12_piece0
15/06/03 19:28:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 9 ms
15/06/03 19:28:47 INFO storage.MemoryStore: ensureFreeSpace(22032) called with curMem=423766, maxMem=740960501
15/06/03 19:28:47 INFO storage.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 21.5 KB, free 706.2 MB)
15/06/03 19:28:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4=\xD3\xE7,\xE4I_'\x00\x84
15/06/03 19:28:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE46\xDE!\x00\x83,\xE4=\xD3\xE7
15/06/03 19:28:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4(\x7F\xE1\x00\x85,\xE46\xDE!\x00\x83
15/06/03 19:28:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11
15/06/03 19:28:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x1A\x15\x07,\xE4(\x7F\xE1\x00\x85
15/06/03 19:28:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4W\xCF\x00\x00\x84,\xE4f9\xC6
15/06/03 19:28:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x0B\xAA\xC1\x00\x84,\xE4\x1A\x15\x07
15/06/03 19:28:47 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4I_'\x00\x84,\xE4W\xCF\x00\x00\x84
15/06/03 19:28:47 INFO rdd.NewHadoopRDD: Input split: localhost:,\xE4\x0B\xAA\xC1\x00\x84
15/06/03 19:28:47 INFO storage.MemoryStore: ensureFreeSpace(24583) called with curMem=445798, maxMem=740960501
15/06/03 19:28:47 INFO storage.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KB, free 706.2 MB)
15/06/03 19:28:47 INFO storage.BlockManagerMaster: Updated info of block broadcast_11_piece0
15/06/03 19:28:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 8 ms
15/06/03 19:28:47 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=470381, maxMem=740960501
15/06/03 19:28:47 INFO storage.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 380.7 KB, free 705.8 MB)
15/06/03 19:28:47 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x2afe0c93 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 19:28:47 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x2afe0c93, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 19:28:47 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:28:47 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:28:47 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f762001b, negotiated timeout = 60000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:28:47 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:53 INFO regionserver.HStore: Closed V
15/06/03 19:28:53 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:28:53 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:28:53 INFO executor.Executor: Finished task 4.0 in stage 7.0 (TID 111). 2043 bytes result sent to driver
15/06/03 19:28:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 115
15/06/03 19:28:53 INFO executor.Executor: Running task 8.0 in stage 7.0 (TID 115)
15/06/03 19:28:53 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4f9\xC6,\xE4t\xA2B\x00\x82
15/06/03 19:28:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:53 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:53 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:58 INFO regionserver.HStore: Closed V
15/06/03 19:28:58 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:28:58 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:28:58 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 107). 2206 bytes result sent to driver
15/06/03 19:28:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 116
15/06/03 19:28:58 INFO regionserver.HStore: Closed V
15/06/03 19:28:58 INFO executor.Executor: Running task 9.0 in stage 7.0 (TID 116)
15/06/03 19:28:58 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:28:58 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:28:58 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4t\xA2B\x00\x82,\xE4{\xA2C\x00\x83
15/06/03 19:28:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:58 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:58 INFO executor.Executor: Finished task 5.0 in stage 7.0 (TID 112). 2206 bytes result sent to driver
15/06/03 19:28:58 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:28:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 117
15/06/03 19:28:58 INFO executor.Executor: Running task 10.0 in stage 7.0 (TID 117)
15/06/03 19:28:58 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4{\xA2C\x00\x83,\xE4\x80\xCB\xE7\x00\x83
15/06/03 19:28:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:58 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 19:28:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:28:58 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 19:28:59 INFO regionserver.HStore: Closed V
15/06/03 19:28:59 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:28:59 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:28:59 INFO executor.Executor: Finished task 7.0 in stage 7.0 (TID 114). 2206 bytes result sent to driver
15/06/03 19:28:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 118
15/06/03 19:28:59 INFO executor.Executor: Running task 11.0 in stage 7.0 (TID 118)
15/06/03 19:29:00 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x80\xCB\xE7\x00\x83,\xE4\x8CK\xE5\x00\x86
15/06/03 19:29:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:00 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:29:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:00 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:29:00 INFO regionserver.HStore: Closed V
15/06/03 19:29:00 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:29:00 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:29:00 INFO regionserver.HStore: Closed V
15/06/03 19:29:00 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:29:00 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:29:01 INFO executor.Executor: Finished task 2.0 in stage 7.0 (TID 109). 2206 bytes result sent to driver
15/06/03 19:29:01 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 108). 2206 bytes result sent to driver
15/06/03 19:29:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 119
15/06/03 19:29:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 120
15/06/03 19:29:01 INFO executor.Executor: Running task 12.0 in stage 7.0 (TID 119)
15/06/03 19:29:01 INFO executor.Executor: Running task 13.0 in stage 7.0 (TID 120)
15/06/03 19:29:01 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x9A\xB2c\x00\x85,\xE4\xA7.\xA7\x00\x82
15/06/03 19:29:01 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x8CK\xE5\x00\x86,\xE4\x9A\xB2c\x00\x85
15/06/03 19:29:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:01 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:29:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:01 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:29:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:01 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:29:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:01 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:29:01 INFO regionserver.HStore: Closed V
15/06/03 19:29:01 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:29:01 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:29:01 INFO executor.Executor: Finished task 3.0 in stage 7.0 (TID 110). 2206 bytes result sent to driver
15/06/03 19:29:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 121
15/06/03 19:29:01 INFO executor.Executor: Running task 14.0 in stage 7.0 (TID 121)
15/06/03 19:29:01 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xA7.\xA7\x00\x82,\xE4\xB2\xBAC\x00\x82
15/06/03 19:29:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:01 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:01 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:01 INFO regionserver.HStore: Closed V
15/06/03 19:29:01 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:29:01 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:29:01 INFO executor.Executor: Finished task 6.0 in stage 7.0 (TID 113). 2206 bytes result sent to driver
15/06/03 19:29:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 122
15/06/03 19:29:01 INFO executor.Executor: Running task 15.0 in stage 7.0 (TID 122)
15/06/03 19:29:01 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xB2\xBAC\x00\x82,\xE4\xC1(\x85\x00\x85
15/06/03 19:29:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:01 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:01 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:03 INFO regionserver.HStore: Closed V
15/06/03 19:29:03 INFO regionserver.HRegion: Closed 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72.
15/06/03 19:29:03 WARN regionserver.HRegion: Region 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72. already closed
15/06/03 19:29:03 INFO executor.Executor: Finished task 10.0 in stage 7.0 (TID 117). 2043 bytes result sent to driver
15/06/03 19:29:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 123
15/06/03 19:29:03 INFO executor.Executor: Running task 16.0 in stage 7.0 (TID 123)
15/06/03 19:29:03 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xC1(\x85\x00\x85,\xE4\xCF\x8E\xE5\x00\x83
15/06/03 19:29:03 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:03 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:03 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:03 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:04 INFO regionserver.HStore: Closed V
15/06/03 19:29:04 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:29:04 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:29:04 INFO executor.Executor: Finished task 9.0 in stage 7.0 (TID 116). 2043 bytes result sent to driver
15/06/03 19:29:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 124
15/06/03 19:29:04 INFO executor.Executor: Running task 17.0 in stage 7.0 (TID 124)
15/06/03 19:29:04 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xCF\x8E\xE5\x00\x83,\xE4\xDD\xF5\xC2\x00\x85
15/06/03 19:29:04 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:04 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:04 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:04 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:06 INFO regionserver.HStore: Closed V
15/06/03 19:29:06 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:29:06 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:29:06 INFO executor.Executor: Finished task 8.0 in stage 7.0 (TID 115). 2206 bytes result sent to driver
15/06/03 19:29:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 125
15/06/03 19:29:06 INFO executor.Executor: Running task 18.0 in stage 7.0 (TID 125)
15/06/03 19:29:06 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xDD\xF5\xC2\x00\x85,\xE4\xECc\xA7
15/06/03 19:29:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:06 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:06 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:09 INFO regionserver.HStore: Closed V
15/06/03 19:29:09 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:29:09 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:29:09 INFO executor.Executor: Finished task 11.0 in stage 7.0 (TID 118). 2206 bytes result sent to driver
15/06/03 19:29:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 126
15/06/03 19:29:09 INFO executor.Executor: Running task 19.0 in stage 7.0 (TID 126)
15/06/03 19:29:09 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xECc\xA7,\xE4\xF7k\x86\x00\x85
15/06/03 19:29:09 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:09 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:09 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:09 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:29:10 INFO regionserver.HStore: Closed V
15/06/03 19:29:10 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:29:10 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:29:10 INFO executor.Executor: Finished task 14.0 in stage 7.0 (TID 121). 2206 bytes result sent to driver
15/06/03 19:29:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 127
15/06/03 19:29:10 INFO executor.Executor: Running task 20.0 in stage 7.0 (TID 127)
15/06/03 19:29:10 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xF7k\x86\x00\x85,\xE5\x02\xF4\x83\x00\x82
15/06/03 19:29:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:10 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:10 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:10 INFO regionserver.HStore: Closed V
15/06/03 19:29:10 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:29:10 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:29:10 INFO executor.Executor: Finished task 13.0 in stage 7.0 (TID 120). 2206 bytes result sent to driver
15/06/03 19:29:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 128
15/06/03 19:29:10 INFO executor.Executor: Running task 21.0 in stage 7.0 (TID 128)
15/06/03 19:29:10 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x02\xF4\x83\x00\x82,\xE5\x11ae
15/06/03 19:29:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:10 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:10 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:11 INFO regionserver.HStore: Closed V
15/06/03 19:29:11 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:29:11 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:29:11 INFO executor.Executor: Finished task 12.0 in stage 7.0 (TID 119). 2206 bytes result sent to driver
15/06/03 19:29:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 129
15/06/03 19:29:11 INFO executor.Executor: Running task 22.0 in stage 7.0 (TID 129)
15/06/03 19:29:11 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x11ae,\xE5\x1F\xCB\xA1\x00\x83
15/06/03 19:29:11 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:11 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:11 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:11 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:12 INFO regionserver.HStore: Closed V
15/06/03 19:29:12 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:29:12 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:29:12 INFO executor.Executor: Finished task 15.0 in stage 7.0 (TID 122). 2206 bytes result sent to driver
15/06/03 19:29:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 130
15/06/03 19:29:12 INFO executor.Executor: Running task 23.0 in stage 7.0 (TID 130)
15/06/03 19:29:12 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x1F\xCB\xA1\x00\x83,\xE5.:\x08
15/06/03 19:29:12 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:12 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:12 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:12 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:13 INFO regionserver.HStore: Closed V
15/06/03 19:29:13 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:29:13 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:29:13 INFO executor.Executor: Finished task 16.0 in stage 7.0 (TID 123). 2206 bytes result sent to driver
15/06/03 19:29:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 131
15/06/03 19:29:13 INFO executor.Executor: Running task 24.0 in stage 7.0 (TID 131)
15/06/03 19:29:13 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5.:\x08,\xE5<\xA2D\x00\x85
15/06/03 19:29:13 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:13 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:13 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:13 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:14 INFO regionserver.HStore: Closed V
15/06/03 19:29:14 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:29:14 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:29:14 INFO executor.Executor: Finished task 17.0 in stage 7.0 (TID 124). 2206 bytes result sent to driver
15/06/03 19:29:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 132
15/06/03 19:29:14 INFO executor.Executor: Running task 25.0 in stage 7.0 (TID 132)
15/06/03 19:29:14 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5<\xA2D\x00\x85,\xE5CNa\x00\x85
15/06/03 19:29:14 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:14 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:14 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:14 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:29:16 INFO regionserver.HStore: Closed V
15/06/03 19:29:16 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:29:16 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:29:16 INFO executor.Executor: Finished task 18.0 in stage 7.0 (TID 125). 2206 bytes result sent to driver
15/06/03 19:29:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 133
15/06/03 19:29:16 INFO executor.Executor: Running task 26.0 in stage 7.0 (TID 133)
15/06/03 19:29:16 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5CNa\x00\x85,\xE5N\xD3\x85\x00\x85
15/06/03 19:29:16 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:16 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:16 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:16 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:17 INFO regionserver.HStore: Closed V
15/06/03 19:29:17 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:29:17 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:29:17 INFO executor.Executor: Finished task 19.0 in stage 7.0 (TID 126). 2206 bytes result sent to driver
15/06/03 19:29:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 134
15/06/03 19:29:17 INFO executor.Executor: Running task 27.0 in stage 7.0 (TID 134)
15/06/03 19:29:17 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5N\xD3\x85\x00\x85,\xE5]:\xC7\x00\x82
15/06/03 19:29:17 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:17 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:17 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:17 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:18 INFO regionserver.HStore: Closed V
15/06/03 19:29:18 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:29:18 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:29:18 INFO executor.Executor: Finished task 20.0 in stage 7.0 (TID 127). 2206 bytes result sent to driver
15/06/03 19:29:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 135
15/06/03 19:29:18 INFO executor.Executor: Running task 28.0 in stage 7.0 (TID 135)
15/06/03 19:29:18 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5]:\xC7\x00\x82,\xE5k\xA2\xE1\x00\x83
15/06/03 19:29:18 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:18 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:18 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:18 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:19 INFO regionserver.HStore: Closed V
15/06/03 19:29:19 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:29:19 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:29:19 INFO executor.Executor: Finished task 25.0 in stage 7.0 (TID 132). 2206 bytes result sent to driver
15/06/03 19:29:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 136
15/06/03 19:29:19 INFO executor.Executor: Running task 29.0 in stage 7.0 (TID 136)
15/06/03 19:29:19 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5k\xA2\xE1\x00\x83,\xE5z\x00G\x00\x81
15/06/03 19:29:19 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:19 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:19 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:19 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:20 INFO regionserver.HStore: Closed V
15/06/03 19:29:20 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:29:20 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:29:20 INFO executor.Executor: Finished task 21.0 in stage 7.0 (TID 128). 2206 bytes result sent to driver
15/06/03 19:29:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 137
15/06/03 19:29:20 INFO executor.Executor: Running task 30.0 in stage 7.0 (TID 137)
15/06/03 19:29:20 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5z\x00G\x00\x81,\xE5\x86\x8B\x06
15/06/03 19:29:20 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:20 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:20 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:20 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:29:21 INFO regionserver.HStore: Closed V
15/06/03 19:29:21 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:29:21 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:29:21 INFO executor.Executor: Finished task 22.0 in stage 7.0 (TID 129). 2206 bytes result sent to driver
15/06/03 19:29:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 138
15/06/03 19:29:21 INFO executor.Executor: Running task 31.0 in stage 7.0 (TID 138)
15/06/03 19:29:21 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x86\x8B\x06,\xE5\x92\x0Cc\x00\x82
15/06/03 19:29:21 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:21 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:21 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:21 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:22 INFO regionserver.HStore: Closed V
15/06/03 19:29:22 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:29:22 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:29:22 INFO executor.Executor: Finished task 23.0 in stage 7.0 (TID 130). 2206 bytes result sent to driver
15/06/03 19:29:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 139
15/06/03 19:29:22 INFO executor.Executor: Running task 32.0 in stage 7.0 (TID 139)
15/06/03 19:29:22 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x92\x0Cc\x00\x82,\xE5\xA0q\x06\x00\x86
15/06/03 19:29:22 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:22 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:22 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:22 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:23 INFO regionserver.HStore: Closed V
15/06/03 19:29:23 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:29:23 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:29:23 INFO executor.Executor: Finished task 24.0 in stage 7.0 (TID 131). 2206 bytes result sent to driver
15/06/03 19:29:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 140
15/06/03 19:29:23 INFO executor.Executor: Running task 33.0 in stage 7.0 (TID 140)
15/06/03 19:29:23 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xA0q\x06\x00\x86,\xE5\xAE\xD6B\x00\x82
15/06/03 19:29:23 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:23 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:23 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:23 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:25 INFO regionserver.HStore: Closed V
15/06/03 19:29:25 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:29:25 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:29:25 INFO executor.Executor: Finished task 26.0 in stage 7.0 (TID 133). 2043 bytes result sent to driver
15/06/03 19:29:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 141
15/06/03 19:29:25 INFO executor.Executor: Running task 34.0 in stage 7.0 (TID 141)
15/06/03 19:29:25 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xAE\xD6B\x00\x82,\xE5\xBDA`\x00\x85
15/06/03 19:29:25 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:25 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:25 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:25 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:28 INFO regionserver.HStore: Closed V
15/06/03 19:29:28 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:29:28 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:29:28 INFO executor.Executor: Finished task 27.0 in stage 7.0 (TID 134). 2206 bytes result sent to driver
15/06/03 19:29:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 142
15/06/03 19:29:28 INFO executor.Executor: Running task 35.0 in stage 7.0 (TID 142)
15/06/03 19:29:28 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xBDA`\x00\x85,
15/06/03 19:29:28 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:28 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:28 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:28 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:29:28 INFO regionserver.HStore: Closed V
15/06/03 19:29:28 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:29:28 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:29:28 INFO executor.Executor: Finished task 28.0 in stage 7.0 (TID 135). 2206 bytes result sent to driver
15/06/03 19:29:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 143
15/06/03 19:29:28 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 143)
15/06/03 19:29:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
15/06/03 19:29:28 INFO storage.MemoryStore: ensureFreeSpace(9158) called with curMem=860176, maxMem=740960501
15/06/03 19:29:28 INFO storage.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.9 KB, free 705.8 MB)
15/06/03 19:29:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_13_piece0
15/06/03 19:29:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 10 ms
15/06/03 19:29:28 INFO storage.MemoryStore: ensureFreeSpace(20288) called with curMem=869334, maxMem=740960501
15/06/03 19:29:28 INFO storage.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.8 KB, free 705.8 MB)
15/06/03 19:29:28 INFO rdd.NewHadoopRDD: Input split: localhost:,\xE42Y\x08
15/06/03 19:29:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
15/06/03 19:29:28 INFO storage.MemoryStore: ensureFreeSpace(24634) called with curMem=889622, maxMem=740960501
15/06/03 19:29:28 INFO storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 24.1 KB, free 705.8 MB)
15/06/03 19:29:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_10_piece0
15/06/03 19:29:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 10 ms
15/06/03 19:29:28 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=914256, maxMem=740960501
15/06/03 19:29:28 INFO storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 380.7 KB, free 705.4 MB)
15/06/03 19:29:28 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:29 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:29:29 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:29 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:29:30 INFO regionserver.HStore: Closed V
15/06/03 19:29:30 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:29:30 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:29:30 INFO executor.Executor: Finished task 31.0 in stage 7.0 (TID 138). 2206 bytes result sent to driver
15/06/03 19:29:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 144
15/06/03 19:29:30 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 144)
15/06/03 19:29:30 INFO rdd.NewHadoopRDD: Input split: localhost:\xE42Y\x08,\xE4q \x05
15/06/03 19:29:30 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:30 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:29:30 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:30 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:29:30 INFO regionserver.HStore: Closed V
15/06/03 19:29:30 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:29:30 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:29:31 INFO executor.Executor: Finished task 30.0 in stage 7.0 (TID 137). 2206 bytes result sent to driver
15/06/03 19:29:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 145
15/06/03 19:29:31 INFO executor.Executor: Running task 2.0 in stage 8.0 (TID 145)
15/06/03 19:29:31 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4q \x05,\xE4\xAF\xEED
15/06/03 19:29:31 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:31 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:29:31 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:31 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:29:31 INFO regionserver.HStore: Closed V
15/06/03 19:29:31 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:29:31 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:29:31 INFO executor.Executor: Finished task 29.0 in stage 7.0 (TID 136). 2206 bytes result sent to driver
15/06/03 19:29:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 146
15/06/03 19:29:31 INFO executor.Executor: Running task 3.0 in stage 8.0 (TID 146)
15/06/03 19:29:31 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xAF\xEED,\xE4\xCA[\x84
15/06/03 19:29:31 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:31 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:29:31 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:31 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:29:33 INFO regionserver.HStore: Closed V
15/06/03 19:29:33 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 19:29:33 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 19:29:33 INFO executor.Executor: Finished task 3.0 in stage 8.0 (TID 146). 1993 bytes result sent to driver
15/06/03 19:29:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 147
15/06/03 19:29:33 INFO executor.Executor: Running task 4.0 in stage 8.0 (TID 147)
15/06/03 19:29:33 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xCA[\x84,\xE4\xFC\x90'
15/06/03 19:29:33 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:33 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:29:33 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:33 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:29:34 INFO regionserver.HStore: Closed V
15/06/03 19:29:34 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 19:29:34 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 19:29:34 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 143). 1993 bytes result sent to driver
15/06/03 19:29:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 148
15/06/03 19:29:34 INFO executor.Executor: Running task 5.0 in stage 8.0 (TID 148)
15/06/03 19:29:34 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xFC\x90',\xE5;h!
15/06/03 19:29:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:34 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:29:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:34 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:29:35 INFO regionserver.HStore: Closed V
15/06/03 19:29:35 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:29:35 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:29:35 INFO executor.Executor: Finished task 32.0 in stage 7.0 (TID 139). 2206 bytes result sent to driver
15/06/03 19:29:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 149
15/06/03 19:29:35 INFO executor.Executor: Running task 6.0 in stage 8.0 (TID 149)
15/06/03 19:29:35 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5;h!,\xE5z)h
15/06/03 19:29:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:35 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:29:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:35 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:29:35 INFO regionserver.HStore: Closed V
15/06/03 19:29:35 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:29:35 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:29:35 INFO executor.Executor: Finished task 33.0 in stage 7.0 (TID 140). 2206 bytes result sent to driver
15/06/03 19:29:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 150
15/06/03 19:29:35 INFO executor.Executor: Running task 7.0 in stage 8.0 (TID 150)
15/06/03 19:29:35 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5z)h,\xE5\x94\xAE\x01
15/06/03 19:29:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:35 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:29:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:35 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:29:36 INFO regionserver.HStore: Closed V
15/06/03 19:29:36 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 19:29:36 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 19:29:36 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 144). 2156 bytes result sent to driver
15/06/03 19:29:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 151
15/06/03 19:29:36 INFO executor.Executor: Running task 8.0 in stage 8.0 (TID 151)
15/06/03 19:29:36 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x94\xAE\x01,\xE5\xC6\xE6\x87
15/06/03 19:29:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:36 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 19:29:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:36 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 19:29:36 INFO regionserver.HStore: Closed V
15/06/03 19:29:36 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 19:29:36 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 19:29:36 INFO executor.Executor: Finished task 2.0 in stage 8.0 (TID 145). 2156 bytes result sent to driver
15/06/03 19:29:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 152
15/06/03 19:29:36 INFO executor.Executor: Running task 9.0 in stage 8.0 (TID 152)
15/06/03 19:29:36 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xC6\xE6\x87,
15/06/03 19:29:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:36 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 19:29:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:29:36 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 19:29:37 INFO regionserver.HStore: Closed V
15/06/03 19:29:37 INFO regionserver.HRegion: Closed 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893.
15/06/03 19:29:37 WARN regionserver.HRegion: Region 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893. already closed
15/06/03 19:29:37 INFO executor.Executor: Finished task 9.0 in stage 8.0 (TID 152). 1993 bytes result sent to driver
15/06/03 19:29:37 INFO regionserver.HStore: Closed V
15/06/03 19:29:37 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:29:37 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:29:37 INFO executor.Executor: Finished task 34.0 in stage 7.0 (TID 141). 2206 bytes result sent to driver
15/06/03 19:29:38 INFO regionserver.HStore: Closed V
15/06/03 19:29:38 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:29:38 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:29:38 INFO executor.Executor: Finished task 35.0 in stage 7.0 (TID 142). 2206 bytes result sent to driver
15/06/03 19:29:38 INFO regionserver.HStore: Closed V
15/06/03 19:29:38 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 19:29:38 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 19:29:38 INFO executor.Executor: Finished task 4.0 in stage 8.0 (TID 147). 2156 bytes result sent to driver
15/06/03 19:29:38 INFO regionserver.HStore: Closed V
15/06/03 19:29:38 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 19:29:38 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 19:29:38 INFO executor.Executor: Finished task 7.0 in stage 8.0 (TID 150). 2156 bytes result sent to driver
15/06/03 19:29:39 INFO regionserver.HStore: Closed V
15/06/03 19:29:39 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 19:29:39 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 19:29:39 INFO executor.Executor: Finished task 5.0 in stage 8.0 (TID 148). 2156 bytes result sent to driver
15/06/03 19:29:39 INFO regionserver.HStore: Closed V
15/06/03 19:29:39 INFO regionserver.HRegion: Closed 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893.
15/06/03 19:29:39 WARN regionserver.HRegion: Region 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893. already closed
15/06/03 19:29:39 INFO executor.Executor: Finished task 8.0 in stage 8.0 (TID 151). 1993 bytes result sent to driver
15/06/03 19:29:39 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 19:29:39 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbbf2f762001b
15/06/03 19:29:39 INFO zookeeper.ZooKeeper: Session: 0x14dbbf2f762001b closed
15/06/03 19:29:39 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 19:29:40 INFO regionserver.HStore: Closed V
15/06/03 19:29:40 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 19:29:40 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 19:29:40 INFO executor.Executor: Finished task 6.0 in stage 8.0 (TID 149). 2156 bytes result sent to driver
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 153
15/06/03 19:29:40 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 153)
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 154
15/06/03 19:29:40 INFO executor.Executor: Running task 1.0 in stage 9.0 (TID 154)
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 155
15/06/03 19:29:40 INFO executor.Executor: Running task 2.0 in stage 9.0 (TID 155)
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 156
15/06/03 19:29:40 INFO executor.Executor: Running task 3.0 in stage 9.0 (TID 156)
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 157
15/06/03 19:29:40 INFO executor.Executor: Running task 4.0 in stage 9.0 (TID 157)
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 158
15/06/03 19:29:40 INFO executor.Executor: Running task 5.0 in stage 9.0 (TID 158)
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 159
15/06/03 19:29:40 INFO executor.Executor: Running task 6.0 in stage 9.0 (TID 159)
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 160
15/06/03 19:29:40 INFO executor.Executor: Running task 7.0 in stage 9.0 (TID 160)
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
15/06/03 19:29:40 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
15/06/03 19:29:40 INFO storage.MemoryStore: ensureFreeSpace(9699) called with curMem=1304051, maxMem=740960501
15/06/03 19:29:40 INFO storage.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.5 KB, free 705.4 MB)
15/06/03 19:29:40 INFO storage.BlockManagerMaster: Updated info of block broadcast_14_piece0
15/06/03 19:29:40 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 7 ms
15/06/03 19:29:40 INFO storage.MemoryStore: ensureFreeSpace(23080) called with curMem=1313750, maxMem=740960501
15/06/03 19:29:40 INFO storage.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.5 KB, free 705.4 MB)
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@10.0.1.65:65270/user/MapOutputTracker#599918699]
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@10.0.1.65:65270/user/MapOutputTracker#599918699]
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 19:29:40 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:40 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 32768, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:40 ERROR executor.Executor: Exception in task 4.0 in stage 9.0 (TID 157)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 161
15/06/03 19:29:40 INFO executor.Executor: Running task 8.0 in stage 9.0 (TID 161)
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:40 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 276065, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 2.0 in stage 9.0 (TID 155)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 162
15/06/03 19:29:41 INFO executor.Executor: Running task 4.1 in stage 9.0 (TID 162)
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 405794, 1971-12-23, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 5.0 in stage 9.0 (TID 158)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 163
15/06/03 19:29:41 INFO executor.Executor: Running task 2.1 in stage 9.0 (TID 163)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 544581, 1994-06-11, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 6.0 in stage 9.0 (TID 159)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 566304, 1994-02-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 3.0 in stage 9.0 (TID 156)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 164
15/06/03 19:29:41 INFO executor.Executor: Running task 5.1 in stage 9.0 (TID 164)
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 165
15/06/03 19:29:41 INFO executor.Executor: Running task 6.1 in stage 9.0 (TID 165)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 507394, 1970-01-19, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 1.0 in stage 9.0 (TID 154)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 559744, 1992-10-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 7.0 in stage 9.0 (TID 160)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 166
15/06/03 19:29:41 INFO executor.Executor: Running task 3.1 in stage 9.0 (TID 166)
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 167
15/06/03 19:29:41 INFO executor.Executor: Running task 1.1 in stage 9.0 (TID 167)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 571431, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 0.0 in stage 9.0 (TID 153)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 168
15/06/03 19:29:41 INFO executor.Executor: Running task 7.1 in stage 9.0 (TID 168)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { -323606302, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 8.0 in stage 9.0 (TID 161)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 169
15/06/03 19:29:41 INFO executor.Executor: Running task 0.1 in stage 9.0 (TID 169)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 32768, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 4.1 in stage 9.0 (TID 162)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 170
15/06/03 19:29:41 INFO executor.Executor: Running task 8.1 in stage 9.0 (TID 170)
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 276065, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 2.1 in stage 9.0 (TID 163)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 171
15/06/03 19:29:41 INFO executor.Executor: Running task 4.2 in stage 9.0 (TID 171)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 405794, 1971-12-23, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 5.1 in stage 9.0 (TID 164)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 172
15/06/03 19:29:41 INFO executor.Executor: Running task 2.2 in stage 9.0 (TID 172)
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 544581, 1994-06-11, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 507394, 1970-01-19, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 1.1 in stage 9.0 (TID 167)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 6.1 in stage 9.0 (TID 165)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 173
15/06/03 19:29:41 INFO executor.Executor: Running task 5.2 in stage 9.0 (TID 173)
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 174
15/06/03 19:29:41 INFO executor.Executor: Running task 1.2 in stage 9.0 (TID 174)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 566304, 1994-02-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 3.1 in stage 9.0 (TID 166)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 175
15/06/03 19:29:41 INFO executor.Executor: Running task 6.2 in stage 9.0 (TID 175)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 559744, 1992-10-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 7.1 in stage 9.0 (TID 168)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 176
15/06/03 19:29:41 INFO executor.Executor: Running task 3.2 in stage 9.0 (TID 176)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 571431, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 0.1 in stage 9.0 (TID 169)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 177
15/06/03 19:29:41 INFO executor.Executor: Running task 7.2 in stage 9.0 (TID 177)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { -323606302, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 8.1 in stage 9.0 (TID 170)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 178
15/06/03 19:29:41 INFO executor.Executor: Running task 0.2 in stage 9.0 (TID 178)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 32768, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 4.2 in stage 9.0 (TID 171)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 179
15/06/03 19:29:41 INFO executor.Executor: Running task 8.2 in stage 9.0 (TID 179)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 276065, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 2.2 in stage 9.0 (TID 172)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 180
15/06/03 19:29:41 INFO executor.Executor: Running task 4.3 in stage 9.0 (TID 180)
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 405794, 1971-12-23, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 5.2 in stage 9.0 (TID 173)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 181
15/06/03 19:29:41 INFO executor.Executor: Running task 2.3 in stage 9.0 (TID 181)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 507394, 1970-01-19, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 1.2 in stage 9.0 (TID 174)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 182
15/06/03 19:29:41 INFO executor.Executor: Running task 5.3 in stage 9.0 (TID 182)
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 544581, 1994-06-11, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 6.2 in stage 9.0 (TID 175)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 566304, 1994-02-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 183
15/06/03 19:29:41 INFO executor.Executor: Running task 1.3 in stage 9.0 (TID 183)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 3.2 in stage 9.0 (TID 176)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { -323606302, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 8.2 in stage 9.0 (TID 179)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 184
15/06/03 19:29:41 INFO executor.Executor: Running task 6.3 in stage 9.0 (TID 184)
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 185
15/06/03 19:29:41 INFO executor.Executor: Running task 3.3 in stage 9.0 (TID 185)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 559744, 1992-10-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 7.2 in stage 9.0 (TID 177)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 186
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 571431, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 0.2 in stage 9.0 (TID 178)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.Executor: Running task 8.3 in stage 9.0 (TID 186)
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 187
15/06/03 19:29:41 INFO executor.Executor: Running task 7.3 in stage 9.0 (TID 187)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:29:41 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 32768, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:29:41 ERROR executor.Executor: Exception in task 4.3 in stage 9.0 (TID 180)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 19:29:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 188
15/06/03 19:29:41 INFO executor.Executor: Running task 0.3 in stage 9.0 (TID 188)
15/06/03 19:29:41 INFO executor.Executor: Executor is trying to kill task 8.3 in stage 9.0 (TID 186)
15/06/03 19:29:41 INFO executor.Executor: Executor is trying to kill task 1.3 in stage 9.0 (TID 183)
15/06/03 19:29:41 INFO executor.Executor: Executor is trying to kill task 7.3 in stage 9.0 (TID 187)
15/06/03 19:29:41 INFO executor.Executor: Executor is trying to kill task 6.3 in stage 9.0 (TID 184)
15/06/03 19:29:41 INFO executor.Executor: Executor is trying to kill task 0.3 in stage 9.0 (TID 188)
15/06/03 19:29:41 INFO executor.Executor: Executor is trying to kill task 2.3 in stage 9.0 (TID 181)
15/06/03 19:29:41 INFO executor.Executor: Executor killed task 6.3 in stage 9.0 (TID 184)
15/06/03 19:29:41 INFO executor.Executor: Executor is trying to kill task 3.3 in stage 9.0 (TID 185)
15/06/03 19:29:41 INFO executor.Executor: Executor is trying to kill task 5.3 in stage 9.0 (TID 182)
15/06/03 19:29:41 INFO executor.Executor: Executor killed task 7.3 in stage 9.0 (TID 187)
15/06/03 19:29:41 INFO executor.Executor: Executor killed task 3.3 in stage 9.0 (TID 185)
15/06/03 19:29:41 INFO executor.Executor: Executor killed task 8.3 in stage 9.0 (TID 186)
15/06/03 19:29:41 INFO executor.Executor: Executor killed task 1.3 in stage 9.0 (TID 183)
15/06/03 19:29:41 INFO executor.Executor: Executor killed task 2.3 in stage 9.0 (TID 181)
15/06/03 19:29:41 INFO executor.Executor: Executor killed task 5.3 in stage 9.0 (TID 182)
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:29:41 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:29:41 INFO executor.Executor: Executor killed task 0.3 in stage 9.0 (TID 188)
15/06/03 19:47:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 189
15/06/03 19:47:26 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 189)
15/06/03 19:47:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
15/06/03 19:47:26 INFO storage.MemoryStore: ensureFreeSpace(10616) called with curMem=1336830, maxMem=740960501
15/06/03 19:47:26 INFO storage.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.4 KB, free 705.3 MB)
15/06/03 19:47:26 INFO storage.BlockManagerMaster: Updated info of block broadcast_17_piece0
15/06/03 19:47:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 12 ms
15/06/03 19:47:26 INFO storage.MemoryStore: ensureFreeSpace(25440) called with curMem=1347446, maxMem=740960501
15/06/03 19:47:26 INFO storage.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 24.8 KB, free 705.3 MB)
15/06/03 19:47:26 INFO rdd.NewHadoopRDD: Input split: localhost:,
15/06/03 19:47:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
15/06/03 19:47:26 INFO storage.MemoryStore: ensureFreeSpace(24495) called with curMem=1372886, maxMem=740960501
15/06/03 19:47:26 INFO storage.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.9 KB, free 705.3 MB)
15/06/03 19:47:26 INFO storage.BlockManagerMaster: Updated info of block broadcast_16_piece0
15/06/03 19:47:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 7 ms
15/06/03 19:47:26 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=1397381, maxMem=740960501
15/06/03 19:47:26 INFO storage.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 380.7 KB, free 704.9 MB)
15/06/03 19:47:26 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7b4c058 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 19:47:26 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x7b4c058, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 19:47:26 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:47:26 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:47:26 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f762001c, negotiated timeout = 60000
15/06/03 19:47:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:26 INFO regionserver.HRegion: Onlined 7faeee339b2231d747e63345d70c6bfc; next sequenceid=156
15/06/03 19:47:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:26 INFO regionserver.HRegion: Onlined 7faeee339b2231d747e63345d70c6bfc; next sequenceid=156
15/06/03 19:47:29 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 19:47:29 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbbf2f762001c
15/06/03 19:47:29 INFO zookeeper.ZooKeeper: Session: 0x14dbbf2f762001c closed
15/06/03 19:47:29 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 19:47:29 INFO regionserver.HStore: Closed V
15/06/03 19:47:29 INFO regionserver.HRegion: Closed 1392,,1433352630566.7faeee339b2231d747e63345d70c6bfc.
15/06/03 19:47:29 WARN regionserver.HRegion: Region 1392,,1433352630566.7faeee339b2231d747e63345d70c6bfc. already closed
15/06/03 19:47:31 INFO storage.MemoryStore: ensureFreeSpace(23093764) called with curMem=1787176, maxMem=740960501
15/06/03 19:47:31 INFO storage.MemoryStore: Block taskresult_189 stored as bytes in memory (estimated size 22.0 MB, free 682.9 MB)
15/06/03 19:47:31 INFO storage.BlockManagerMaster: Updated info of block taskresult_189
15/06/03 19:47:31 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 189). 23093764 bytes result sent via BlockManager)
15/06/03 19:47:31 INFO storage.BlockManager: Removing block taskresult_189
15/06/03 19:47:31 INFO storage.MemoryStore: Block taskresult_189 of size 23093764 dropped from memory (free 739173325)
15/06/03 19:47:31 INFO storage.BlockManagerMaster: Updated info of block taskresult_189
15/06/03 19:47:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 190
15/06/03 19:47:33 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 190)
15/06/03 19:47:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22
15/06/03 19:47:33 INFO storage.MemoryStore: ensureFreeSpace(10976) called with curMem=1787176, maxMem=740960501
15/06/03 19:47:33 INFO storage.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 10.7 KB, free 704.9 MB)
15/06/03 19:47:33 INFO storage.BlockManagerMaster: Updated info of block broadcast_22_piece0
15/06/03 19:47:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 9 ms
15/06/03 19:47:33 INFO storage.MemoryStore: ensureFreeSpace(26360) called with curMem=1798152, maxMem=740960501
15/06/03 19:47:33 INFO storage.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 25.7 KB, free 704.9 MB)
15/06/03 19:47:33 INFO rdd.NewHadoopRDD: Input split: localhost:,
15/06/03 19:47:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
15/06/03 19:47:33 INFO storage.MemoryStore: ensureFreeSpace(24483) called with curMem=1824512, maxMem=740960501
15/06/03 19:47:33 INFO storage.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 23.9 KB, free 704.9 MB)
15/06/03 19:47:33 INFO storage.BlockManagerMaster: Updated info of block broadcast_21_piece0
15/06/03 19:47:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 6 ms
15/06/03 19:47:33 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=1848995, maxMem=740960501
15/06/03 19:47:33 INFO storage.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 380.7 KB, free 704.5 MB)
15/06/03 19:47:33 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1b5a8a08 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 19:47:33 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x1b5a8a08, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 19:47:33 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:47:33 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:47:33 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f762001d, negotiated timeout = 60000
15/06/03 19:47:33 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:33 INFO regionserver.HRegion: Onlined d65bd0417f9cf02dd3f1226e5df1e576; next sequenceid=4
15/06/03 19:47:33 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:33 INFO regionserver.HRegion: Onlined d65bd0417f9cf02dd3f1226e5df1e576; next sequenceid=4
15/06/03 19:47:33 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 19:47:33 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbbf2f762001d
15/06/03 19:47:33 INFO zookeeper.ZooKeeper: Session: 0x14dbbf2f762001d closed
15/06/03 19:47:33 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 19:47:33 INFO regionserver.HStore: Closed V
15/06/03 19:47:33 INFO regionserver.HRegion: Closed 1472,,1433352632097.d65bd0417f9cf02dd3f1226e5df1e576.
15/06/03 19:47:33 WARN regionserver.HRegion: Region 1472,,1433352632097.d65bd0417f9cf02dd3f1226e5df1e576. already closed
15/06/03 19:47:33 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 190). 2494 bytes result sent to driver
15/06/03 19:47:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 191
15/06/03 19:47:33 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 191)
15/06/03 19:47:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
15/06/03 19:47:33 INFO storage.MemoryStore: ensureFreeSpace(11035) called with curMem=2238790, maxMem=740960501
15/06/03 19:47:33 INFO storage.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.8 KB, free 704.5 MB)
15/06/03 19:47:33 INFO storage.BlockManagerMaster: Updated info of block broadcast_25_piece0
15/06/03 19:47:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 14 ms
15/06/03 19:47:33 INFO storage.MemoryStore: ensureFreeSpace(26616) called with curMem=2249825, maxMem=740960501
15/06/03 19:47:33 INFO storage.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 26.0 KB, free 704.5 MB)
15/06/03 19:47:33 INFO rdd.NewHadoopRDD: Input split: localhost:,
15/06/03 19:47:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
15/06/03 19:47:33 INFO storage.MemoryStore: ensureFreeSpace(24483) called with curMem=2276441, maxMem=740960501
15/06/03 19:47:33 INFO storage.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 23.9 KB, free 704.4 MB)
15/06/03 19:47:33 INFO storage.BlockManagerMaster: Updated info of block broadcast_24_piece0
15/06/03 19:47:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 11 ms
15/06/03 19:47:33 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=2300924, maxMem=740960501
15/06/03 19:47:33 INFO storage.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 380.7 KB, free 704.1 MB)
15/06/03 19:47:33 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x4a4fb546 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 19:47:33 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x4a4fb546, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 19:47:33 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:47:33 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:47:33 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f762001e, negotiated timeout = 60000
15/06/03 19:47:33 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:33 INFO regionserver.HRegion: Onlined d65bd0417f9cf02dd3f1226e5df1e576; next sequenceid=4
15/06/03 19:47:33 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:33 INFO regionserver.HRegion: Onlined d65bd0417f9cf02dd3f1226e5df1e576; next sequenceid=4
15/06/03 19:47:33 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 19:47:33 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbbf2f762001e
15/06/03 19:47:33 INFO zookeeper.ZooKeeper: Session: 0x14dbbf2f762001e closed
15/06/03 19:47:33 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 19:47:34 INFO regionserver.HStore: Closed V
15/06/03 19:47:34 INFO regionserver.HRegion: Closed 1472,,1433352632097.d65bd0417f9cf02dd3f1226e5df1e576.
15/06/03 19:47:34 WARN regionserver.HRegion: Region 1472,,1433352632097.d65bd0417f9cf02dd3f1226e5df1e576. already closed
15/06/03 19:47:34 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 191). 2494 bytes result sent to driver
15/06/03 19:47:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 192
15/06/03 19:47:34 INFO executor.Executor: Running task 0.0 in stage 16.0 (TID 192)
15/06/03 19:47:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
15/06/03 19:47:34 INFO storage.MemoryStore: ensureFreeSpace(11109) called with curMem=2690719, maxMem=740960501
15/06/03 19:47:34 INFO storage.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.8 KB, free 704.1 MB)
15/06/03 19:47:34 INFO storage.BlockManagerMaster: Updated info of block broadcast_27_piece0
15/06/03 19:47:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 9 ms
15/06/03 19:47:34 INFO storage.MemoryStore: ensureFreeSpace(26528) called with curMem=2701828, maxMem=740960501
15/06/03 19:47:34 INFO storage.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 25.9 KB, free 704.0 MB)
15/06/03 19:47:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 193
15/06/03 19:47:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 194
15/06/03 19:47:34 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 193)
15/06/03 19:47:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 195
15/06/03 19:47:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
15/06/03 19:47:34 INFO executor.Executor: Running task 1.0 in stage 14.0 (TID 194)
15/06/03 19:47:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 196
15/06/03 19:47:34 INFO executor.Executor: Running task 2.0 in stage 14.0 (TID 195)
15/06/03 19:47:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 197
15/06/03 19:47:34 INFO executor.Executor: Running task 3.0 in stage 14.0 (TID 196)
15/06/03 19:47:34 INFO executor.Executor: Running task 4.0 in stage 14.0 (TID 197)
15/06/03 19:47:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 198
15/06/03 19:47:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 199
15/06/03 19:47:34 INFO executor.Executor: Running task 5.0 in stage 14.0 (TID 198)
15/06/03 19:47:34 INFO rdd.NewHadoopRDD: Input split: localhost:,
15/06/03 19:47:34 INFO executor.Executor: Running task 6.0 in stage 14.0 (TID 199)
15/06/03 19:47:34 INFO storage.MemoryStore: ensureFreeSpace(11022) called with curMem=2728356, maxMem=740960501
15/06/03 19:47:34 INFO storage.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 10.8 KB, free 704.0 MB)
15/06/03 19:47:34 INFO storage.BlockManagerMaster: Updated info of block broadcast_28_piece0
15/06/03 19:47:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 9 ms
15/06/03 19:47:34 INFO storage.MemoryStore: ensureFreeSpace(26240) called with curMem=2739378, maxMem=740960501
15/06/03 19:47:34 INFO storage.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 25.6 KB, free 704.0 MB)
15/06/03 19:47:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
15/06/03 19:47:34 INFO storage.MemoryStore: ensureFreeSpace(24493) called with curMem=2765618, maxMem=740960501
15/06/03 19:47:34 INFO storage.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 23.9 KB, free 704.0 MB)
15/06/03 19:47:34 INFO storage.BlockManagerMaster: Updated info of block broadcast_20_piece0
15/06/03 19:47:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 8 ms
15/06/03 19:47:34 INFO rdd.NewHadoopRDD: Input split: localhost:,\xE4\x0B\xAA\xC1\x00\x84
15/06/03 19:47:34 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4I_'\x00\x84,\xE4W\xCF\x00\x00\x84
15/06/03 19:47:34 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=2790111, maxMem=740960501
15/06/03 19:47:34 INFO storage.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 380.7 KB, free 703.6 MB)
15/06/03 19:47:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
15/06/03 19:47:34 INFO storage.MemoryStore: ensureFreeSpace(24658) called with curMem=3179906, maxMem=740960501
15/06/03 19:47:34 INFO storage.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 24.1 KB, free 703.6 MB)
15/06/03 19:47:34 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x1A\x15\x07,\xE4(\x7F\xE1\x00\x85
15/06/03 19:47:34 INFO storage.BlockManagerMaster: Updated info of block broadcast_19_piece0
15/06/03 19:47:34 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x0B\xAA\xC1\x00\x84,\xE4\x1A\x15\x07
15/06/03 19:47:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 10 ms
15/06/03 19:47:34 INFO rdd.NewHadoopRDD: Input split: localhost:\xE46\xDE!\x00\x83,\xE4=\xD3\xE7
15/06/03 19:47:34 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4=\xD3\xE7,\xE4I_'\x00\x84
15/06/03 19:47:34 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4(\x7F\xE1\x00\x85,\xE46\xDE!\x00\x83
15/06/03 19:47:34 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x3e3e6f7c connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 19:47:34 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x3e3e6f7c, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 19:47:34 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 19:47:34 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 19:47:34 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbbf2f762001f, negotiated timeout = 60000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined 12bec0d4e38b2e121553db95fd50c0e4; next sequenceid=13
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined 12bec0d4e38b2e121553db95fd50c0e4; next sequenceid=13
15/06/03 19:47:34 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=3204564, maxMem=740960501
15/06/03 19:47:34 INFO storage.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 380.7 KB, free 703.2 MB)
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:34 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 19:47:35 INFO regionserver.HStore: Closed V
15/06/03 19:47:35 INFO regionserver.HRegion: Closed 1424,,1433352631192.12bec0d4e38b2e121553db95fd50c0e4.
15/06/03 19:47:35 WARN regionserver.HRegion: Region 1424,,1433352631192.12bec0d4e38b2e121553db95fd50c0e4. already closed
15/06/03 19:47:35 INFO executor.Executor: Finished task 0.0 in stage 16.0 (TID 192). 1993 bytes result sent to driver
15/06/03 19:47:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 200
15/06/03 19:47:35 INFO executor.Executor: Running task 7.0 in stage 14.0 (TID 200)
15/06/03 19:47:35 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4W\xCF\x00\x00\x84,\xE4f9\xC6
15/06/03 19:47:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:35 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:35 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:38 INFO storage.BlockManager: Removing broadcast 27
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_27
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_27 of size 26528 dropped from memory (free 737392670)
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_27_piece0
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_27_piece0 of size 11109 dropped from memory (free 737403779)
15/06/03 19:47:38 INFO storage.BlockManagerMaster: Updated info of block broadcast_27_piece0
15/06/03 19:47:38 INFO storage.BlockManager: Removing broadcast 25
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_25_piece0
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_25_piece0 of size 11035 dropped from memory (free 737414814)
15/06/03 19:47:38 INFO storage.BlockManagerMaster: Updated info of block broadcast_25_piece0
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_25
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_25 of size 26616 dropped from memory (free 737441430)
15/06/03 19:47:38 INFO storage.BlockManager: Removing broadcast 24
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_24_piece0
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_24_piece0 of size 24483 dropped from memory (free 737465913)
15/06/03 19:47:38 INFO storage.BlockManagerMaster: Updated info of block broadcast_24_piece0
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_24
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_24 of size 389795 dropped from memory (free 737855708)
15/06/03 19:47:38 INFO storage.BlockManager: Removing broadcast 22
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_22_piece0
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_22_piece0 of size 10976 dropped from memory (free 737866684)
15/06/03 19:47:38 INFO storage.BlockManagerMaster: Updated info of block broadcast_22_piece0
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_22
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_22 of size 26360 dropped from memory (free 737893044)
15/06/03 19:47:38 INFO storage.BlockManager: Removing broadcast 21
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_21_piece0
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_21_piece0 of size 24483 dropped from memory (free 737917527)
15/06/03 19:47:38 INFO storage.BlockManagerMaster: Updated info of block broadcast_21_piece0
15/06/03 19:47:38 INFO storage.BlockManager: Removing block broadcast_21
15/06/03 19:47:38 INFO storage.MemoryStore: Block broadcast_21 of size 389795 dropped from memory (free 738307322)
15/06/03 19:47:38 INFO regionserver.HStore: Closed V
15/06/03 19:47:38 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:47:38 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:47:39 INFO executor.Executor: Finished task 4.0 in stage 14.0 (TID 197). 2156 bytes result sent to driver
15/06/03 19:47:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 201
15/06/03 19:47:39 INFO executor.Executor: Running task 8.0 in stage 14.0 (TID 201)
15/06/03 19:47:39 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4f9\xC6,\xE4t\xA2B\x00\x82
15/06/03 19:47:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:39 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:39 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:42 INFO regionserver.HStore: Closed V
15/06/03 19:47:42 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:47:42 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:47:42 INFO regionserver.HStore: Closed V
15/06/03 19:47:42 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:47:42 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:47:42 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 193). 2156 bytes result sent to driver
15/06/03 19:47:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 202
15/06/03 19:47:42 INFO executor.Executor: Running task 9.0 in stage 14.0 (TID 202)
15/06/03 19:47:42 INFO executor.Executor: Finished task 5.0 in stage 14.0 (TID 198). 2156 bytes result sent to driver
15/06/03 19:47:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 203
15/06/03 19:47:42 INFO executor.Executor: Running task 10.0 in stage 14.0 (TID 203)
15/06/03 19:47:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4{\xA2C\x00\x83,\xE4\x80\xCB\xE7\x00\x83
15/06/03 19:47:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:42 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4t\xA2B\x00\x82,\xE4{\xA2C\x00\x83
15/06/03 19:47:42 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 19:47:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:42 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:42 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 19:47:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:42 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 19:47:43 INFO regionserver.HStore: Closed V
15/06/03 19:47:43 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:47:43 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:47:43 INFO executor.Executor: Finished task 2.0 in stage 14.0 (TID 195). 2156 bytes result sent to driver
15/06/03 19:47:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 204
15/06/03 19:47:43 INFO executor.Executor: Running task 11.0 in stage 14.0 (TID 204)
15/06/03 19:47:43 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x80\xCB\xE7\x00\x83,\xE4\x8CK\xE5\x00\x86
15/06/03 19:47:43 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:43 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:47:43 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:43 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:47:43 INFO regionserver.HStore: Closed V
15/06/03 19:47:43 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:47:43 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:47:43 INFO executor.Executor: Finished task 6.0 in stage 14.0 (TID 199). 2156 bytes result sent to driver
15/06/03 19:47:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 205
15/06/03 19:47:43 INFO executor.Executor: Running task 12.0 in stage 14.0 (TID 205)
15/06/03 19:47:43 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x8CK\xE5\x00\x86,\xE4\x9A\xB2c\x00\x85
15/06/03 19:47:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:44 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:47:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:44 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:47:44 INFO regionserver.HStore: Closed V
15/06/03 19:47:44 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:47:44 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:47:44 INFO executor.Executor: Finished task 3.0 in stage 14.0 (TID 196). 2156 bytes result sent to driver
15/06/03 19:47:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 206
15/06/03 19:47:44 INFO executor.Executor: Running task 13.0 in stage 14.0 (TID 206)
15/06/03 19:47:44 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\x9A\xB2c\x00\x85,\xE4\xA7.\xA7\x00\x82
15/06/03 19:47:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:44 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:47:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:44 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 19:47:44 INFO regionserver.HStore: Closed V
15/06/03 19:47:44 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 19:47:44 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 19:47:44 INFO executor.Executor: Finished task 1.0 in stage 14.0 (TID 194). 2156 bytes result sent to driver
15/06/03 19:47:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 207
15/06/03 19:47:44 INFO executor.Executor: Running task 14.0 in stage 14.0 (TID 207)
15/06/03 19:47:44 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xA7.\xA7\x00\x82,\xE4\xB2\xBAC\x00\x82
15/06/03 19:47:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:44 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:44 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:44 INFO regionserver.HStore: Closed V
15/06/03 19:47:44 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:47:44 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:47:44 INFO executor.Executor: Finished task 7.0 in stage 14.0 (TID 200). 2156 bytes result sent to driver
15/06/03 19:47:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 208
15/06/03 19:47:45 INFO executor.Executor: Running task 15.0 in stage 14.0 (TID 208)
15/06/03 19:47:45 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xB2\xBAC\x00\x82,\xE4\xC1(\x85\x00\x85
15/06/03 19:47:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:45 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:45 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:45 INFO regionserver.HStore: Closed V
15/06/03 19:47:45 INFO regionserver.HRegion: Closed 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72.
15/06/03 19:47:45 WARN regionserver.HRegion: Region 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72. already closed
15/06/03 19:47:45 INFO executor.Executor: Finished task 10.0 in stage 14.0 (TID 203). 1993 bytes result sent to driver
15/06/03 19:47:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 209
15/06/03 19:47:45 INFO executor.Executor: Running task 16.0 in stage 14.0 (TID 209)
15/06/03 19:47:45 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xC1(\x85\x00\x85,\xE4\xCF\x8E\xE5\x00\x83
15/06/03 19:47:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:45 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:45 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:45 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:46 INFO regionserver.HStore: Closed V
15/06/03 19:47:46 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:47:46 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:47:46 INFO executor.Executor: Finished task 9.0 in stage 14.0 (TID 202). 1993 bytes result sent to driver
15/06/03 19:47:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 210
15/06/03 19:47:46 INFO executor.Executor: Running task 17.0 in stage 14.0 (TID 210)
15/06/03 19:47:46 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xCF\x8E\xE5\x00\x83,\xE4\xDD\xF5\xC2\x00\x85
15/06/03 19:47:46 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:46 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:46 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:46 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:47 INFO regionserver.HStore: Closed V
15/06/03 19:47:47 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 19:47:47 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 19:47:48 INFO executor.Executor: Finished task 8.0 in stage 14.0 (TID 201). 2156 bytes result sent to driver
15/06/03 19:47:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 211
15/06/03 19:47:48 INFO executor.Executor: Running task 18.0 in stage 14.0 (TID 211)
15/06/03 19:47:48 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xDD\xF5\xC2\x00\x85,\xE4\xECc\xA7
15/06/03 19:47:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:48 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:48 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:49 INFO regionserver.HStore: Closed V
15/06/03 19:47:49 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:47:49 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:47:50 INFO executor.Executor: Finished task 11.0 in stage 14.0 (TID 204). 2156 bytes result sent to driver
15/06/03 19:47:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 212
15/06/03 19:47:50 INFO executor.Executor: Running task 19.0 in stage 14.0 (TID 212)
15/06/03 19:47:50 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xECc\xA7,\xE4\xF7k\x86\x00\x85
15/06/03 19:47:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:50 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:50 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 19:47:51 INFO regionserver.HStore: Closed V
15/06/03 19:47:51 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:47:51 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:47:51 INFO regionserver.HStore: Closed V
15/06/03 19:47:51 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:47:51 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:47:51 INFO executor.Executor: Finished task 14.0 in stage 14.0 (TID 207). 2156 bytes result sent to driver
15/06/03 19:47:51 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 213
15/06/03 19:47:51 INFO executor.Executor: Running task 20.0 in stage 14.0 (TID 213)
15/06/03 19:47:51 INFO executor.Executor: Finished task 13.0 in stage 14.0 (TID 206). 2156 bytes result sent to driver
15/06/03 19:47:51 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xF7k\x86\x00\x85,\xE5\x02\xF4\x83\x00\x82
15/06/03 19:47:51 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 214
15/06/03 19:47:51 INFO executor.Executor: Running task 21.0 in stage 14.0 (TID 214)
15/06/03 19:47:51 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:51 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:51 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x02\xF4\x83\x00\x82,\xE5\x11ae
15/06/03 19:47:51 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:51 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:51 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:51 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:51 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:51 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:51 INFO regionserver.HStore: Closed V
15/06/03 19:47:51 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 19:47:51 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 19:47:52 INFO executor.Executor: Finished task 12.0 in stage 14.0 (TID 205). 2156 bytes result sent to driver
15/06/03 19:47:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 215
15/06/03 19:47:52 INFO executor.Executor: Running task 22.0 in stage 14.0 (TID 215)
15/06/03 19:47:52 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x11ae,\xE5\x1F\xCB\xA1\x00\x83
15/06/03 19:47:52 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:52 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:52 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:52 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:52 INFO regionserver.HStore: Closed V
15/06/03 19:47:52 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:47:52 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:47:53 INFO executor.Executor: Finished task 16.0 in stage 14.0 (TID 209). 2156 bytes result sent to driver
15/06/03 19:47:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 216
15/06/03 19:47:53 INFO executor.Executor: Running task 23.0 in stage 14.0 (TID 216)
15/06/03 19:47:53 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x1F\xCB\xA1\x00\x83,\xE5.:\x08
15/06/03 19:47:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:53 INFO regionserver.HStore: Closed V
15/06/03 19:47:53 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:47:53 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:47:53 INFO executor.Executor: Finished task 15.0 in stage 14.0 (TID 208). 2156 bytes result sent to driver
15/06/03 19:47:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 217
15/06/03 19:47:53 INFO executor.Executor: Running task 24.0 in stage 14.0 (TID 217)
15/06/03 19:47:53 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5.:\x08,\xE5<\xA2D\x00\x85
15/06/03 19:47:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:53 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:54 INFO regionserver.HStore: Closed V
15/06/03 19:47:54 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:47:54 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:47:54 INFO executor.Executor: Finished task 17.0 in stage 14.0 (TID 210). 2156 bytes result sent to driver
15/06/03 19:47:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 218
15/06/03 19:47:54 INFO executor.Executor: Running task 25.0 in stage 14.0 (TID 218)
15/06/03 19:47:54 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5<\xA2D\x00\x85,\xE5CNa\x00\x85
15/06/03 19:47:54 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:54 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:54 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:54 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 19:47:55 INFO regionserver.HStore: Closed V
15/06/03 19:47:55 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:47:55 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:47:55 INFO executor.Executor: Finished task 18.0 in stage 14.0 (TID 211). 1993 bytes result sent to driver
15/06/03 19:47:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 219
15/06/03 19:47:55 INFO executor.Executor: Running task 26.0 in stage 14.0 (TID 219)
15/06/03 19:47:55 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5CNa\x00\x85,\xE5N\xD3\x85\x00\x85
15/06/03 19:47:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:55 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:55 INFO regionserver.HStore: Closed V
15/06/03 19:47:55 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 19:47:55 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 19:47:55 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:55 INFO executor.Executor: Finished task 19.0 in stage 14.0 (TID 212). 1993 bytes result sent to driver
15/06/03 19:47:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 220
15/06/03 19:47:55 INFO executor.Executor: Running task 27.0 in stage 14.0 (TID 220)
15/06/03 19:47:55 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5N\xD3\x85\x00\x85,\xE5]:\xC7\x00\x82
15/06/03 19:47:56 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:56 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:56 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:56 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:57 INFO regionserver.HStore: Closed V
15/06/03 19:47:57 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:47:57 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:47:57 INFO executor.Executor: Finished task 20.0 in stage 14.0 (TID 213). 2156 bytes result sent to driver
15/06/03 19:47:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 221
15/06/03 19:47:57 INFO executor.Executor: Running task 28.0 in stage 14.0 (TID 221)
15/06/03 19:47:57 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5]:\xC7\x00\x82,\xE5k\xA2\xE1\x00\x83
15/06/03 19:47:57 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:57 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:57 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:57 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:58 INFO regionserver.HStore: Closed V
15/06/03 19:47:58 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:47:58 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:47:58 INFO executor.Executor: Finished task 25.0 in stage 14.0 (TID 218). 2156 bytes result sent to driver
15/06/03 19:47:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 222
15/06/03 19:47:58 INFO executor.Executor: Running task 29.0 in stage 14.0 (TID 222)
15/06/03 19:47:58 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5k\xA2\xE1\x00\x83,\xE5z\x00G\x00\x81
15/06/03 19:47:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:58 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:58 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:58 INFO regionserver.HStore: Closed V
15/06/03 19:47:58 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:47:58 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:47:58 INFO executor.Executor: Finished task 21.0 in stage 14.0 (TID 214). 2156 bytes result sent to driver
15/06/03 19:47:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 223
15/06/03 19:47:58 INFO executor.Executor: Running task 30.0 in stage 14.0 (TID 223)
15/06/03 19:47:58 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5z\x00G\x00\x81,\xE5\x86\x8B\x06
15/06/03 19:47:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:58 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:58 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 19:47:59 INFO regionserver.HStore: Closed V
15/06/03 19:47:59 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:47:59 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:47:59 INFO executor.Executor: Finished task 22.0 in stage 14.0 (TID 215). 2156 bytes result sent to driver
15/06/03 19:47:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 224
15/06/03 19:47:59 INFO executor.Executor: Running task 31.0 in stage 14.0 (TID 224)
15/06/03 19:47:59 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x86\x8B\x06,\xE5\x92\x0Cc\x00\x82
15/06/03 19:47:59 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:59 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:47:59 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:59 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:47:59 INFO regionserver.HStore: Closed V
15/06/03 19:47:59 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:47:59 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:47:59 INFO executor.Executor: Finished task 24.0 in stage 14.0 (TID 217). 2156 bytes result sent to driver
15/06/03 19:47:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 225
15/06/03 19:47:59 INFO executor.Executor: Running task 32.0 in stage 14.0 (TID 225)
15/06/03 19:47:59 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x92\x0Cc\x00\x82,\xE5\xA0q\x06\x00\x86
15/06/03 19:47:59 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:59 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:47:59 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:47:59 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:48:00 INFO regionserver.HStore: Closed V
15/06/03 19:48:00 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 19:48:00 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 19:48:00 INFO executor.Executor: Finished task 23.0 in stage 14.0 (TID 216). 2156 bytes result sent to driver
15/06/03 19:48:00 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 226
15/06/03 19:48:00 INFO executor.Executor: Running task 33.0 in stage 14.0 (TID 226)
15/06/03 19:48:00 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xA0q\x06\x00\x86,\xE5\xAE\xD6B\x00\x82
15/06/03 19:48:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:00 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:48:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:00 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:48:02 INFO regionserver.HStore: Closed V
15/06/03 19:48:02 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:48:02 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:48:02 INFO executor.Executor: Finished task 26.0 in stage 14.0 (TID 219). 2156 bytes result sent to driver
15/06/03 19:48:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 227
15/06/03 19:48:02 INFO executor.Executor: Running task 34.0 in stage 14.0 (TID 227)
15/06/03 19:48:02 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xAE\xD6B\x00\x82,\xE5\xBDA`\x00\x85
15/06/03 19:48:02 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:02 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:48:02 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:02 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:48:03 INFO regionserver.HStore: Closed V
15/06/03 19:48:03 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:48:03 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:48:03 INFO executor.Executor: Finished task 27.0 in stage 14.0 (TID 220). 2156 bytes result sent to driver
15/06/03 19:48:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 228
15/06/03 19:48:03 INFO executor.Executor: Running task 35.0 in stage 14.0 (TID 228)
15/06/03 19:48:03 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xBDA`\x00\x85,
15/06/03 19:48:03 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:03 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:48:03 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:03 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 19:48:04 INFO regionserver.HStore: Closed V
15/06/03 19:48:04 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:48:04 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:48:05 INFO executor.Executor: Finished task 28.0 in stage 14.0 (TID 221). 1993 bytes result sent to driver
15/06/03 19:48:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 229
15/06/03 19:48:05 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 229)
15/06/03 19:48:05 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29
15/06/03 19:48:05 INFO storage.MemoryStore: ensureFreeSpace(11486) called with curMem=2653179, maxMem=740960501
15/06/03 19:48:05 INFO storage.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 11.2 KB, free 704.1 MB)
15/06/03 19:48:05 INFO storage.BlockManagerMaster: Updated info of block broadcast_29_piece0
15/06/03 19:48:05 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 9 ms
15/06/03 19:48:05 INFO storage.MemoryStore: ensureFreeSpace(29920) called with curMem=2664665, maxMem=740960501
15/06/03 19:48:05 INFO storage.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 29.2 KB, free 704.1 MB)
15/06/03 19:48:05 INFO regionserver.HStore: Closed V
15/06/03 19:48:05 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:48:05 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:48:05 INFO rdd.NewHadoopRDD: Input split: localhost:,\xE42Y\x08
15/06/03 19:48:05 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
15/06/03 19:48:05 INFO executor.Executor: Finished task 30.0 in stage 14.0 (TID 223). 1993 bytes result sent to driver
15/06/03 19:48:05 INFO storage.MemoryStore: ensureFreeSpace(24481) called with curMem=2694585, maxMem=740960501
15/06/03 19:48:05 INFO storage.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 23.9 KB, free 704.0 MB)
15/06/03 19:48:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 230
15/06/03 19:48:05 INFO storage.BlockManagerMaster: Updated info of block broadcast_15_piece0
15/06/03 19:48:05 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 37 ms
15/06/03 19:48:05 INFO executor.Executor: Running task 1.0 in stage 15.0 (TID 230)
15/06/03 19:48:05 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=2719066, maxMem=740960501
15/06/03 19:48:05 INFO storage.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 380.7 KB, free 703.7 MB)
15/06/03 19:48:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:05 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:48:05 INFO rdd.NewHadoopRDD: Input split: localhost:\xE42Y\x08,\xE4q \x05
15/06/03 19:48:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:05 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:48:05 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
15/06/03 19:48:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:05 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:48:05 INFO storage.MemoryStore: ensureFreeSpace(99) called with curMem=3108861, maxMem=740960501
15/06/03 19:48:05 INFO storage.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 99.0 B, free 703.7 MB)
15/06/03 19:48:05 INFO storage.BlockManagerMaster: Updated info of block broadcast_18_piece0
15/06/03 19:48:05 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 11 ms
15/06/03 19:48:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:05 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:48:05 INFO storage.MemoryStore: ensureFreeSpace(144) called with curMem=3108960, maxMem=740960501
15/06/03 19:48:05 INFO storage.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 144.0 B, free 703.7 MB)
15/06/03 19:48:05 INFO regionserver.HStore: Closed V
15/06/03 19:48:05 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 19:48:05 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 19:48:05 INFO executor.Executor: Finished task 29.0 in stage 14.0 (TID 222). 1993 bytes result sent to driver
15/06/03 19:48:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 231
15/06/03 19:48:05 INFO executor.Executor: Running task 2.0 in stage 15.0 (TID 231)
15/06/03 19:48:05 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4q \x05,\xE4\xAF\xEED
15/06/03 19:48:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:05 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:48:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:05 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:48:05 INFO regionserver.HStore: Closed V
15/06/03 19:48:05 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:48:05 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:48:06 INFO executor.Executor: Finished task 31.0 in stage 14.0 (TID 224). 1993 bytes result sent to driver
15/06/03 19:48:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 232
15/06/03 19:48:06 INFO executor.Executor: Running task 3.0 in stage 15.0 (TID 232)
15/06/03 19:48:06 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xAF\xEED,\xE4\xCA[\x84
15/06/03 19:48:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:06 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:48:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:06 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 19:48:08 INFO regionserver.HStore: Closed V
15/06/03 19:48:08 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:48:08 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:48:08 INFO executor.Executor: Finished task 32.0 in stage 14.0 (TID 225). 2156 bytes result sent to driver
15/06/03 19:48:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 233
15/06/03 19:48:08 INFO executor.Executor: Running task 4.0 in stage 15.0 (TID 233)
15/06/03 19:48:08 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xCA[\x84,\xE4\xFC\x90'
15/06/03 19:48:08 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:08 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:48:08 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:08 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:48:09 INFO regionserver.HStore: Closed V
15/06/03 19:48:09 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 19:48:09 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 19:48:09 INFO executor.Executor: Finished task 3.0 in stage 15.0 (TID 232). 2201 bytes result sent to driver
15/06/03 19:48:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 234
15/06/03 19:48:09 INFO executor.Executor: Running task 5.0 in stage 15.0 (TID 234)
15/06/03 19:48:09 INFO rdd.NewHadoopRDD: Input split: localhost:\xE4\xFC\x90',\xE5;h!
15/06/03 19:48:09 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:09 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:48:09 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:09 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:48:09 INFO regionserver.HStore: Closed V
15/06/03 19:48:09 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:48:09 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:48:09 INFO executor.Executor: Finished task 33.0 in stage 14.0 (TID 226). 2156 bytes result sent to driver
15/06/03 19:48:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 235
15/06/03 19:48:09 INFO executor.Executor: Running task 6.0 in stage 15.0 (TID 235)
15/06/03 19:48:09 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5;h!,\xE5z)h
15/06/03 19:48:09 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:09 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:48:09 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:09 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:48:10 INFO regionserver.HStore: Closed V
15/06/03 19:48:10 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 19:48:10 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 19:48:10 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 229). 2201 bytes result sent to driver
15/06/03 19:48:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 236
15/06/03 19:48:10 INFO executor.Executor: Running task 7.0 in stage 15.0 (TID 236)
15/06/03 19:48:10 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5z)h,\xE5\x94\xAE\x01
15/06/03 19:48:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:10 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:48:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:10 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 19:48:10 INFO regionserver.HStore: Closed V
15/06/03 19:48:10 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:48:10 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:48:10 INFO executor.Executor: Finished task 34.0 in stage 14.0 (TID 227). 2156 bytes result sent to driver
15/06/03 19:48:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 237
15/06/03 19:48:10 INFO executor.Executor: Running task 8.0 in stage 15.0 (TID 237)
15/06/03 19:48:10 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\x94\xAE\x01,\xE5\xC6\xE6\x87
15/06/03 19:48:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:10 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 19:48:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:10 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 19:48:10 INFO regionserver.HStore: Closed V
15/06/03 19:48:10 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 19:48:10 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 19:48:11 INFO executor.Executor: Finished task 35.0 in stage 14.0 (TID 228). 2156 bytes result sent to driver
15/06/03 19:48:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 238
15/06/03 19:48:11 INFO executor.Executor: Running task 9.0 in stage 15.0 (TID 238)
15/06/03 19:48:11 INFO rdd.NewHadoopRDD: Input split: localhost:\xE5\xC6\xE6\x87,
15/06/03 19:48:11 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:11 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 19:48:11 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 19:48:11 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 19:48:11 INFO regionserver.HStore: Closed V
15/06/03 19:48:11 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 19:48:11 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 19:48:11 INFO executor.Executor: Finished task 1.0 in stage 15.0 (TID 230). 2201 bytes result sent to driver
15/06/03 19:48:11 INFO regionserver.HStore: Closed V
15/06/03 19:48:11 INFO regionserver.HRegion: Closed 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893.
15/06/03 19:48:11 WARN regionserver.HRegion: Region 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893. already closed
15/06/03 19:48:11 INFO executor.Executor: Finished task 9.0 in stage 15.0 (TID 238). 2038 bytes result sent to driver
15/06/03 19:48:11 INFO regionserver.HStore: Closed V
15/06/03 19:48:11 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 19:48:11 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 19:48:11 INFO executor.Executor: Finished task 2.0 in stage 15.0 (TID 231). 2201 bytes result sent to driver
15/06/03 19:48:12 INFO regionserver.HStore: Closed V
15/06/03 19:48:12 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 19:48:12 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 19:48:12 INFO executor.Executor: Finished task 4.0 in stage 15.0 (TID 233). 2038 bytes result sent to driver
15/06/03 19:48:12 INFO regionserver.HStore: Closed V
15/06/03 19:48:12 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 19:48:12 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 19:48:12 INFO executor.Executor: Finished task 7.0 in stage 15.0 (TID 236). 2038 bytes result sent to driver
15/06/03 19:48:13 INFO regionserver.HStore: Closed V
15/06/03 19:48:13 INFO regionserver.HRegion: Closed 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893.
15/06/03 19:48:13 WARN regionserver.HRegion: Region 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893. already closed
15/06/03 19:48:13 INFO executor.Executor: Finished task 8.0 in stage 15.0 (TID 237). 2038 bytes result sent to driver
15/06/03 19:48:13 INFO regionserver.HStore: Closed V
15/06/03 19:48:13 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 19:48:13 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 19:48:13 INFO executor.Executor: Finished task 5.0 in stage 15.0 (TID 234). 2038 bytes result sent to driver
15/06/03 19:48:13 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 19:48:13 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbbf2f762001f
15/06/03 19:48:13 INFO zookeeper.ZooKeeper: Session: 0x14dbbf2f762001f closed
15/06/03 19:48:13 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 19:48:13 INFO regionserver.HStore: Closed V
15/06/03 19:48:13 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 19:48:13 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 19:48:13 INFO executor.Executor: Finished task 6.0 in stage 15.0 (TID 235). 2038 bytes result sent to driver
15/06/03 19:48:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 239
15/06/03 19:48:13 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 239)
15/06/03 19:48:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 240
15/06/03 19:48:13 INFO executor.Executor: Running task 1.0 in stage 17.0 (TID 240)
15/06/03 19:48:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 241
15/06/03 19:48:13 INFO executor.Executor: Running task 2.0 in stage 17.0 (TID 241)
15/06/03 19:48:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 242
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
15/06/03 19:48:13 INFO executor.Executor: Running task 3.0 in stage 17.0 (TID 242)
15/06/03 19:48:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
15/06/03 19:48:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 243
15/06/03 19:48:13 INFO executor.Executor: Running task 4.0 in stage 17.0 (TID 243)
15/06/03 19:48:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 244
15/06/03 19:48:13 INFO executor.Executor: Running task 5.0 in stage 17.0 (TID 244)
15/06/03 19:48:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 245
15/06/03 19:48:13 INFO executor.Executor: Running task 6.0 in stage 17.0 (TID 245)
15/06/03 19:48:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 246
15/06/03 19:48:13 INFO executor.Executor: Running task 7.0 in stage 17.0 (TID 246)
15/06/03 19:48:13 INFO storage.MemoryStore: ensureFreeSpace(11161) called with curMem=3109104, maxMem=740960501
15/06/03 19:48:13 INFO storage.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 10.9 KB, free 703.7 MB)
15/06/03 19:48:13 INFO storage.BlockManagerMaster: Updated info of block broadcast_30_piece0
15/06/03 19:48:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 13 ms
15/06/03 19:48:13 INFO storage.MemoryStore: ensureFreeSpace(26912) called with curMem=3120265, maxMem=740960501
15/06/03 19:48:13 INFO storage.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 26.3 KB, free 703.6 MB)
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@10.0.1.65:65270/user/MapOutputTracker#599918699]
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@10.0.1.65:65270/user/MapOutputTracker#599918699]
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
15/06/03 19:48:13 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:13 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 0.0 in stage 17.0 (TID 239)
java.lang.NumberFormatException: For input string: "3="
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 247
15/06/03 19:48:14 INFO executor.Executor: Running task 8.0 in stage 17.0 (TID 247)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 4.0 in stage 17.0 (TID 243)
java.lang.NumberFormatException: For input string: ";=;>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 248
15/06/03 19:48:14 INFO executor.Executor: Running task 0.1 in stage 17.0 (TID 248)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 6.0 in stage 17.0 (TID 245)
java.lang.NumberFormatException: For input string: "::::53477"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 249
15/06/03 19:48:14 INFO executor.Executor: Running task 4.1 in stage 17.0 (TID 249)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 8.0 in stage 17.0 (TID 247)
java.lang.NumberFormatException: For input string: ";0516>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 250
15/06/03 19:48:14 INFO executor.Executor: Running task 6.1 in stage 17.0 (TID 250)
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 1.0 in stage 17.0 (TID 240)
java.lang.NumberFormatException: For input string: ":<5:"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 251
15/06/03 19:48:14 INFO executor.Executor: Running task 8.1 in stage 17.0 (TID 251)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 3.0 in stage 17.0 (TID 242)
java.lang.NumberFormatException: For input string: "04416    "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:347)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 252
15/06/03 19:48:14 INFO executor.Executor: Running task 1.1 in stage 17.0 (TID 252)
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 2.0 in stage 17.0 (TID 241)
java.lang.NumberFormatException: For input string: ":="
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 0.1 in stage 17.0 (TID 248)
java.lang.NumberFormatException: For input string: "3="
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 253
15/06/03 19:48:14 INFO executor.Executor: Running task 3.1 in stage 17.0 (TID 253)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 5.0 in stage 17.0 (TID 244)
java.lang.ArrayIndexOutOfBoundsException: -1
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:211)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 254
15/06/03 19:48:14 INFO executor.Executor: Running task 2.1 in stage 17.0 (TID 254)
15/06/03 19:48:14 INFO storage.BlockManager: Removing broadcast 29
15/06/03 19:48:14 INFO storage.BlockManager: Removing block broadcast_29_piece0
15/06/03 19:48:14 INFO storage.MemoryStore: Block broadcast_29_piece0 of size 11486 dropped from memory (free 737824810)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 255
15/06/03 19:48:14 INFO storage.BlockManagerMaster: Updated info of block broadcast_29_piece0
15/06/03 19:48:14 INFO storage.BlockManager: Removing block broadcast_29
15/06/03 19:48:14 INFO storage.MemoryStore: Block broadcast_29 of size 29920 dropped from memory (free 737854730)
15/06/03 19:48:14 INFO storage.BlockManager: Removing broadcast 28
15/06/03 19:48:14 INFO storage.BlockManager: Removing block broadcast_28
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.MemoryStore: Block broadcast_28 of size 26240 dropped from memory (free 737880970)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.BlockManager: Removing block broadcast_28_piece0
15/06/03 19:48:14 INFO storage.MemoryStore: Block broadcast_28_piece0 of size 11022 dropped from memory (free 737891992)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.BlockManagerMaster: Updated info of block broadcast_28_piece0
15/06/03 19:48:14 INFO executor.Executor: Running task 0.2 in stage 17.0 (TID 255)
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 7.0 in stage 17.0 (TID 246)
java.lang.NumberFormatException: For input string: ";5018>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 256
15/06/03 19:48:14 INFO executor.Executor: Running task 5.1 in stage 17.0 (TID 256)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 8.1 in stage 17.0 (TID 251)
java.lang.NumberFormatException: For input string: ";0516>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 257
15/06/03 19:48:14 INFO executor.Executor: Running task 7.1 in stage 17.0 (TID 257)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 4.1 in stage 17.0 (TID 249)
java.lang.NumberFormatException: For input string: ";=;>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 258
15/06/03 19:48:14 INFO executor.Executor: Running task 8.2 in stage 17.0 (TID 258)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 6.1 in stage 17.0 (TID 250)
java.lang.NumberFormatException: For input string: "::::53477"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 259
15/06/03 19:48:14 INFO executor.Executor: Running task 4.2 in stage 17.0 (TID 259)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 1.1 in stage 17.0 (TID 252)
java.lang.NumberFormatException: For input string: ":<5:"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 260
15/06/03 19:48:14 INFO executor.Executor: Running task 6.2 in stage 17.0 (TID 260)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 0.2 in stage 17.0 (TID 255)
java.lang.NumberFormatException: For input string: "3="
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 261
15/06/03 19:48:14 INFO executor.Executor: Running task 1.2 in stage 17.0 (TID 261)
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 3.1 in stage 17.0 (TID 253)
java.lang.NumberFormatException: For input string: "04416    "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:347)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 262
15/06/03 19:48:14 INFO executor.Executor: Running task 0.3 in stage 17.0 (TID 262)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 2.1 in stage 17.0 (TID 254)
java.lang.NumberFormatException: For input string: ":="
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 263
15/06/03 19:48:14 INFO executor.Executor: Running task 3.2 in stage 17.0 (TID 263)
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 8.2 in stage 17.0 (TID 258)
java.lang.NumberFormatException: For input string: ";0516>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 264
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO executor.Executor: Running task 2.2 in stage 17.0 (TID 264)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 4.2 in stage 17.0 (TID 259)
java.lang.NumberFormatException: For input string: ";=;>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 265
15/06/03 19:48:14 INFO executor.Executor: Running task 8.3 in stage 17.0 (TID 265)
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 0.3 in stage 17.0 (TID 262)
java.lang.NumberFormatException: For input string: "3="
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 266
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 19:48:14 INFO executor.Executor: Running task 4.3 in stage 17.0 (TID 266)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 19:48:14 INFO executor.Executor: Executor is trying to kill task 4.3 in stage 17.0 (TID 266)
15/06/03 19:48:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 19:48:14 ERROR executor.Executor: Exception in task 4.3 in stage 17.0 (TID 266)
org.apache.spark.TaskKilledException
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 19:48:14 INFO executor.Executor: Executor is trying to kill task 3.2 in stage 17.0 (TID 263)
15/06/03 19:48:14 INFO executor.Executor: Executor is trying to kill task 5.1 in stage 17.0 (TID 256)
15/06/03 19:48:14 INFO executor.Executor: Executor is trying to kill task 6.2 in stage 17.0 (TID 260)
15/06/03 19:48:14 INFO executor.Executor: Executor is trying to kill task 7.1 in stage 17.0 (TID 257)
15/06/03 19:48:14 INFO executor.Executor: Executor is trying to kill task 2.2 in stage 17.0 (TID 264)
15/06/03 19:48:14 INFO executor.Executor: Executor is trying to kill task 1.2 in stage 17.0 (TID 261)
15/06/03 19:48:14 INFO executor.Executor: Executor killed task 5.1 in stage 17.0 (TID 256)
15/06/03 19:48:14 INFO executor.Executor: Executor is trying to kill task 8.3 in stage 17.0 (TID 265)
15/06/03 19:48:14 INFO executor.Executor: Executor killed task 6.2 in stage 17.0 (TID 260)
15/06/03 19:48:14 INFO executor.Executor: Executor killed task 2.2 in stage 17.0 (TID 264)
15/06/03 19:48:14 INFO executor.Executor: Executor killed task 1.2 in stage 17.0 (TID 261)
15/06/03 19:48:14 INFO executor.Executor: Executor killed task 7.1 in stage 17.0 (TID 257)
15/06/03 19:48:14 INFO executor.Executor: Executor killed task 3.2 in stage 17.0 (TID 263)
15/06/03 19:48:14 INFO executor.Executor: Executor killed task 8.3 in stage 17.0 (TID 265)
15/06/03 19:49:31 INFO storage.BlockManager: Removing broadcast 20
15/06/03 19:49:31 INFO storage.BlockManager: Removing block broadcast_20_piece0
15/06/03 19:49:31 INFO storage.MemoryStore: Block broadcast_20_piece0 of size 24493 dropped from memory (free 737916485)
15/06/03 19:49:31 INFO storage.BlockManagerMaster: Updated info of block broadcast_20_piece0
15/06/03 19:49:31 INFO storage.BlockManager: Removing block broadcast_20
15/06/03 19:49:31 INFO storage.MemoryStore: Block broadcast_20 of size 389795 dropped from memory (free 738306280)
15/06/03 19:49:31 INFO storage.BlockManager: Removing broadcast 30
15/06/03 19:49:31 INFO storage.BlockManager: Removing block broadcast_30_piece0
15/06/03 19:49:31 INFO storage.MemoryStore: Block broadcast_30_piece0 of size 11161 dropped from memory (free 738317441)
15/06/03 19:49:31 INFO storage.BlockManagerMaster: Updated info of block broadcast_30_piece0
15/06/03 19:49:31 INFO storage.BlockManager: Removing block broadcast_30
15/06/03 19:49:31 INFO storage.MemoryStore: Block broadcast_30 of size 26912 dropped from memory (free 738344353)
15/06/03 19:49:31 INFO storage.BlockManager: Removing broadcast 26
15/06/03 19:49:31 INFO storage.BlockManager: Removing broadcast 23
15/06/03 19:51:00 WARN client.HConnectionManager$HConnectionImplementation: Checking master connection
com.google.protobuf.ServiceException: java.io.IOException: Call to localhost/127.0.0.1:65236 failed on local exception: java.io.EOFException
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1678)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.isMasterRunning(MasterProtos.java:44411)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$MasterServiceState.isMasterRunning(HConnectionManager.java:1512)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.isKeepAliveMasterConnectedAndRunning(HConnectionManager.java:2157)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getKeepAliveMasterService(HConnectionManager.java:1863)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.isMasterRunning(HConnectionManager.java:881)
	at org.apache.hadoop.hbase.client.HBaseAdmin.isMasterRunning(HBaseAdmin.java:295)
	at com.splicemachine.hbase.HBaseRegionLoads.getAdmin(HBaseRegionLoads.java:111)
	at com.splicemachine.hbase.HBaseRegionLoads.fetchRegionLoads(HBaseRegionLoads.java:136)
	at com.splicemachine.hbase.HBaseRegionLoads.access$000(HBaseRegionLoads.java:27)
	at com.splicemachine.hbase.HBaseRegionLoads$1.run(HBaseRegionLoads.java:46)
	at com.splicemachine.concurrent.DynamicScheduledRunnable.run(DynamicScheduledRunnable.java:28)
	at com.splicemachine.concurrent.LoggingScheduledThreadPoolExecutor$LoggingRunnable.run(LoggingScheduledThreadPoolExecutor.java:60)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Call to localhost/127.0.0.1:65236 failed on local exception: java.io.EOFException
	at org.apache.hadoop.hbase.ipc.RpcClient.wrapException(RpcClient.java:1489)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1461)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)
	... 20 more
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.readResponse(RpcClient.java:1076)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.run(RpcClient.java:727)
15/06/03 19:51:00 INFO timestamp.TimestampClient: TimestampClient was disconnected from the server
15/06/03 19:51:00 WARN client.HConnectionManager$HConnectionImplementation: Checking master connection
com.google.protobuf.ServiceException: java.net.ConnectException: Connection refused
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1678)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.isMasterRunning(MasterProtos.java:44411)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$MasterServiceState.isMasterRunning(HConnectionManager.java:1512)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.isKeepAliveMasterConnectedAndRunning(HConnectionManager.java:2157)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getKeepAliveMasterService(HConnectionManager.java:1863)
	at org.apache.hadoop.hbase.client.HBaseAdmin$MasterCallable.prepare(HBaseAdmin.java:3376)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:113)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3403)
	at org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus(HBaseAdmin.java:2191)
	at com.splicemachine.hbase.HBaseRegionLoads.fetchRegionLoads(HBaseRegionLoads.java:138)
	at com.splicemachine.hbase.HBaseRegionLoads.access$000(HBaseRegionLoads.java:27)
	at com.splicemachine.hbase.HBaseRegionLoads$1.run(HBaseRegionLoads.java:46)
	at com.splicemachine.concurrent.DynamicScheduledRunnable.run(DynamicScheduledRunnable.java:28)
	at com.splicemachine.concurrent.LoggingScheduledThreadPoolExecutor$LoggingRunnable.run(LoggingScheduledThreadPoolExecutor.java:60)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:578)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:868)
	at org.apache.hadoop.hbase.ipc.RpcClient.getConnection(RpcClient.java:1543)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1442)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)
	... 22 more
15/06/03 19:51:00 WARN client.HConnectionManager$HConnectionImplementation: Checking master connection
com.google.protobuf.ServiceException: java.io.IOException: Call to localhost/127.0.0.1:65236 failed on local exception: org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: localhost/127.0.0.1:65236
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1678)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.isMasterRunning(MasterProtos.java:44411)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$MasterServiceState.isMasterRunning(HConnectionManager.java:1512)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.isKeepAliveMasterConnectedAndRunning(HConnectionManager.java:2157)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getKeepAliveMasterService(HConnectionManager.java:1863)
	at org.apache.hadoop.hbase.client.HBaseAdmin$MasterCallable.prepare(HBaseAdmin.java:3376)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:113)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3403)
	at org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus(HBaseAdmin.java:2191)
	at com.splicemachine.utils.SpliceUtilities.getMasterServer(SpliceUtilities.java:70)
	at com.splicemachine.si.impl.timestamp.TimestampClient.getHost(TimestampClient.java:140)
	at com.splicemachine.si.impl.timestamp.TimestampClient.connectIfNeeded(TimestampClient.java:168)
	at com.splicemachine.si.impl.timestamp.TimestampClient.channelDisconnected(TimestampClient.java:310)
	at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:120)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:493)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:361)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Call to localhost/127.0.0.1:65236 failed on local exception: org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: localhost/127.0.0.1:65236
	at org.apache.hadoop.hbase.ipc.RpcClient.wrapException(RpcClient.java:1489)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1461)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)
	... 34 more
Caused by: org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: localhost/127.0.0.1:65236
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:853)
	at org.apache.hadoop.hbase.ipc.RpcClient.getConnection(RpcClient.java:1543)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1442)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.getClusterStatus(MasterProtos.java:44399)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$5.getClusterStatus(HConnectionManager.java:2133)
	at org.apache.hadoop.hbase.client.HBaseAdmin$16.call(HBaseAdmin.java:2195)
	at org.apache.hadoop.hbase.client.HBaseAdmin$16.call(HBaseAdmin.java:2191)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3403)
	at org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus(HBaseAdmin.java:2191)
	at com.splicemachine.hbase.HBaseRegionLoads.fetchRegionLoads(HBaseRegionLoads.java:138)
	at com.splicemachine.hbase.HBaseRegionLoads.access$000(HBaseRegionLoads.java:27)
	at com.splicemachine.hbase.HBaseRegionLoads$1.run(HBaseRegionLoads.java:46)
	at com.splicemachine.concurrent.DynamicScheduledRunnable.run(DynamicScheduledRunnable.java:28)
	at com.splicemachine.concurrent.LoggingScheduledThreadPoolExecutor$LoggingRunnable.run(LoggingScheduledThreadPoolExecutor.java:60)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	... 3 more
15/06/03 19:51:00 ERROR executor.CoarseGrainedExecutorBackend: Driver Disassociated [akka.tcp://sparkExecutor@10.0.1.65:65276] -> [akka.tcp://sparkDriver@10.0.1.65:65270] disassociated! Shutting down.
15/06/03 19:51:00 WARN remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkDriver@10.0.1.65:65270] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
