15/06/05 09:08:49 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
15/06/05 09:08:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/06/05 09:08:52 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/05 09:08:52 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/05 09:08:52 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/05 09:08:52 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/06/05 09:08:52 INFO Remoting: Starting remoting
15/06/05 09:08:52 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@192.168.0.83:60704]
15/06/05 09:08:52 INFO util.Utils: Successfully started service 'driverPropsFetcher' on port 60704.
15/06/05 09:08:52 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/05 09:08:52 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/05 09:08:52 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/05 09:08:52 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/06/05 09:08:52 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/06/05 09:08:52 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/06/05 09:08:52 INFO Remoting: Starting remoting
15/06/05 09:08:52 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@192.168.0.83:60706]
15/06/05 09:08:52 INFO util.Utils: Successfully started service 'sparkExecutor' on port 60706.
15/06/05 09:08:52 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/06/05 09:08:52 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@192.168.0.83:60700/user/CoarseGrainedScheduler
15/06/05 09:08:52 INFO worker.WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@192.168.0.83:60643/user/Worker
15/06/05 09:08:53 INFO worker.WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@192.168.0.83:60643/user/Worker
15/06/05 09:08:53 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
15/06/05 09:08:53 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/05 09:08:53 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/05 09:08:53 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/05 09:08:53 INFO util.AkkaUtils: Connecting to MapOutputTracker: akka.tcp://sparkDriver@192.168.0.83:60700/user/MapOutputTracker
15/06/05 09:08:53 INFO util.AkkaUtils: Connecting to BlockManagerMaster: akka.tcp://sparkDriver@192.168.0.83:60700/user/BlockManagerMaster
15/06/05 09:08:53 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20150605090853-3702
15/06/05 09:08:53 INFO storage.MemoryStore: MemoryStore started with capacity 706.6 MB
15/06/05 09:08:53 INFO netty.NettyBlockTransferService: Server created on 60709
15/06/05 09:08:53 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/06/05 09:08:53 INFO storage.BlockManagerMaster: Registered BlockManager
15/06/05 09:08:53 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.83:60700/user/HeartbeatReceiver
15/06/05 09:08:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
15/06/05 09:08:53 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/06/05 09:08:53 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
15/06/05 09:08:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
15/06/05 09:08:53 INFO storage.MemoryStore: ensureFreeSpace(10687) called with curMem=0, maxMem=740960501
15/06/05 09:08:53 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 706.6 MB)
15/06/05 09:08:53 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/06/05 09:08:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 179 ms
15/06/05 09:08:54 INFO storage.MemoryStore: ensureFreeSpace(30152) called with curMem=10687, maxMem=740960501
15/06/05 09:08:54 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 29.4 KB, free 706.6 MB)
15/06/05 09:08:54 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x322a4437 connecting to ZooKeeper ensemble=localhost:2181
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 02/24/2015 20:43 GMT
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:host.name=192.168.0.83
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:java.version=1.7.0_71
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.7.0_71.jdk/Contents/Home/jre
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/Users/jleach/.m2/repository/com/splicemachine/splice_machine_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_machine-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_si_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_si_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_si-cdh5.3.2/1.1.1-SNAPSHOT/splice_si-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_constants-cdh5.3.2/1.1.1-SNAPSHOT/splice_constants-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/googlecode/concurrentlinkedhashmap/concurrentlinkedhashmap-lru/1.4.2/concurrentlinkedhashmap-lru-1.4.2.jar:/Users/jleach/.m2/repository/joda-time/joda-time/2.3/joda-time-2.3.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-client/2.5.0-cdh5.3.2/hadoop-client-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.5.0-cdh5.3.2/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.5.0-cdh5.3.2/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.5.0-cdh5.3.2/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.5.0-cdh5.3.2/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.5.0-cdh5.3.2/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.5.0-cdh5.3.2/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.5.0-cdh5.3.2/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.5.0-cdh5.3.2/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.5.0-cdh5.3.2/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-aws/2.5.0-cdh5.3.2/hadoop-aws-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.2.3/jackson-databind-2.2.3.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.2.3/jackson-core-2.2.3.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.2.3/jackson-annotations-2.2.3.jar:/Users/jleach/.m2/repository/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-annotations/2.5.0-cdh5.3.2/hadoop-annotations-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-common/2.5.0-cdh5.3.2/hadoop-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/Users/jleach/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/jleach/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/jleach/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/Users/jleach/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/jleach/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/jleach/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/jleach/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/Users/jleach/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/Users/jleach/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/jleach/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/jleach/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/jleach/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/jleach/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/jleach/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/jleach/.m2/repository/org/apache/avro/avro/1.7.6-cdh5.3.2/avro-1.7.6-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/jleach/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-auth/2.5.0-cdh5.3.2/hadoop-auth-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/jleach/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/jleach/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/jleach/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/Users/jleach/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/jleach/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.5.0-cdh5.3.2/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-client/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-client-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-common/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-common-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/cloudera/htrace/htrace-core/2.04/htrace-core-2.04.jar:/Users/jleach/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-common/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-common-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-core/2.5.0-mr1-cdh5.3.2/hadoop-core-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/jleach/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-protocol/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-protocol-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-server/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-server-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-prefix-tree/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-prefix-tree-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop2-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/yammer/metrics/metrics-core/2.1.2/metrics-core-2.1.2.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/jleach/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.5.0-cdh5.3.2/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-testing-util/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-testing-util-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-server/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-server-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop2-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-minicluster/2.5.0-mr1-cdh5.3.2/hadoop-minicluster-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-test/2.5.0-mr1-cdh5.3.2/hadoop-test-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftplet-api/1.0.0/ftplet-api-1.0.0.jar:/Users/jleach/.m2/repository/org/apache/mina/mina-core/2.0.0-M5/mina-core-2.0.0-M5.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftpserver-core/1.0.0/ftpserver-core-1.0.0.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftpserver-deprecated/1.0.0-M2/ftpserver-deprecated-1.0.0-M2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-common/2.5.0-cdh5.3.2/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-beeline/0.13.1-cdh5.3.2/hive-beeline-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-metastore/0.13.1-cdh5.3.2/hive-metastore-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/jleach/.m2/repository/org/apache/derby/derby/10.10.1.1/derby-10.10.1.1.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/Users/jleach/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/jleach/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/Users/jleach/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/Users/jleach/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/Users/jleach/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/Users/jleach/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/Users/jleach/.m2/repository/org/apache/thrift/libthrift/0.9.0-cdh5-2/libthrift-0.9.0-cdh5-2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-cli/0.13.1-cdh5.3.2/hive-cli-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-serde/0.13.1-cdh5.3.2/hive-serde-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-exec/0.13.1-cdh5.3.2/hive-exec-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-ant/0.13.1-cdh5.3.2/hive-ant-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/Users/jleach/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/Users/jleach/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/jleach/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/Users/jleach/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/Users/jleach/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-common/0.13.1-cdh5.3.2/hive-common-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-contrib/0.13.1-cdh5.3.2/hive-contrib-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-hbase-handler/0.13.1-cdh5.3.2/hive-hbase-handler-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-hwi/0.13.1-cdh5.3.2/hive-hwi-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-jdbc/0.13.1-cdh5.3.2/hive-jdbc-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/jleach/.m2/repository/org/apache/httpcomponents/httpcore/4.2.5/httpcore-4.2.5.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-service/0.13.1-cdh5.3.2/hive-service-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/Users/jleach/.m2/repository/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar:/Users/jleach/.m2/repository/javax/mail/mail/1.4.1/mail-1.4.1.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar:/Users/jleach/.m2/repository/asm/asm-commons/3.1/asm-commons-3.1.jar:/Users/jleach/.m2/repository/asm/asm-tree/3.1/asm-tree-3.1.jar:/Users/jleach/.m2/repository/org/apache/thrift/libfb303/0.9.0/libfb303-0.9.0.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-shims/0.13.1-cdh5.3.2/hive-shims-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-common/0.13.1-cdh5.3.2/hive-shims-common-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-common-secure/0.13.1-cdh5.3.2/hive-shims-common-secure-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-0.23/0.13.1-cdh5.3.2/hive-shims-0.23-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-scheduler/0.13.1-cdh5.3.2/hive-shims-scheduler-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.3.2/zookeeper-3.4.5-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/spark/spark-assembly-hadoop2.5.0-cdh5.3.2/1.2.0/spark-assembly-hadoop2.5.0-cdh5.3.2-1.2.0.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_protocol-cdh5.3.2/1.1.1-SNAPSHOT/splice_protocol-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/asynchbase/1.5.5/asynchbase-1.5.5.jar:/Users/jleach/.m2/repository/com/stumbleupon/async/1.4.0/async-1.4.0.jar:/Users/jleach/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_machine_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/splicetest/sqlj/sqlj-it-procs/1.0.2-SNAPSHOT/sqlj-it-procs-1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/jleach/.m2/repository/com/carrotsearch/hppc/0.5.2/hppc-0.5.2.jar:/Users/jleach/.m2/repository/com/carrotsearch/java-sizeof/0.0.4/java-sizeof-0.0.4.jar:/Users/jleach/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/jleach/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/jleach/.m2/repository/org/ow2/asm/asm/4.0/asm-4.0.jar:/Users/jleach/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/jleach/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/jleach/.m2/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/jleach/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/jleach/.m2/repository/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar:/Users/jleach/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/Users/jleach/.m2/repository/com/lmax/disruptor/3.2.1/disruptor-3.2.1.jar:/Users/jleach/.m2/repository/com/splicemachine/utilities/1.1.2-SNAPSHOT/utilities-1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/stats/1.0.0-SNAPSHOT/stats-1.0.0-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice-web/1.1.0-SNAPSHOT/splice-web-1.1.0-SNAPSHOT.tar.gz:/Users/jleach/.m2/repository/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar:/Users/jleach/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/jleach/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/jleach/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/jleach/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/jleach/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/jleach/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.7.1/jackson-jaxrs-1.7.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar:/Users/jleach/.m2/repository/asm/asm/3.1/asm-3.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/jleach/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/jleach/.m2/repository/commons-dbutils/commons-dbutils/1.5/commons-dbutils-1.5.jar:/Users/jleach/.m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar:/Users/jleach/.m2/repository/commons-lang/commons-lang/2.5/commons-lang-2.5.jar:/Users/jleach/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:/Users/jleach/.m2/repository/de/javakaffee/kryo-serializers/0.26/kryo-serializers-0.26.jar:/Users/jleach/.m2/repository/io/netty/netty/3.6.6.Final/netty-3.6.6.Final.jar:/Users/jleach/.m2/repository/net/sf/ehcache/ehcache-core/2.6.6/ehcache-core-2.6.6.jar:/Users/jleach/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/jleach/.m2/repository/net/sf/supercsv/super-csv/2.3.2-SNAPSHOT-splice/super-csv-2.3.2-SNAPSHOT-splice.jar:/Users/jleach/.m2/repository/com/splicemachine/db/1.1.1.2-SNAPSHOT/db-1.1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/dbclient/1.1.1.2-SNAPSHOT/dbclient-1.1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/lucene/lucene-core/4.3.1/lucene-core-4.3.1.jar:/Users/jleach/.m2/repository/org/apache/mrunit/mrunit/1.0.0/mrunit-1.0.0-hadoop2.jar:/Users/jleach/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.1/uncommons-maths-1.2.1.jar:/Users/jleach/.m2/repository/org/uncommons/watchmaker/watchmaker-framework/0.7.1/watchmaker-framework-0.7.1.jar:/Users/jleach/.m2/repository/junit/junit/4.11/junit-4.11.jar:/Users/jleach/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/jleach/.m2/repository/org/mockito/mockito-all/1.9.5/mockito-all-1.9.5.jar:/Users/jleach/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/jleach/Documents/workspace/spliceengine/cdh5.3.2/splice_machine_test/target/classes
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:java.library.path=:/Users/jleach/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/var/folders/01/td1fd0hx6z72c6g58vqc1tnr0000gp/T/
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:os.name=Mac OS X
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:os.arch=x86_64
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:os.version=10.10.3
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:user.name=jleach
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:user.home=/Users/jleach
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Client environment:user.dir=/Users/jleach/Documents/workspace/spliceengine/cdh5.3.2/splice_machine_test/work/app-20150605090844-0000/0
15/06/05 09:08:54 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x322a4437, quorum=localhost:2181, baseZNode=/hbase
15/06/05 09:08:54 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:54 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:08:54 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:54 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:08:54 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
15/06/05 09:08:54 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/05 09:08:54 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50010, negotiated timeout = 60000
15/06/05 09:08:55 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x67346637 connecting to ZooKeeper ensemble=localhost:2181
15/06/05 09:08:55 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x67346637, quorum=localhost:2181, baseZNode=/hbase
15/06/05 09:08:55 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:55 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:08:55 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50011, negotiated timeout = 60000
15/06/05 09:08:55 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x729a045f connecting to ZooKeeper ensemble=localhost:2181
15/06/05 09:08:55 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x729a045f, quorum=localhost:2181, baseZNode=/hbase
15/06/05 09:08:55 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:55 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:08:55 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
15/06/05 09:08:55 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/05 09:08:55 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:55 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:08:55 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50012, negotiated timeout = 60000
15/06/05 09:08:56 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x41da02b2 connecting to ZooKeeper ensemble=localhost:2181
15/06/05 09:08:56 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x41da02b2, quorum=localhost:2181, baseZNode=/hbase
15/06/05 09:08:56 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:56 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:08:56 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
15/06/05 09:08:56 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/05 09:08:56 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:56 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:08:56 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50013, negotiated timeout = 60000
15/06/05 09:08:57 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x4a70f0ba connecting to ZooKeeper ensemble=localhost:2181
15/06/05 09:08:57 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x4a70f0ba, quorum=localhost:2181, baseZNode=/hbase
15/06/05 09:08:57 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:57 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:08:58 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
15/06/05 09:08:58 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/05 09:08:58 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:58 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:08:58 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50014, negotiated timeout = 60000
15/06/05 09:08:59 INFO temp.TempTable: Temp Table initial bucket count: 16
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Process identifier=spliceconnection connecting to ZooKeeper ensemble=localhost:2181
15/06/05 09:08:59 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=spliceconnection, quorum=localhost:2181, baseZNode=/hbase
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50015, negotiated timeout = 60000
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x34cc2998 connecting to ZooKeeper ensemble=localhost:2181
15/06/05 09:08:59 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x34cc2998, quorum=localhost:2181, baseZNode=/hbase
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50016, negotiated timeout = 60000
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6ee37fcd connecting to ZooKeeper ensemble=localhost:2181
15/06/05 09:08:59 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x6ee37fcd, quorum=localhost:2181, baseZNode=/hbase
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:08:59 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50017, negotiated timeout = 60000
15/06/05 09:08:59 ERROR utils.PipelineConstants: No Native Snappy Installed: Splice Machine's Write Pipeline will not compress data over the wire.
15/06/05 09:08:59 INFO hbase.SpliceDriver: Booting the SpliceDriver
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /ddl already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /ddl/ongoingChanges already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /ddl/activeServers already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /spliceTasks already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /spliceJobs already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates/__CONGLOM_SEQUENCE already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /derbyPropertyPath already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /transactions already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /transactions/maxReservedTimestamp already exists and this is not a retry
15/06/05 09:08:59 INFO zookeeper.RecoverableZooKeeper: Node /transactions/minimum already exists and this is not a retry
15/06/05 09:08:59 INFO db.SpliceDatabase: Booting the Splice Machine database
15/06/05 09:08:59 INFO txn.SpliceTimestampSource: Creating the TimestampClient...
15/06/05 09:08:59 INFO timestamp.TimestampClient: TimestampClient on region server successfully registered with JMX
15/06/05 09:08:59 INFO timestamp.TimestampClient: Attempting to connect to server (host 192.168.0.83, port 60012)
15/06/05 09:08:59 INFO timestamp.TimestampClient: Successfully connected to server
15/06/05 09:08:59 INFO catalog.SpliceDataDictionary: Splice Software Version = 1.1.1
15/06/05 09:08:59 INFO catalog.SpliceDataDictionary: Splice Catalog Version = 1.1.1
15/06/05 09:09:00 INFO hbase.SpliceDriver: Splice Engine is Running, Enabling Services
15/06/05 09:09:00 INFO hbase.SpliceDriver: Splice Machine Release = 1.1.1-SNAPSHOT
15/06/05 09:09:00 INFO hbase.SpliceDriver: Splice Machine Version Hash = 355d353208
15/06/05 09:09:00 INFO hbase.SpliceDriver: Splice Machine Build Time = 2015-06-05 14:02 +0000
15/06/05 09:09:00 INFO hbase.SpliceDriver: Splice Machine URL = http://www.splicemachine.com
15/06/05 09:09:00 INFO hbase.SpliceDriver: Services successfully started, enabling JDBC connections...
15/06/05 09:09:00 INFO rdd.NewHadoopRDD: Input split: 192.168.0.83:,
15/06/05 09:09:00 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
15/06/05 09:09:00 INFO storage.MemoryStore: ensureFreeSpace(22621) called with curMem=40839, maxMem=740960501
15/06/05 09:09:00 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 706.6 MB)
15/06/05 09:09:00 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/06/05 09:09:00 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 11 ms
15/06/05 09:09:00 INFO hbase.SpliceDriver: Ready to accept JDBC connections on 0.0.0.0:1527
15/06/05 09:09:00 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=63460, maxMem=740960501
15/06/05 09:09:00 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 380.7 KB, free 706.2 MB)
15/06/05 09:09:00 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x4b9c7b0d connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/05 09:09:00 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x4b9c7b0d, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/05 09:09:00 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:09:00 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:09:00 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad50018, negotiated timeout = 60000
15/06/05 09:09:00 INFO hfile.CacheConfig: Allocating LruBlockCache with maximum size 1.9 G
15/06/05 09:09:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:09:00 INFO util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
15/06/05 09:09:00 INFO util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
15/06/05 09:09:00 INFO regionserver.HRegion: Onlined 15e413209c546b7076549311e3132617; next sequenceid=32
15/06/05 09:09:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:09:00 INFO regionserver.HRegion: Onlined 15e413209c546b7076549311e3132617; next sequenceid=32
15/06/05 09:09:02 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/05 09:09:02 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dc40afad50018
15/06/05 09:09:02 INFO zookeeper.ZooKeeper: Session: 0x14dc40afad50018 closed
15/06/05 09:09:02 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/05 09:09:02 INFO regionserver.HStore: Closed V
15/06/05 09:09:02 INFO regionserver.HRegion: Closed 1392,,1433432704157.15e413209c546b7076549311e3132617.
15/06/05 09:09:02 WARN regionserver.HRegion: Region 1392,,1433432704157.15e413209c546b7076549311e3132617. already closed
15/06/05 09:09:02 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1925 bytes result sent to driver
15/06/05 09:09:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
15/06/05 09:09:02 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
15/06/05 09:09:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
15/06/05 09:09:02 INFO storage.MemoryStore: ensureFreeSpace(9872) called with curMem=453255, maxMem=740960501
15/06/05 09:09:02 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 9.6 KB, free 706.2 MB)
15/06/05 09:09:02 INFO storage.BlockManagerMaster: Updated info of block broadcast_2_piece0
15/06/05 09:09:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 9 ms
15/06/05 09:09:02 INFO storage.MemoryStore: ensureFreeSpace(21984) called with curMem=463127, maxMem=740960501
15/06/05 09:09:02 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 21.5 KB, free 706.2 MB)
15/06/05 09:09:03 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 721 bytes result sent to driver
15/06/05 09:09:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
15/06/05 09:09:03 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
15/06/05 09:09:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
15/06/05 09:09:03 INFO storage.MemoryStore: ensureFreeSpace(951) called with curMem=485111, maxMem=740960501
15/06/05 09:09:03 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 951.0 B, free 706.2 MB)
15/06/05 09:09:03 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/06/05 09:09:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 10 ms
15/06/05 09:09:03 INFO storage.MemoryStore: ensureFreeSpace(1392) called with curMem=486062, maxMem=740960501
15/06/05 09:09:03 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 1392.0 B, free 706.2 MB)
15/06/05 09:09:03 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 606 bytes result sent to driver
15/06/05 09:09:47 INFO storage.BlockManager: Removing broadcast 2
15/06/05 09:09:47 INFO storage.BlockManager: Removing block broadcast_2
15/06/05 09:09:47 INFO storage.MemoryStore: Block broadcast_2 of size 21984 dropped from memory (free 740495031)
15/06/05 09:09:47 INFO storage.BlockManager: Removing block broadcast_2_piece0
15/06/05 09:09:47 INFO storage.MemoryStore: Block broadcast_2_piece0 of size 9872 dropped from memory (free 740504903)
15/06/05 09:09:47 INFO storage.BlockManagerMaster: Updated info of block broadcast_2_piece0
15/06/05 09:09:47 INFO storage.BlockManager: Removing broadcast 3
15/06/05 09:09:47 INFO storage.BlockManager: Removing block broadcast_3_piece0
15/06/05 09:09:47 INFO storage.MemoryStore: Block broadcast_3_piece0 of size 951 dropped from memory (free 740505854)
15/06/05 09:09:47 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/06/05 09:09:47 INFO storage.BlockManager: Removing block broadcast_3
15/06/05 09:09:47 INFO storage.MemoryStore: Block broadcast_3 of size 1392 dropped from memory (free 740507246)
15/06/05 09:17:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
15/06/05 09:17:55 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
15/06/05 09:17:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
15/06/05 09:17:55 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 4)
15/06/05 09:17:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
15/06/05 09:17:55 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 5)
15/06/05 09:17:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
15/06/05 09:17:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
15/06/05 09:17:55 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 6)
15/06/05 09:17:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
15/06/05 09:17:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
15/06/05 09:17:55 INFO executor.Executor: Running task 4.0 in stage 3.0 (TID 7)
15/06/05 09:17:55 INFO executor.Executor: Running task 5.0 in stage 3.0 (TID 8)
15/06/05 09:17:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
15/06/05 09:17:55 INFO executor.Executor: Running task 6.0 in stage 3.0 (TID 9)
15/06/05 09:17:55 INFO storage.MemoryStore: ensureFreeSpace(6748) called with curMem=453255, maxMem=740960501
15/06/05 09:17:55 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.6 KB, free 706.2 MB)
15/06/05 09:17:55 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
15/06/05 09:17:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 13 ms
15/06/05 09:17:55 INFO storage.MemoryStore: ensureFreeSpace(14768) called with curMem=460003, maxMem=740960501
15/06/05 09:17:55 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.4 KB, free 706.2 MB)
15/06/05 09:17:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.83:,\xE4\x0B\xCCG\x00\x82
15/06/05 09:17:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.83:\xE4\x1AP\xC3\x00\x82,\xE4(\xDF\xE4\x00\x83
15/06/05 09:17:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.83:\xE4(\xDF\xE4\x00\x83,\xE47c#\x00\x82
15/06/05 09:17:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
15/06/05 09:17:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.83:\xE4Tr\x80\x00\x83,
15/06/05 09:17:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.83:\xE47c#\x00\x82,\xE4E\xEAB\x00\x84
15/06/05 09:17:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.83:\xE4E\xEAB\x00\x84,\xE4Tr\x80\x00\x83
15/06/05 09:17:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.83:\xE4\x0B\xCCG\x00\x82,\xE4\x1AP\xC3\x00\x82
15/06/05 09:17:55 INFO storage.MemoryStore: ensureFreeSpace(22532) called with curMem=474771, maxMem=740960501
15/06/05 09:17:55 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.0 KB, free 706.2 MB)
15/06/05 09:17:55 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/06/05 09:17:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 8 ms
15/06/05 09:17:55 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=497303, maxMem=740960501
15/06/05 09:17:55 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 380.7 KB, free 705.8 MB)
15/06/05 09:17:55 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x389e058e connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/05 09:17:55 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x389e058e, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/05 09:17:55 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:17:55 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/05 09:17:55 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dc40afad5001b, negotiated timeout = 60000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:55 INFO regionserver.HRegion: Onlined cc29f7fe9f9fd932551fa393db6be396; next sequenceid=1216
15/06/05 09:17:58 INFO regionserver.HStore: Closed V
15/06/05 09:17:58 INFO regionserver.HRegion: Closed 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396.
15/06/05 09:17:58 WARN regionserver.HRegion: Region 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396. already closed
15/06/05 09:17:59 INFO executor.Executor: Finished task 6.0 in stage 3.0 (TID 9). 2014 bytes result sent to driver
15/06/05 09:18:02 INFO regionserver.HStore: Closed V
15/06/05 09:18:02 INFO regionserver.HRegion: Closed 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396.
15/06/05 09:18:02 WARN regionserver.HRegion: Region 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396. already closed
15/06/05 09:18:02 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 2014 bytes result sent to driver
15/06/05 09:18:02 INFO regionserver.HStore: Closed V
15/06/05 09:18:02 INFO regionserver.HRegion: Closed 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396.
15/06/05 09:18:02 WARN regionserver.HRegion: Region 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396. already closed
15/06/05 09:18:02 INFO regionserver.HStore: Closed V
15/06/05 09:18:02 INFO regionserver.HRegion: Closed 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396.
15/06/05 09:18:02 WARN regionserver.HRegion: Region 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396. already closed
15/06/05 09:18:03 INFO executor.Executor: Finished task 2.0 in stage 3.0 (TID 5). 2014 bytes result sent to driver
15/06/05 09:18:03 INFO regionserver.HStore: Closed V
15/06/05 09:18:03 INFO regionserver.HRegion: Closed 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396.
15/06/05 09:18:03 WARN regionserver.HRegion: Region 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396. already closed
15/06/05 09:18:03 INFO regionserver.HStore: Closed V
15/06/05 09:18:03 INFO regionserver.HRegion: Closed 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396.
15/06/05 09:18:03 WARN regionserver.HRegion: Region 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396. already closed
15/06/05 09:18:03 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/05 09:18:03 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dc40afad5001b
15/06/05 09:18:03 INFO executor.Executor: Finished task 3.0 in stage 3.0 (TID 6). 2014 bytes result sent to driver
15/06/05 09:18:03 INFO executor.Executor: Finished task 5.0 in stage 3.0 (TID 8). 2014 bytes result sent to driver
15/06/05 09:18:03 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 4). 2014 bytes result sent to driver
15/06/05 09:18:03 INFO zookeeper.ZooKeeper: Session: 0x14dc40afad5001b closed
15/06/05 09:18:03 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/05 09:18:03 INFO regionserver.HStore: Closed V
15/06/05 09:18:03 INFO regionserver.HRegion: Closed 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396.
15/06/05 09:18:03 WARN regionserver.HRegion: Region 1360,,1433432703382.cc29f7fe9f9fd932551fa393db6be396. already closed
15/06/05 09:18:03 INFO executor.Executor: Finished task 4.0 in stage 3.0 (TID 7). 2014 bytes result sent to driver
15/06/05 09:18:03 INFO storage.BlockManager: Removing broadcast 4
15/06/05 09:18:03 INFO storage.BlockManager: Removing block broadcast_4
15/06/05 09:18:03 INFO storage.MemoryStore: Block broadcast_4 of size 389795 dropped from memory (free 740463198)
15/06/05 09:18:03 INFO storage.BlockManager: Removing block broadcast_4_piece0
15/06/05 09:18:03 INFO storage.MemoryStore: Block broadcast_4_piece0 of size 22532 dropped from memory (free 740485730)
15/06/05 09:18:03 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/06/05 09:18:03 INFO storage.BlockManager: Removing broadcast 5
15/06/05 09:18:03 INFO storage.BlockManager: Removing block broadcast_5_piece0
15/06/05 09:18:03 INFO storage.MemoryStore: Block broadcast_5_piece0 of size 6748 dropped from memory (free 740492478)
15/06/05 09:18:03 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
15/06/05 09:18:03 INFO storage.BlockManager: Removing block broadcast_5
15/06/05 09:18:03 INFO storage.MemoryStore: Block broadcast_5 of size 14768 dropped from memory (free 740507246)
15/06/05 09:18:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
15/06/05 09:18:03 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 10)
15/06/05 09:18:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
15/06/05 09:18:03 INFO storage.MemoryStore: ensureFreeSpace(5963) called with curMem=453255, maxMem=740960501
15/06/05 09:18:03 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KB, free 706.2 MB)
15/06/05 09:18:03 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
15/06/05 09:18:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 9 ms
15/06/05 09:18:03 INFO storage.MemoryStore: ensureFreeSpace(12160) called with curMem=459218, maxMem=740960501
15/06/05 09:18:03 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.9 KB, free 706.2 MB)
15/06/05 09:18:04 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 10). 718 bytes result sent to driver
15/06/05 09:18:46 INFO storage.BlockManager: Removing broadcast 6
15/06/05 09:18:46 INFO storage.BlockManager: Removing block broadcast_6_piece0
15/06/05 09:18:46 INFO storage.MemoryStore: Block broadcast_6_piece0 of size 5963 dropped from memory (free 740495086)
15/06/05 09:18:46 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
15/06/05 09:18:46 INFO storage.BlockManager: Removing block broadcast_6
15/06/05 09:18:46 INFO storage.MemoryStore: Block broadcast_6 of size 12160 dropped from memory (free 740507246)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
15/06/05 09:19:49 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 11)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
15/06/05 09:19:49 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 12)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
15/06/05 09:19:49 INFO executor.Executor: Running task 2.0 in stage 5.0 (TID 13)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
15/06/05 09:19:49 INFO executor.Executor: Running task 3.0 in stage 5.0 (TID 14)
15/06/05 09:19:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
15/06/05 09:19:49 INFO storage.MemoryStore: ensureFreeSpace(14166) called with curMem=453255, maxMem=740960501
15/06/05 09:19:49 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 13.8 KB, free 706.2 MB)
15/06/05 09:19:49 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
15/06/05 09:19:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 8 ms
15/06/05 09:19:49 INFO storage.MemoryStore: ensureFreeSpace(63112) called with curMem=467421, maxMem=740960501
15/06/05 09:19:49 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 61.6 KB, free 706.1 MB)
15/06/05 09:19:49 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:49 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:49 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:49 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:49 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:49 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:49 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:49 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:49 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:49 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:49 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:49 ERROR executor.Executor: Exception in task 0.0 in stage 5.0 (TID 11)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:49 ERROR executor.Executor: Exception in task 1.0 in stage 5.0 (TID 12)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:49 ERROR executor.Executor: Exception in task 2.0 in stage 5.0 (TID 13)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:49 ERROR executor.Executor: Exception in task 3.0 in stage 5.0 (TID 14)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
15/06/05 09:19:49 INFO executor.Executor: Running task 1.1 in stage 5.0 (TID 15)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
15/06/05 09:19:49 INFO executor.Executor: Running task 0.1 in stage 5.0 (TID 16)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
15/06/05 09:19:49 INFO executor.Executor: Running task 2.1 in stage 5.0 (TID 17)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
15/06/05 09:19:49 INFO executor.Executor: Running task 3.1 in stage 5.0 (TID 18)
15/06/05 09:19:49 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:49 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:49 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:49 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:49 ERROR executor.Executor: Exception in task 0.1 in stage 5.0 (TID 16)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:49 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:49 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:49 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:49 ERROR executor.Executor: Exception in task 3.1 in stage 5.0 (TID 18)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:49 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:49 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:49 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:49 ERROR executor.Executor: Exception in task 1.1 in stage 5.0 (TID 15)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:49 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:49 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:49 ERROR executor.Executor: Exception in task 2.1 in stage 5.0 (TID 17)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
15/06/05 09:19:49 INFO executor.Executor: Running task 0.2 in stage 5.0 (TID 19)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
15/06/05 09:19:49 INFO executor.Executor: Running task 3.2 in stage 5.0 (TID 20)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
15/06/05 09:19:49 INFO executor.Executor: Running task 1.2 in stage 5.0 (TID 21)
15/06/05 09:19:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
15/06/05 09:19:49 INFO executor.Executor: Running task 2.2 in stage 5.0 (TID 22)
15/06/05 09:19:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:50 ERROR executor.Executor: Exception in task 0.2 in stage 5.0 (TID 19)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:50 ERROR executor.Executor: Exception in task 1.2 in stage 5.0 (TID 21)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:50 ERROR executor.Executor: Exception in task 3.2 in stage 5.0 (TID 20)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
15/06/05 09:19:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:50 INFO executor.Executor: Running task 0.3 in stage 5.0 (TID 23)
15/06/05 09:19:50 ERROR executor.Executor: Exception in task 2.2 in stage 5.0 (TID 22)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
15/06/05 09:19:50 INFO executor.Executor: Running task 1.3 in stage 5.0 (TID 24)
15/06/05 09:19:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
15/06/05 09:19:50 INFO executor.Executor: Running task 3.3 in stage 5.0 (TID 25)
15/06/05 09:19:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
15/06/05 09:19:50 INFO executor.Executor: Running task 2.3 in stage 5.0 (TID 26)
15/06/05 09:19:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:50 ERROR executor.Executor: Exception in task 1.3 in stage 5.0 (TID 24)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:50 ERROR executor.Executor: Exception in task 0.3 in stage 5.0 (TID 23)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:50 ERROR executor.Executor: Exception in task 3.3 in stage 5.0 (TID 25)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:19:50 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/05 09:19:50 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/05 09:19:50 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/05 09:19:50 ERROR executor.Executor: Exception in task 2.3 in stage 5.0 (TID 26)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 355d353208
Splice Machine Build Time: 2015-06-05 14:02 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:140)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/05 09:20:29 INFO storage.BlockManager: Removing broadcast 7
15/06/05 09:20:29 INFO storage.BlockManager: Removing block broadcast_7
15/06/05 09:20:29 INFO storage.MemoryStore: Block broadcast_7 of size 63112 dropped from memory (free 740493080)
15/06/05 09:20:29 INFO storage.BlockManager: Removing block broadcast_7_piece0
15/06/05 09:20:29 INFO storage.MemoryStore: Block broadcast_7_piece0 of size 14166 dropped from memory (free 740507246)
15/06/05 09:20:29 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dc40afad50017, likely server has closed socket, closing socket connection and attempting reconnect
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dc40afad50013, likely server has closed socket, closing socket connection and attempting reconnect
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dc40afad50014, likely server has closed socket, closing socket connection and attempting reconnect
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dc40afad50015, likely server has closed socket, closing socket connection and attempting reconnect
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dc40afad50011, likely server has closed socket, closing socket connection and attempting reconnect
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dc40afad50012, likely server has closed socket, closing socket connection and attempting reconnect
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dc40afad50010, likely server has closed socket, closing socket connection and attempting reconnect
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dc40afad50016, likely server has closed socket, closing socket connection and attempting reconnect
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:16 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50015 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:16 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50010 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:16 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:16 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50012 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:16 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:16 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50016 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:17 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:17 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50017 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:17 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:17 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50013 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:17 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:17 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50014 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:17 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:17 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50016 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50010 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50012 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50015 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50010 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50013 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50016 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:18 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:18 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50017 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:19 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:19 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50013 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:19 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:19 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50014 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:19 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:19 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50012 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:19 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:19 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50015 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:19 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:19 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50017 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:19 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:19 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:20 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:20 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50014 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:20 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:20 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50016 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:20 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:20 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50010 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:20 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:20 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50015 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:20 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:20 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50012 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:20 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:20 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:20 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:20 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50013 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50013 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50016 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50010 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50017 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50014 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50015 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50014 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50012 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:21 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:21 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50017 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:22 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:22 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50010 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:22 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:22 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50015 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:22 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:22 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:22 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:22 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50013 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/05 09:23:23 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 2: SIGINT
15/06/05 09:23:23 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/05 09:23:23 WARN zookeeper.ClientCnxn: Session 0x14dc40afad50013 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
