15/06/03 17:14:28 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
15/06/03 17:14:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/06/03 17:14:31 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/03 17:14:31 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/03 17:14:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/03 17:14:31 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/06/03 17:14:31 INFO Remoting: Starting remoting
15/06/03 17:14:32 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@192.168.0.21:64535]
15/06/03 17:14:32 INFO util.Utils: Successfully started service 'driverPropsFetcher' on port 64535.
15/06/03 17:14:32 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/03 17:14:32 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/03 17:14:32 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/03 17:14:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/06/03 17:14:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/06/03 17:14:32 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/06/03 17:14:32 INFO Remoting: Starting remoting
15/06/03 17:14:32 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@192.168.0.21:64537]
15/06/03 17:14:32 INFO util.Utils: Successfully started service 'sparkExecutor' on port 64537.
15/06/03 17:14:32 INFO worker.WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@192.168.0.21:64459/user/Worker
15/06/03 17:14:32 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@192.168.0.21:64531/user/CoarseGrainedScheduler
15/06/03 17:14:32 INFO worker.WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@192.168.0.21:64459/user/Worker
15/06/03 17:14:32 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
15/06/03 17:14:32 INFO spark.SecurityManager: Changing view acls to: jleach
15/06/03 17:14:32 INFO spark.SecurityManager: Changing modify acls to: jleach
15/06/03 17:14:32 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jleach); users with modify permissions: Set(jleach)
15/06/03 17:14:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/06/03 17:14:32 INFO util.AkkaUtils: Connecting to MapOutputTracker: akka.tcp://sparkDriver@192.168.0.21:64531/user/MapOutputTracker
15/06/03 17:14:32 INFO util.AkkaUtils: Connecting to BlockManagerMaster: akka.tcp://sparkDriver@192.168.0.21:64531/user/BlockManagerMaster
15/06/03 17:14:32 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20150603171432-ba40
15/06/03 17:14:32 INFO storage.MemoryStore: MemoryStore started with capacity 706.6 MB
15/06/03 17:14:33 INFO netty.NettyBlockTransferService: Server created on 64540
15/06/03 17:14:33 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/06/03 17:14:33 INFO storage.BlockManagerMaster: Registered BlockManager
15/06/03 17:14:33 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.21:64531/user/HeartbeatReceiver
15/06/03 17:14:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
15/06/03 17:14:33 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/06/03 17:14:33 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
15/06/03 17:14:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
15/06/03 17:14:34 INFO storage.MemoryStore: ensureFreeSpace(8149) called with curMem=0, maxMem=740960501
15/06/03 17:14:34 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KB, free 706.6 MB)
15/06/03 17:14:34 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/06/03 17:14:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 208 ms
15/06/03 17:14:34 INFO storage.MemoryStore: ensureFreeSpace(18824) called with curMem=8149, maxMem=740960501
15/06/03 17:14:34 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.4 KB, free 706.6 MB)
15/06/03 17:14:34 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x4bf3f11c connecting to ZooKeeper ensemble=localhost:2181
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 02/24/2015 20:43 GMT
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:host.name=192.168.0.21
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:java.version=1.7.0_71
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.7.0_71.jdk/Contents/Home/jre
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/Users/jleach/.m2/repository/com/splicemachine/splice_machine_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_machine-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_si_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_si_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_si-cdh5.3.2/1.1.1-SNAPSHOT/splice_si-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_constants-cdh5.3.2/1.1.1-SNAPSHOT/splice_constants-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/googlecode/concurrentlinkedhashmap/concurrentlinkedhashmap-lru/1.4.2/concurrentlinkedhashmap-lru-1.4.2.jar:/Users/jleach/.m2/repository/joda-time/joda-time/2.3/joda-time-2.3.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-client/2.5.0-cdh5.3.2/hadoop-client-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.5.0-cdh5.3.2/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.5.0-cdh5.3.2/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.5.0-cdh5.3.2/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.5.0-cdh5.3.2/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.5.0-cdh5.3.2/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.5.0-cdh5.3.2/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.5.0-cdh5.3.2/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.5.0-cdh5.3.2/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.5.0-cdh5.3.2/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-aws/2.5.0-cdh5.3.2/hadoop-aws-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.2.3/jackson-databind-2.2.3.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.2.3/jackson-core-2.2.3.jar:/Users/jleach/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.2.3/jackson-annotations-2.2.3.jar:/Users/jleach/.m2/repository/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-annotations/2.5.0-cdh5.3.2/hadoop-annotations-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-common/2.5.0-cdh5.3.2/hadoop-common-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/Users/jleach/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/jleach/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/jleach/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/Users/jleach/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/Users/jleach/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty/6.1.26.cloudera.4/jetty-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26.cloudera.4/jetty-util-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/jleach/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/jleach/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/Users/jleach/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/Users/jleach/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/jleach/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/jleach/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/jleach/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/jleach/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/jleach/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users/jleach/.m2/repository/org/apache/avro/avro/1.7.6-cdh5.3.2/avro-1.7.6-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/jleach/.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-auth/2.5.0-cdh5.3.2/hadoop-auth-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/jleach/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/Users/jleach/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/Users/jleach/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/Users/jleach/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/Users/jleach/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/jleach/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.5.0-cdh5.3.2/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/Users/jleach/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-client/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-client-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-common/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-common-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/cloudera/htrace/htrace-core/2.04/htrace-core-2.04.jar:/Users/jleach/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-common/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-common-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-core/2.5.0-mr1-cdh5.3.2/hadoop-core-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/Users/jleach/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-protocol/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-protocol-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-server/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-server-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-prefix-tree/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-prefix-tree-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop2-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/yammer/metrics/metrics-core/2.1.2/metrics-core-2.1.2.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26.cloudera.4/jetty-sslengine-6.1.26.cloudera.4.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/Users/jleach/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/Users/jleach/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.5.0-cdh5.3.2/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-testing-util/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-testing-util-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-server/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-server-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT/hbase-hadoop2-compat-0.98.6-cdh5.3.2-splice1.0.2-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-minicluster/2.5.0-mr1-cdh5.3.2/hadoop-minicluster-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-test/2.5.0-mr1-cdh5.3.2/hadoop-test-2.5.0-mr1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftplet-api/1.0.0/ftplet-api-1.0.0.jar:/Users/jleach/.m2/repository/org/apache/mina/mina-core/2.0.0-M5/mina-core-2.0.0-M5.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftpserver-core/1.0.0/ftpserver-core-1.0.0.jar:/Users/jleach/.m2/repository/org/apache/ftpserver/ftpserver-deprecated/1.0.0-M2/ftpserver-deprecated-1.0.0-M2.jar:/Users/jleach/.m2/repository/org/apache/hadoop/hadoop-common/2.5.0-cdh5.3.2/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-beeline/0.13.1-cdh5.3.2/hive-beeline-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-metastore/0.13.1-cdh5.3.2/hive-metastore-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/jleach/.m2/repository/org/apache/derby/derby/10.10.1.1/derby-10.10.1.1.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/Users/jleach/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/Users/jleach/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/jleach/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/Users/jleach/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/Users/jleach/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/Users/jleach/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/Users/jleach/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/Users/jleach/.m2/repository/org/apache/thrift/libthrift/0.9.0-cdh5-2/libthrift-0.9.0-cdh5-2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-cli/0.13.1-cdh5.3.2/hive-cli-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-serde/0.13.1-cdh5.3.2/hive-serde-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-exec/0.13.1-cdh5.3.2/hive-exec-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-ant/0.13.1-cdh5.3.2/hive-ant-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/Users/jleach/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/jleach/.m2/repository/org/apache/commons/commons-lang3/3.1/commons-lang3-3.1.jar:/Users/jleach/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/jleach/.m2/repository/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/Users/jleach/.m2/repository/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/Users/jleach/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-common/0.13.1-cdh5.3.2/hive-common-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-contrib/0.13.1-cdh5.3.2/hive-contrib-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-hbase-handler/0.13.1-cdh5.3.2/hive-hbase-handler-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-hwi/0.13.1-cdh5.3.2/hive-hwi-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-jdbc/0.13.1-cdh5.3.2/hive-jdbc-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/Users/jleach/.m2/repository/org/apache/httpcomponents/httpcore/4.2.5/httpcore-4.2.5.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-service/0.13.1-cdh5.3.2/hive-service-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/Users/jleach/.m2/repository/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar:/Users/jleach/.m2/repository/javax/mail/mail/1.4.1/mail-1.4.1.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar:/Users/jleach/.m2/repository/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar:/Users/jleach/.m2/repository/asm/asm-commons/3.1/asm-commons-3.1.jar:/Users/jleach/.m2/repository/asm/asm-tree/3.1/asm-tree-3.1.jar:/Users/jleach/.m2/repository/org/apache/thrift/libfb303/0.9.0/libfb303-0.9.0.jar:/Users/jleach/.m2/repository/org/apache/hive/hive-shims/0.13.1-cdh5.3.2/hive-shims-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-common/0.13.1-cdh5.3.2/hive-shims-common-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-common-secure/0.13.1-cdh5.3.2/hive-shims-common-secure-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-0.23/0.13.1-cdh5.3.2/hive-shims-0.23-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/hive/shims/hive-shims-scheduler/0.13.1-cdh5.3.2/hive-shims-scheduler-0.13.1-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5-cdh5.3.2/zookeeper-3.4.5-cdh5.3.2.jar:/Users/jleach/.m2/repository/org/apache/spark/spark-assembly-hadoop2.5.0-cdh5.3.2/1.2.0/spark-assembly-hadoop2.5.0-cdh5.3.2-1.2.0.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_protocol-cdh5.3.2/1.1.1-SNAPSHOT/splice_protocol-cdh5.3.2-1.1.1-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/asynchbase/1.5.5/asynchbase-1.5.5.jar:/Users/jleach/.m2/repository/com/stumbleupon/async/1.4.0/async-1.4.0.jar:/Users/jleach/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar:/Users/jleach/.m2/repository/com/splicemachine/splice_machine_adapter_98-cdh5.3.2/1.1.1-SNAPSHOT/splice_machine_adapter_98-cdh5.3.2-1.1.1-SNAPSHOT-tests.jar:/Users/jleach/.m2/repository/org/splicetest/sqlj/sqlj-it-procs/1.0.2-SNAPSHOT/sqlj-it-procs-1.0.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/jleach/.m2/repository/com/carrotsearch/hppc/0.5.2/hppc-0.5.2.jar:/Users/jleach/.m2/repository/com/carrotsearch/java-sizeof/0.0.4/java-sizeof-0.0.4.jar:/Users/jleach/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/jleach/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/jleach/.m2/repository/org/ow2/asm/asm/4.0/asm-4.0.jar:/Users/jleach/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/jleach/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/jleach/.m2/repository/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar:/Users/jleach/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/jleach/.m2/repository/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar:/Users/jleach/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/Users/jleach/.m2/repository/com/lmax/disruptor/3.2.1/disruptor-3.2.1.jar:/Users/jleach/.m2/repository/com/splicemachine/utilities/1.1.2-SNAPSHOT/utilities-1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/stats/1.0.0-SNAPSHOT/stats-1.0.0-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/splice-web/1.1.0-SNAPSHOT/splice-web-1.1.0-SNAPSHOT.tar.gz:/Users/jleach/.m2/repository/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar:/Users/jleach/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/jleach/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/jleach/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/jleach/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/jleach/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/jleach/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.7.1/jackson-jaxrs-1.7.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:/Users/jleach/.m2/repository/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar:/Users/jleach/.m2/repository/asm/asm/3.1/asm-3.1.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/jleach/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/Users/jleach/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/jleach/.m2/repository/commons-dbutils/commons-dbutils/1.5/commons-dbutils-1.5.jar:/Users/jleach/.m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar:/Users/jleach/.m2/repository/commons-lang/commons-lang/2.5/commons-lang-2.5.jar:/Users/jleach/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:/Users/jleach/.m2/repository/de/javakaffee/kryo-serializers/0.26/kryo-serializers-0.26.jar:/Users/jleach/.m2/repository/io/netty/netty/3.6.6.Final/netty-3.6.6.Final.jar:/Users/jleach/.m2/repository/net/sf/ehcache/ehcache-core/2.6.6/ehcache-core-2.6.6.jar:/Users/jleach/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/jleach/.m2/repository/net/sf/supercsv/super-csv/2.3.2-SNAPSHOT-splice/super-csv-2.3.2-SNAPSHOT-splice.jar:/Users/jleach/.m2/repository/com/splicemachine/db/1.1.1.2-SNAPSHOT/db-1.1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/com/splicemachine/dbclient/1.1.1.2-SNAPSHOT/dbclient-1.1.1.2-SNAPSHOT.jar:/Users/jleach/.m2/repository/org/apache/lucene/lucene-core/4.3.1/lucene-core-4.3.1.jar:/Users/jleach/.m2/repository/org/apache/mrunit/mrunit/1.0.0/mrunit-1.0.0-hadoop2.jar:/Users/jleach/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.1/uncommons-maths-1.2.1.jar:/Users/jleach/.m2/repository/org/uncommons/watchmaker/watchmaker-framework/0.7.1/watchmaker-framework-0.7.1.jar:/Users/jleach/.m2/repository/junit/junit/4.11/junit-4.11.jar:/Users/jleach/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/Users/jleach/.m2/repository/org/mockito/mockito-all/1.9.5/mockito-all-1.9.5.jar:/Users/jleach/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/jleach/Documents/workspace/spliceengine/cdh5.3.2/splice_machine_test/target/classes
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:java.library.path=:/Users/jleach/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/var/folders/01/td1fd0hx6z72c6g58vqc1tnr0000gp/T/
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:os.name=Mac OS X
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:os.arch=x86_64
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:os.version=10.10.3
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:user.name=jleach
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:user.home=/Users/jleach
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Client environment:user.dir=/Users/jleach/Documents/workspace/spliceengine/cdh5.3.2/splice_machine_test/work/app-20150603171422-0000/0
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x4bf3f11c, quorum=localhost:2181, baseZNode=/hbase
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb000f, negotiated timeout = 60000
15/06/03 17:14:34 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x2da20234 connecting to ZooKeeper ensemble=localhost:2181
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x2da20234, quorum=localhost:2181, baseZNode=/hbase
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0010, negotiated timeout = 60000
15/06/03 17:14:34 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x3032bedb connecting to ZooKeeper ensemble=localhost:2181
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x3032bedb, quorum=localhost:2181, baseZNode=/hbase
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0011, negotiated timeout = 60000
15/06/03 17:14:34 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x5b14d75e connecting to ZooKeeper ensemble=localhost:2181
15/06/03 17:14:34 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x5b14d75e, quorum=localhost:2181, baseZNode=/hbase
15/06/03 17:14:34 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:34 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 17:14:35 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:35 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:35 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0012, negotiated timeout = 60000
15/06/03 17:14:35 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
15/06/03 17:14:35 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x3222590b connecting to ZooKeeper ensemble=localhost:2181
15/06/03 17:14:36 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x3222590b, quorum=localhost:2181, baseZNode=/hbase
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0013, negotiated timeout = 60000
15/06/03 17:14:36 INFO temp.TempTable: Temp Table initial bucket count: 16
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Process identifier=spliceconnection connecting to ZooKeeper ensemble=localhost:2181
15/06/03 17:14:36 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=spliceconnection, quorum=localhost:2181, baseZNode=/hbase
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0014, negotiated timeout = 60000
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x2a507f30 connecting to ZooKeeper ensemble=localhost:2181
15/06/03 17:14:36 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x2a507f30, quorum=localhost:2181, baseZNode=/hbase
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0015, negotiated timeout = 60000
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x103fecbf connecting to ZooKeeper ensemble=localhost:2181
15/06/03 17:14:36 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x103fecbf, quorum=localhost:2181, baseZNode=/hbase
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:36 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0016, negotiated timeout = 60000
15/06/03 17:14:36 ERROR utils.PipelineConstants: No Native Snappy Installed: Splice Machine's Write Pipeline will not compress data over the wire.
15/06/03 17:14:36 INFO hbase.SpliceDriver: Booting the SpliceDriver
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /ddl already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /ddl/ongoingChanges already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /ddl/activeServers already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /spliceTasks already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /spliceJobs already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates/__CONGLOM_SEQUENCE already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /derbyPropertyPath already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /conglomerates already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /transactions already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /transactions/maxReservedTimestamp already exists and this is not a retry
15/06/03 17:14:36 INFO zookeeper.RecoverableZooKeeper: Node /transactions/minimum already exists and this is not a retry
15/06/03 17:14:36 INFO db.SpliceDatabase: Booting the Splice Machine database
15/06/03 17:14:36 INFO txn.SpliceTimestampSource: Creating the TimestampClient...
15/06/03 17:14:36 INFO timestamp.TimestampClient: TimestampClient on region server successfully registered with JMX
15/06/03 17:14:36 INFO timestamp.TimestampClient: Attempting to connect to server (host 192.168.0.21, port 60012)
15/06/03 17:14:36 INFO timestamp.TimestampClient: Successfully connected to server
15/06/03 17:14:36 INFO catalog.SpliceDataDictionary: Splice Software Version = 1.1.1
15/06/03 17:14:36 INFO catalog.SpliceDataDictionary: Splice Catalog Version = 1.1.1
15/06/03 17:14:37 INFO hbase.SpliceDriver: Splice Engine is Running, Enabling Services
15/06/03 17:14:37 INFO hbase.SpliceDriver: Splice Machine Release = 1.1.1-SNAPSHOT
15/06/03 17:14:37 INFO hbase.SpliceDriver: Splice Machine Version Hash = 08c20a115b
15/06/03 17:14:37 INFO hbase.SpliceDriver: Splice Machine Build Time = 2015-06-03 17:28 +0000
15/06/03 17:14:37 INFO hbase.SpliceDriver: Splice Machine URL = http://www.splicemachine.com
15/06/03 17:14:37 INFO hbase.SpliceDriver: Services successfully started, enabling JDBC connections...
15/06/03 17:14:37 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\x01,\xF1\xC1\xC1
15/06/03 17:14:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
15/06/03 17:14:37 INFO storage.MemoryStore: ensureFreeSpace(24584) called with curMem=26973, maxMem=740960501
15/06/03 17:14:37 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KB, free 706.6 MB)
15/06/03 17:14:37 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/06/03 17:14:37 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 12 ms
15/06/03 17:14:37 INFO hbase.SpliceDriver: Ready to accept JDBC connections on 0.0.0.0:1527
15/06/03 17:14:37 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=51557, maxMem=740960501
15/06/03 17:14:37 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 380.7 KB, free 706.2 MB)
15/06/03 17:14:37 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x390ed28e connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 17:14:37 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x390ed28e, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 17:14:37 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:14:37 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:14:37 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0017, negotiated timeout = 60000
15/06/03 17:14:37 INFO hfile.CacheConfig: Allocating LruBlockCache with maximum size 1.9 G
15/06/03 17:14:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:14:37 INFO util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
15/06/03 17:14:37 INFO util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
15/06/03 17:14:37 INFO regionserver.HRegion: Onlined 790b5e2020bc448c107762790b7552e9; next sequenceid=15
15/06/03 17:14:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:14:37 INFO regionserver.HRegion: Onlined 790b5e2020bc448c107762790b7552e9; next sequenceid=15
15/06/03 17:14:37 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 17:14:37 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb0017
15/06/03 17:14:37 INFO zookeeper.ZooKeeper: Session: 0x14dbb7837eb0017 closed
15/06/03 17:14:37 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 17:14:37 INFO regionserver.HStore: Closed V
15/06/03 17:14:37 INFO regionserver.HRegion: Closed 209,,1433352600310.790b5e2020bc448c107762790b7552e9.
15/06/03 17:14:37 WARN regionserver.HRegion: Region 209,,1433352600310.790b5e2020bc448c107762790b7552e9. already closed
15/06/03 17:14:37 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2252 bytes result sent to driver
15/06/03 17:15:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
15/06/03 17:15:35 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
15/06/03 17:15:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
15/06/03 17:15:35 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
15/06/03 17:15:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
15/06/03 17:15:35 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
15/06/03 17:15:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
15/06/03 17:15:35 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 4)
15/06/03 17:15:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
15/06/03 17:15:35 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 5)
15/06/03 17:15:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
15/06/03 17:15:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
15/06/03 17:15:35 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 6)
15/06/03 17:15:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
15/06/03 17:15:35 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 7)
15/06/03 17:15:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
15/06/03 17:15:35 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 8)
15/06/03 17:15:35 INFO storage.MemoryStore: ensureFreeSpace(10159) called with curMem=441352, maxMem=740960501
15/06/03 17:15:35 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.9 KB, free 706.2 MB)
15/06/03 17:15:35 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/06/03 17:15:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 15 ms
15/06/03 17:15:35 INFO storage.MemoryStore: ensureFreeSpace(25624) called with curMem=451511, maxMem=740960501
15/06/03 17:15:35 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.0 KB, free 706.2 MB)
15/06/03 17:15:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,\xE4\x0B\xAA\xC1\x00\x84
15/06/03 17:15:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4=\xD3\xE7,\xE4I_'\x00\x84
15/06/03 17:15:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
15/06/03 17:15:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE46\xDE!\x00\x83,\xE4=\xD3\xE7
15/06/03 17:15:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x0B\xAA\xC1\x00\x84,\xE4\x1A\x15\x07
15/06/03 17:15:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x1A\x15\x07,\xE4(\x7F\xE1\x00\x85
15/06/03 17:15:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4I_'\x00\x84,\xE4W\xCF\x00\x00\x84
15/06/03 17:15:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4W\xCF\x00\x00\x84,\xE4f9\xC6
15/06/03 17:15:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4(\x7F\xE1\x00\x85,\xE46\xDE!\x00\x83
15/06/03 17:15:35 INFO storage.MemoryStore: ensureFreeSpace(24641) called with curMem=477135, maxMem=740960501
15/06/03 17:15:35 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.1 KB, free 706.2 MB)
15/06/03 17:15:35 INFO storage.BlockManagerMaster: Updated info of block broadcast_2_piece0
15/06/03 17:15:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 19 ms
15/06/03 17:15:35 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=501776, maxMem=740960501
15/06/03 17:15:35 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 380.7 KB, free 705.8 MB)
15/06/03 17:15:35 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x57bd2c28 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 17:15:35 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x57bd2c28, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 17:15:35 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:15:35 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:15:35 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0019, negotiated timeout = 60000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:35 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:36 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:15:54 INFO regionserver.HStore: Closed V
15/06/03 17:15:54 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:15:54 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:15:54 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 5). 2207 bytes result sent to driver
15/06/03 17:15:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
15/06/03 17:15:54 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 9)
15/06/03 17:15:54 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4f9\xC6,\xE4t\xA2B\x00\x82
15/06/03 17:15:54 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:54 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:15:54 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:15:54 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:16:01 INFO regionserver.HStore: Closed V
15/06/03 17:16:01 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:16:01 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:16:01 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2207 bytes result sent to driver
15/06/03 17:16:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
15/06/03 17:16:01 INFO executor.Executor: Running task 9.0 in stage 1.0 (TID 10)
15/06/03 17:16:01 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4t\xA2B\x00\x82,\xE4{\xA2C\x00\x83
15/06/03 17:16:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:01 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:16:01 INFO regionserver.HStore: Closed V
15/06/03 17:16:01 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:16:01 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:16:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:01 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:16:01 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 6). 2207 bytes result sent to driver
15/06/03 17:16:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
15/06/03 17:16:01 INFO executor.Executor: Running task 10.0 in stage 1.0 (TID 11)
15/06/03 17:16:01 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4{\xA2C\x00\x83,\xE4\x80\xCB\xE7\x00\x83
15/06/03 17:16:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:01 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 17:16:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:01 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 17:16:04 INFO regionserver.HStore: Closed V
15/06/03 17:16:04 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:16:04 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:16:04 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 2207 bytes result sent to driver
15/06/03 17:16:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
15/06/03 17:16:04 INFO executor.Executor: Running task 11.0 in stage 1.0 (TID 12)
15/06/03 17:16:05 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x80\xCB\xE7\x00\x83,\xE4\x8CK\xE5\x00\x86
15/06/03 17:16:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:16:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:16:05 INFO regionserver.HStore: Closed V
15/06/03 17:16:05 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:16:05 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:16:05 INFO regionserver.HStore: Closed V
15/06/03 17:16:05 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:16:05 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:16:05 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 2207 bytes result sent to driver
15/06/03 17:16:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
15/06/03 17:16:05 INFO executor.Executor: Running task 12.0 in stage 1.0 (TID 13)
15/06/03 17:16:05 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 4). 2207 bytes result sent to driver
15/06/03 17:16:05 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x8CK\xE5\x00\x86,\xE4\x9A\xB2c\x00\x85
15/06/03 17:16:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
15/06/03 17:16:05 INFO executor.Executor: Running task 13.0 in stage 1.0 (TID 14)
15/06/03 17:16:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:16:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:16:05 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x9A\xB2c\x00\x85,\xE4\xA7.\xA7\x00\x82
15/06/03 17:16:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:16:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:16:06 INFO regionserver.HStore: Closed V
15/06/03 17:16:06 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:16:06 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:16:06 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 8). 2207 bytes result sent to driver
15/06/03 17:16:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
15/06/03 17:16:06 INFO executor.Executor: Running task 14.0 in stage 1.0 (TID 15)
15/06/03 17:16:06 INFO regionserver.HStore: Closed V
15/06/03 17:16:06 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:16:06 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:16:06 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xA7.\xA7\x00\x82,\xE4\xB2\xBAC\x00\x82
15/06/03 17:16:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:06 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:06 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:06 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 7). 2207 bytes result sent to driver
15/06/03 17:16:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
15/06/03 17:16:06 INFO executor.Executor: Running task 15.0 in stage 1.0 (TID 16)
15/06/03 17:16:06 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xB2\xBAC\x00\x82,\xE4\xC1(\x85\x00\x85
15/06/03 17:16:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:06 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:06 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:09 INFO regionserver.HStore: Closed V
15/06/03 17:16:09 INFO regionserver.HRegion: Closed 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72.
15/06/03 17:16:09 WARN regionserver.HRegion: Region 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72. already closed
15/06/03 17:16:09 INFO executor.Executor: Finished task 10.0 in stage 1.0 (TID 11). 2207 bytes result sent to driver
15/06/03 17:16:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
15/06/03 17:16:09 INFO executor.Executor: Running task 16.0 in stage 1.0 (TID 17)
15/06/03 17:16:09 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xC1(\x85\x00\x85,\xE4\xCF\x8E\xE5\x00\x83
15/06/03 17:16:09 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:09 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:09 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:09 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:12 INFO regionserver.HStore: Closed V
15/06/03 17:16:12 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:16:12 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:16:12 INFO executor.Executor: Finished task 9.0 in stage 1.0 (TID 10). 2207 bytes result sent to driver
15/06/03 17:16:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
15/06/03 17:16:12 INFO executor.Executor: Running task 17.0 in stage 1.0 (TID 18)
15/06/03 17:16:12 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xCF\x8E\xE5\x00\x83,\xE4\xDD\xF5\xC2\x00\x85
15/06/03 17:16:12 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:12 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:12 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:12 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:19 INFO regionserver.HStore: Closed V
15/06/03 17:16:19 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:16:19 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:16:19 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 9). 2207 bytes result sent to driver
15/06/03 17:16:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
15/06/03 17:16:19 INFO executor.Executor: Running task 18.0 in stage 1.0 (TID 19)
15/06/03 17:16:19 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xDD\xF5\xC2\x00\x85,\xE4\xECc\xA7
15/06/03 17:16:19 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:19 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:19 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:19 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:26 INFO regionserver.HStore: Closed V
15/06/03 17:16:26 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:16:26 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:16:26 INFO executor.Executor: Finished task 11.0 in stage 1.0 (TID 12). 2207 bytes result sent to driver
15/06/03 17:16:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
15/06/03 17:16:26 INFO executor.Executor: Running task 19.0 in stage 1.0 (TID 20)
15/06/03 17:16:26 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xECc\xA7,\xE4\xF7k\x86\x00\x85
15/06/03 17:16:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:26 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:26 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:16:27 INFO regionserver.HStore: Closed V
15/06/03 17:16:27 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:16:27 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:16:27 INFO executor.Executor: Finished task 14.0 in stage 1.0 (TID 15). 2207 bytes result sent to driver
15/06/03 17:16:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
15/06/03 17:16:27 INFO executor.Executor: Running task 20.0 in stage 1.0 (TID 21)
15/06/03 17:16:27 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xF7k\x86\x00\x85,\xE5\x02\xF4\x83\x00\x82
15/06/03 17:16:27 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:27 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:27 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:27 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:27 INFO regionserver.HStore: Closed V
15/06/03 17:16:27 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:16:27 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:16:27 INFO executor.Executor: Finished task 13.0 in stage 1.0 (TID 14). 2207 bytes result sent to driver
15/06/03 17:16:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
15/06/03 17:16:27 INFO executor.Executor: Running task 21.0 in stage 1.0 (TID 22)
15/06/03 17:16:28 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x02\xF4\x83\x00\x82,\xE5\x11ae
15/06/03 17:16:28 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:28 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:28 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:28 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:34 INFO regionserver.HStore: Closed V
15/06/03 17:16:34 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:16:34 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:16:34 INFO executor.Executor: Finished task 12.0 in stage 1.0 (TID 13). 2207 bytes result sent to driver
15/06/03 17:16:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
15/06/03 17:16:34 INFO executor.Executor: Running task 22.0 in stage 1.0 (TID 23)
15/06/03 17:16:34 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x11ae,\xE5\x1F\xCB\xA1\x00\x83
15/06/03 17:16:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:34 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:34 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:35 INFO regionserver.HStore: Closed V
15/06/03 17:16:35 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:16:35 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:16:35 INFO executor.Executor: Finished task 15.0 in stage 1.0 (TID 16). 2207 bytes result sent to driver
15/06/03 17:16:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
15/06/03 17:16:35 INFO executor.Executor: Running task 23.0 in stage 1.0 (TID 24)
15/06/03 17:16:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x1F\xCB\xA1\x00\x83,\xE5.:\x08
15/06/03 17:16:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:36 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:36 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:36 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:39 INFO regionserver.HStore: Closed V
15/06/03 17:16:39 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:16:39 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:16:39 INFO executor.Executor: Finished task 16.0 in stage 1.0 (TID 17). 2207 bytes result sent to driver
15/06/03 17:16:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
15/06/03 17:16:39 INFO executor.Executor: Running task 24.0 in stage 1.0 (TID 25)
15/06/03 17:16:39 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5.:\x08,\xE5<\xA2D\x00\x85
15/06/03 17:16:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:39 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:39 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:42 INFO regionserver.HStore: Closed V
15/06/03 17:16:42 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:16:42 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:16:42 INFO executor.Executor: Finished task 17.0 in stage 1.0 (TID 18). 2207 bytes result sent to driver
15/06/03 17:16:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
15/06/03 17:16:42 INFO executor.Executor: Running task 25.0 in stage 1.0 (TID 26)
15/06/03 17:16:42 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5<\xA2D\x00\x85,\xE5CNa\x00\x85
15/06/03 17:16:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:42 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:42 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:16:53 INFO regionserver.HStore: Closed V
15/06/03 17:16:53 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:16:53 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:16:53 INFO executor.Executor: Finished task 18.0 in stage 1.0 (TID 19). 2207 bytes result sent to driver
15/06/03 17:16:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27
15/06/03 17:16:53 INFO executor.Executor: Running task 26.0 in stage 1.0 (TID 27)
15/06/03 17:16:53 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5CNa\x00\x85,\xE5N\xD3\x85\x00\x85
15/06/03 17:16:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:53 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:16:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:53 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:16:55 INFO regionserver.HStore: Closed V
15/06/03 17:16:55 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:16:55 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:16:55 INFO executor.Executor: Finished task 19.0 in stage 1.0 (TID 20). 2207 bytes result sent to driver
15/06/03 17:16:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28
15/06/03 17:16:55 INFO executor.Executor: Running task 27.0 in stage 1.0 (TID 28)
15/06/03 17:16:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5N\xD3\x85\x00\x85,\xE5]:\xC7\x00\x82
15/06/03 17:16:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:55 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:16:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:55 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:16:56 INFO regionserver.HStore: Closed V
15/06/03 17:16:56 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:16:56 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:16:56 INFO executor.Executor: Finished task 20.0 in stage 1.0 (TID 21). 2207 bytes result sent to driver
15/06/03 17:16:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29
15/06/03 17:16:56 INFO executor.Executor: Running task 28.0 in stage 1.0 (TID 29)
15/06/03 17:16:56 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5]:\xC7\x00\x82,\xE5k\xA2\xE1\x00\x83
15/06/03 17:16:56 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:56 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:16:56 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:16:56 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:17:03 INFO regionserver.HStore: Closed V
15/06/03 17:17:03 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:17:03 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:17:04 INFO executor.Executor: Finished task 25.0 in stage 1.0 (TID 26). 2207 bytes result sent to driver
15/06/03 17:17:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30
15/06/03 17:17:04 INFO executor.Executor: Running task 29.0 in stage 1.0 (TID 30)
15/06/03 17:17:04 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5k\xA2\xE1\x00\x83,\xE5z\x00G\x00\x81
15/06/03 17:17:04 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:04 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:17:04 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:04 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:17:06 INFO regionserver.HStore: Closed V
15/06/03 17:17:06 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:17:06 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:17:06 INFO executor.Executor: Finished task 21.0 in stage 1.0 (TID 22). 2207 bytes result sent to driver
15/06/03 17:17:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31
15/06/03 17:17:06 INFO executor.Executor: Running task 30.0 in stage 1.0 (TID 31)
15/06/03 17:17:06 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5z\x00G\x00\x81,\xE5\x86\x8B\x06
15/06/03 17:17:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:06 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:17:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:06 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:17:13 INFO regionserver.HStore: Closed V
15/06/03 17:17:13 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:17:13 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:17:14 INFO executor.Executor: Finished task 22.0 in stage 1.0 (TID 23). 2207 bytes result sent to driver
15/06/03 17:17:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32
15/06/03 17:17:14 INFO executor.Executor: Running task 31.0 in stage 1.0 (TID 32)
15/06/03 17:17:14 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x86\x8B\x06,\xE5\x92\x0Cc\x00\x82
15/06/03 17:17:14 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:14 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:14 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:14 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:15 INFO regionserver.HStore: Closed V
15/06/03 17:17:15 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:17:15 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:17:15 INFO executor.Executor: Finished task 23.0 in stage 1.0 (TID 24). 2207 bytes result sent to driver
15/06/03 17:17:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33
15/06/03 17:17:15 INFO executor.Executor: Running task 32.0 in stage 1.0 (TID 33)
15/06/03 17:17:15 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x92\x0Cc\x00\x82,\xE5\xA0q\x06\x00\x86
15/06/03 17:17:15 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:15 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:15 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:15 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:25 INFO regionserver.HStore: Closed V
15/06/03 17:17:25 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:17:25 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:17:25 INFO executor.Executor: Finished task 24.0 in stage 1.0 (TID 25). 2207 bytes result sent to driver
15/06/03 17:17:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34
15/06/03 17:17:25 INFO executor.Executor: Running task 33.0 in stage 1.0 (TID 34)
15/06/03 17:17:25 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xA0q\x06\x00\x86,\xE5\xAE\xD6B\x00\x82
15/06/03 17:17:25 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:25 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:25 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:25 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:31 INFO regionserver.HStore: Closed V
15/06/03 17:17:31 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:17:31 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:17:31 INFO executor.Executor: Finished task 26.0 in stage 1.0 (TID 27). 2207 bytes result sent to driver
15/06/03 17:17:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35
15/06/03 17:17:31 INFO executor.Executor: Running task 34.0 in stage 1.0 (TID 35)
15/06/03 17:17:31 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xAE\xD6B\x00\x82,\xE5\xBDA`\x00\x85
15/06/03 17:17:31 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:31 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:31 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:31 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:42 INFO regionserver.HStore: Closed V
15/06/03 17:17:42 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:17:42 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:17:42 INFO executor.Executor: Finished task 27.0 in stage 1.0 (TID 28). 2207 bytes result sent to driver
15/06/03 17:17:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36
15/06/03 17:17:42 INFO executor.Executor: Running task 35.0 in stage 1.0 (TID 36)
15/06/03 17:17:42 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xBDA`\x00\x85,
15/06/03 17:17:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:42 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:42 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:17:42 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:17:44 INFO regionserver.HStore: Closed V
15/06/03 17:17:44 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:17:44 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:17:44 INFO executor.Executor: Finished task 28.0 in stage 1.0 (TID 29). 2207 bytes result sent to driver
15/06/03 17:17:53 INFO regionserver.HStore: Closed V
15/06/03 17:17:53 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:17:53 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:17:53 INFO executor.Executor: Finished task 30.0 in stage 1.0 (TID 31). 2207 bytes result sent to driver
15/06/03 17:17:57 INFO regionserver.HStore: Closed V
15/06/03 17:17:57 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:17:57 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:17:57 INFO executor.Executor: Finished task 31.0 in stage 1.0 (TID 32). 2207 bytes result sent to driver
15/06/03 17:17:57 INFO regionserver.HStore: Closed V
15/06/03 17:17:57 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:17:57 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:17:57 INFO executor.Executor: Finished task 29.0 in stage 1.0 (TID 30). 2207 bytes result sent to driver
15/06/03 17:18:00 INFO regionserver.HStore: Closed V
15/06/03 17:18:00 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:18:00 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:18:00 INFO executor.Executor: Finished task 32.0 in stage 1.0 (TID 33). 2207 bytes result sent to driver
15/06/03 17:18:00 INFO regionserver.HStore: Closed V
15/06/03 17:18:00 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:18:00 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:18:00 INFO executor.Executor: Finished task 33.0 in stage 1.0 (TID 34). 2207 bytes result sent to driver
15/06/03 17:18:02 INFO regionserver.HStore: Closed V
15/06/03 17:18:02 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:18:02 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:18:02 INFO executor.Executor: Finished task 34.0 in stage 1.0 (TID 35). 2207 bytes result sent to driver
15/06/03 17:18:02 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 17:18:02 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb0019
15/06/03 17:18:02 INFO zookeeper.ZooKeeper: Session: 0x14dbb7837eb0019 closed
15/06/03 17:18:02 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 17:18:02 INFO regionserver.HStore: Closed V
15/06/03 17:18:02 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:18:02 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:18:02 INFO executor.Executor: Finished task 35.0 in stage 1.0 (TID 36). 2207 bytes result sent to driver
15/06/03 17:18:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37
15/06/03 17:18:02 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 37)
15/06/03 17:18:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38
15/06/03 17:18:02 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 38)
15/06/03 17:18:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39
15/06/03 17:18:02 INFO executor.Executor: Running task 2.0 in stage 2.0 (TID 39)
15/06/03 17:18:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40
15/06/03 17:18:02 INFO executor.Executor: Running task 3.0 in stage 2.0 (TID 40)
15/06/03 17:18:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41
15/06/03 17:18:02 INFO executor.Executor: Running task 4.0 in stage 2.0 (TID 41)
15/06/03 17:18:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
15/06/03 17:18:02 INFO executor.Executor: Running task 5.0 in stage 2.0 (TID 42)
15/06/03 17:18:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43
15/06/03 17:18:02 INFO executor.Executor: Running task 6.0 in stage 2.0 (TID 43)
15/06/03 17:18:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44
15/06/03 17:18:02 INFO executor.Executor: Running task 7.0 in stage 2.0 (TID 44)
15/06/03 17:18:02 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
15/06/03 17:18:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
15/06/03 17:18:02 INFO storage.MemoryStore: ensureFreeSpace(10193) called with curMem=891571, maxMem=740960501
15/06/03 17:18:02 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.0 KB, free 705.8 MB)
15/06/03 17:18:02 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/06/03 17:18:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 13 ms
15/06/03 17:18:02 INFO storage.MemoryStore: ensureFreeSpace(25984) called with curMem=901764, maxMem=740960501
15/06/03 17:18:02 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 25.4 KB, free 705.8 MB)
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@192.168.0.21:64531/user/MapOutputTracker#700156622]
15/06/03 17:18:03 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:03 INFO executor.Executor: Finished task 4.0 in stage 2.0 (TID 41). 1034 bytes result sent to driver
15/06/03 17:18:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45
15/06/03 17:18:03 INFO executor.Executor: Running task 8.0 in stage 2.0 (TID 45)
15/06/03 17:18:03 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 38). 1034 bytes result sent to driver
15/06/03 17:18:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46
15/06/03 17:18:03 INFO executor.Executor: Finished task 2.0 in stage 2.0 (TID 39). 1034 bytes result sent to driver
15/06/03 17:18:03 INFO executor.Executor: Running task 9.0 in stage 2.0 (TID 46)
15/06/03 17:18:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47
15/06/03 17:18:03 INFO executor.Executor: Finished task 3.0 in stage 2.0 (TID 40). 1034 bytes result sent to driver
15/06/03 17:18:03 INFO executor.Executor: Finished task 5.0 in stage 2.0 (TID 42). 1034 bytes result sent to driver
15/06/03 17:18:03 INFO executor.Executor: Finished task 6.0 in stage 2.0 (TID 43). 1034 bytes result sent to driver
15/06/03 17:18:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48
15/06/03 17:18:03 INFO executor.Executor: Running task 10.0 in stage 2.0 (TID 47)
15/06/03 17:18:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49
15/06/03 17:18:03 INFO executor.Executor: Running task 11.0 in stage 2.0 (TID 48)
15/06/03 17:18:03 INFO executor.Executor: Running task 12.0 in stage 2.0 (TID 49)
15/06/03 17:18:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50
15/06/03 17:18:03 INFO executor.Executor: Running task 13.0 in stage 2.0 (TID 50)
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:03 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:08 INFO executor.Executor: Finished task 9.0 in stage 2.0 (TID 46). 1034 bytes result sent to driver
15/06/03 17:18:08 INFO executor.Executor: Finished task 7.0 in stage 2.0 (TID 44). 1034 bytes result sent to driver
15/06/03 17:18:08 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 37). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO executor.Executor: Finished task 10.0 in stage 2.0 (TID 47). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51
15/06/03 17:18:09 INFO executor.Executor: Running task 14.0 in stage 2.0 (TID 51)
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52
15/06/03 17:18:09 INFO executor.Executor: Running task 15.0 in stage 2.0 (TID 52)
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53
15/06/03 17:18:09 INFO executor.Executor: Running task 16.0 in stage 2.0 (TID 53)
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54
15/06/03 17:18:09 INFO executor.Executor: Running task 17.0 in stage 2.0 (TID 54)
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:09 INFO executor.Executor: Finished task 12.0 in stage 2.0 (TID 49). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55
15/06/03 17:18:09 INFO executor.Executor: Running task 18.0 in stage 2.0 (TID 55)
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:09 INFO executor.Executor: Finished task 11.0 in stage 2.0 (TID 48). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
15/06/03 17:18:09 INFO executor.Executor: Running task 19.0 in stage 2.0 (TID 56)
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/06/03 17:18:09 INFO executor.Executor: Finished task 13.0 in stage 2.0 (TID 50). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57
15/06/03 17:18:09 INFO executor.Executor: Running task 20.0 in stage 2.0 (TID 57)
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/06/03 17:18:09 INFO executor.Executor: Finished task 8.0 in stage 2.0 (TID 45). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:09 INFO executor.Executor: Running task 21.0 in stage 2.0 (TID 58)
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:09 INFO executor.Executor: Finished task 15.0 in stage 2.0 (TID 52). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59
15/06/03 17:18:09 INFO executor.Executor: Finished task 14.0 in stage 2.0 (TID 51). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO executor.Executor: Finished task 16.0 in stage 2.0 (TID 53). 1034 bytes result sent to driver
15/06/03 17:18:09 INFO executor.Executor: Running task 22.0 in stage 2.0 (TID 59)
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60
15/06/03 17:18:09 INFO executor.Executor: Running task 23.0 in stage 2.0 (TID 60)
15/06/03 17:18:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61
15/06/03 17:18:09 INFO executor.Executor: Running task 24.0 in stage 2.0 (TID 61)
15/06/03 17:18:11 INFO executor.Executor: Finished task 19.0 in stage 2.0 (TID 56). 1034 bytes result sent to driver
15/06/03 17:18:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62
15/06/03 17:18:11 INFO executor.Executor: Running task 25.0 in stage 2.0 (TID 62)
15/06/03 17:18:11 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:18:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:11 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:11 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:11 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:11 INFO executor.Executor: Finished task 21.0 in stage 2.0 (TID 58). 1034 bytes result sent to driver
15/06/03 17:18:12 INFO executor.Executor: Finished task 20.0 in stage 2.0 (TID 57). 1034 bytes result sent to driver
15/06/03 17:18:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63
15/06/03 17:18:12 INFO executor.Executor: Running task 26.0 in stage 2.0 (TID 63)
15/06/03 17:18:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64
15/06/03 17:18:12 INFO executor.Executor: Running task 27.0 in stage 2.0 (TID 64)
15/06/03 17:18:12 INFO executor.Executor: Finished task 24.0 in stage 2.0 (TID 61). 1034 bytes result sent to driver
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65
15/06/03 17:18:12 INFO executor.Executor: Running task 28.0 in stage 2.0 (TID 65)
15/06/03 17:18:12 INFO executor.Executor: Finished task 22.0 in stage 2.0 (TID 59). 1034 bytes result sent to driver
15/06/03 17:18:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66
15/06/03 17:18:12 INFO executor.Executor: Running task 29.0 in stage 2.0 (TID 66)
15/06/03 17:18:12 INFO executor.Executor: Finished task 23.0 in stage 2.0 (TID 60). 1034 bytes result sent to driver
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67
15/06/03 17:18:12 INFO executor.Executor: Running task 30.0 in stage 2.0 (TID 67)
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:14 INFO executor.Executor: Finished task 28.0 in stage 2.0 (TID 65). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 29.0 in stage 2.0 (TID 66). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 18.0 in stage 2.0 (TID 55). 1042 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 27.0 in stage 2.0 (TID 64). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 30.0 in stage 2.0 (TID 67). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72
15/06/03 17:18:14 INFO executor.Executor: Running task 31.0 in stage 2.0 (TID 68)
15/06/03 17:18:14 INFO executor.Executor: Running task 32.0 in stage 2.0 (TID 69)
15/06/03 17:18:14 INFO executor.Executor: Running task 33.0 in stage 2.0 (TID 70)
15/06/03 17:18:14 INFO executor.Executor: Running task 34.0 in stage 2.0 (TID 71)
15/06/03 17:18:14 INFO executor.Executor: Running task 35.0 in stage 2.0 (TID 72)
15/06/03 17:18:14 INFO executor.Executor: Finished task 17.0 in stage 2.0 (TID 54). 1042 bytes result sent to driver
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:14 INFO executor.Executor: Finished task 32.0 in stage 2.0 (TID 69). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 33.0 in stage 2.0 (TID 70). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 35.0 in stage 2.0 (TID 72). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 31.0 in stage 2.0 (TID 68). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 25.0 in stage 2.0 (TID 62). 1042 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 34.0 in stage 2.0 (TID 71). 1034 bytes result sent to driver
15/06/03 17:18:14 INFO executor.Executor: Finished task 26.0 in stage 2.0 (TID 63). 1042 bytes result sent to driver
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73
15/06/03 17:18:14 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 73)
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74
15/06/03 17:18:14 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 74)
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75
15/06/03 17:18:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
15/06/03 17:18:14 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 75)
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76
15/06/03 17:18:14 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 76)
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77
15/06/03 17:18:14 INFO executor.Executor: Running task 4.0 in stage 4.0 (TID 77)
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
15/06/03 17:18:14 INFO executor.Executor: Running task 5.0 in stage 4.0 (TID 78)
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
15/06/03 17:18:14 INFO executor.Executor: Running task 6.0 in stage 4.0 (TID 79)
15/06/03 17:18:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80
15/06/03 17:18:14 INFO executor.Executor: Running task 7.0 in stage 4.0 (TID 80)
15/06/03 17:18:14 INFO storage.MemoryStore: ensureFreeSpace(10271) called with curMem=927748, maxMem=740960501
15/06/03 17:18:14 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.0 KB, free 705.7 MB)
15/06/03 17:18:14 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
15/06/03 17:18:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 15 ms
15/06/03 17:18:14 INFO storage.MemoryStore: ensureFreeSpace(26080) called with curMem=938019, maxMem=740960501
15/06/03 17:18:14 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 25.5 KB, free 705.7 MB)
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:15 INFO executor.Executor: Finished task 5.0 in stage 4.0 (TID 78). 1180 bytes result sent to driver
15/06/03 17:18:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81
15/06/03 17:18:15 INFO executor.Executor: Running task 8.0 in stage 4.0 (TID 81)
15/06/03 17:18:15 INFO executor.Executor: Finished task 7.0 in stage 4.0 (TID 80). 1180 bytes result sent to driver
15/06/03 17:18:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82
15/06/03 17:18:15 INFO executor.Executor: Running task 9.0 in stage 4.0 (TID 82)
15/06/03 17:18:15 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 74). 1180 bytes result sent to driver
15/06/03 17:18:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83
15/06/03 17:18:15 INFO executor.Executor: Running task 10.0 in stage 4.0 (TID 83)
15/06/03 17:18:15 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 75). 1180 bytes result sent to driver
15/06/03 17:18:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:15 INFO executor.Executor: Finished task 4.0 in stage 4.0 (TID 77). 1180 bytes result sent to driver
15/06/03 17:18:15 INFO executor.Executor: Running task 11.0 in stage 4.0 (TID 84)
15/06/03 17:18:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85
15/06/03 17:18:15 INFO executor.Executor: Running task 12.0 in stage 4.0 (TID 85)
15/06/03 17:18:15 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 76). 1180 bytes result sent to driver
15/06/03 17:18:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86
15/06/03 17:18:15 INFO executor.Executor: Running task 13.0 in stage 4.0 (TID 86)
15/06/03 17:18:15 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 73). 1180 bytes result sent to driver
15/06/03 17:18:15 INFO executor.Executor: Finished task 6.0 in stage 4.0 (TID 79). 1180 bytes result sent to driver
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/06/03 17:18:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:15 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:15 INFO executor.Executor: Running task 14.0 in stage 4.0 (TID 87)
15/06/03 17:18:15 INFO executor.Executor: Running task 15.0 in stage 4.0 (TID 88)
15/06/03 17:18:19 INFO executor.Executor: Finished task 10.0 in stage 4.0 (TID 83). 1180 bytes result sent to driver
15/06/03 17:18:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89
15/06/03 17:18:19 INFO executor.Executor: Running task 16.0 in stage 4.0 (TID 89)
15/06/03 17:18:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/06/03 17:18:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/06/03 17:18:20 INFO executor.Executor: Finished task 9.0 in stage 4.0 (TID 82). 1180 bytes result sent to driver
15/06/03 17:18:20 INFO executor.Executor: Finished task 13.0 in stage 4.0 (TID 86). 1180 bytes result sent to driver
15/06/03 17:18:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 90
15/06/03 17:18:20 INFO executor.Executor: Running task 17.0 in stage 4.0 (TID 90)
15/06/03 17:18:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 91
15/06/03 17:18:20 INFO executor.Executor: Running task 18.0 in stage 4.0 (TID 91)
15/06/03 17:18:20 INFO executor.Executor: Finished task 8.0 in stage 4.0 (TID 81). 1180 bytes result sent to driver
15/06/03 17:18:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 92
15/06/03 17:18:20 INFO executor.Executor: Running task 19.0 in stage 4.0 (TID 92)
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:20 INFO executor.Executor: Finished task 11.0 in stage 4.0 (TID 84). 1180 bytes result sent to driver
15/06/03 17:18:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 93
15/06/03 17:18:20 INFO executor.Executor: Running task 20.0 in stage 4.0 (TID 93)
15/06/03 17:18:20 INFO executor.Executor: Finished task 12.0 in stage 4.0 (TID 85). 1180 bytes result sent to driver
15/06/03 17:18:20 INFO executor.Executor: Finished task 14.0 in stage 4.0 (TID 87). 1180 bytes result sent to driver
15/06/03 17:18:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 94
15/06/03 17:18:20 INFO executor.Executor: Running task 21.0 in stage 4.0 (TID 94)
15/06/03 17:18:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 95
15/06/03 17:18:20 INFO executor.Executor: Running task 22.0 in stage 4.0 (TID 95)
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:20 INFO executor.Executor: Finished task 15.0 in stage 4.0 (TID 88). 1180 bytes result sent to driver
15/06/03 17:18:20 INFO executor.Executor: Finished task 16.0 in stage 4.0 (TID 89). 1180 bytes result sent to driver
15/06/03 17:18:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 96
15/06/03 17:18:20 INFO executor.Executor: Running task 23.0 in stage 4.0 (TID 96)
15/06/03 17:18:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 97
15/06/03 17:18:20 INFO executor.Executor: Running task 24.0 in stage 4.0 (TID 97)
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:21 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:21 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:21 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:21 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/06/03 17:18:21 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:21 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/06/03 17:18:22 INFO executor.Executor: Finished task 19.0 in stage 4.0 (TID 92). 1180 bytes result sent to driver
15/06/03 17:18:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 98
15/06/03 17:18:22 INFO executor.Executor: Running task 25.0 in stage 4.0 (TID 98)
15/06/03 17:18:22 INFO executor.Executor: Finished task 21.0 in stage 4.0 (TID 94). 1180 bytes result sent to driver
15/06/03 17:18:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 99
15/06/03 17:18:22 INFO executor.Executor: Running task 26.0 in stage 4.0 (TID 99)
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:22 INFO executor.Executor: Finished task 22.0 in stage 4.0 (TID 95). 1180 bytes result sent to driver
15/06/03 17:18:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 100
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:22 INFO executor.Executor: Finished task 20.0 in stage 4.0 (TID 93). 1180 bytes result sent to driver
15/06/03 17:18:22 INFO executor.Executor: Running task 27.0 in stage 4.0 (TID 100)
15/06/03 17:18:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 101
15/06/03 17:18:22 INFO executor.Executor: Running task 28.0 in stage 4.0 (TID 101)
15/06/03 17:18:22 INFO executor.Executor: Finished task 23.0 in stage 4.0 (TID 96). 1180 bytes result sent to driver
15/06/03 17:18:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 102
15/06/03 17:18:22 INFO executor.Executor: Running task 29.0 in stage 4.0 (TID 102)
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:22 INFO executor.Executor: Finished task 24.0 in stage 4.0 (TID 97). 1180 bytes result sent to driver
15/06/03 17:18:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 103
15/06/03 17:18:22 INFO storage.BlockManager: Removing broadcast 3
15/06/03 17:18:22 INFO executor.Executor: Running task 30.0 in stage 4.0 (TID 103)
15/06/03 17:18:22 INFO storage.BlockManager: Removing block broadcast_3_piece0
15/06/03 17:18:22 INFO storage.MemoryStore: Block broadcast_3_piece0 of size 10159 dropped from memory (free 740006561)
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:22 INFO storage.BlockManagerMaster: Updated info of block broadcast_3_piece0
15/06/03 17:18:22 INFO storage.BlockManager: Removing block broadcast_3
15/06/03 17:18:22 INFO storage.MemoryStore: Block broadcast_3 of size 25624 dropped from memory (free 740032185)
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:22 INFO storage.BlockManager: Removing broadcast 4
15/06/03 17:18:22 INFO storage.BlockManager: Removing block broadcast_4
15/06/03 17:18:22 INFO storage.MemoryStore: Block broadcast_4 of size 25984 dropped from memory (free 740058169)
15/06/03 17:18:22 INFO storage.BlockManager: Removing block broadcast_4_piece0
15/06/03 17:18:22 INFO storage.MemoryStore: Block broadcast_4_piece0 of size 10193 dropped from memory (free 740068362)
15/06/03 17:18:22 INFO executor.Executor: Finished task 28.0 in stage 4.0 (TID 101). 1180 bytes result sent to driver
15/06/03 17:18:22 INFO storage.BlockManagerMaster: Updated info of block broadcast_4_piece0
15/06/03 17:18:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 104
15/06/03 17:18:22 INFO executor.Executor: Running task 31.0 in stage 4.0 (TID 104)
15/06/03 17:18:22 INFO executor.Executor: Finished task 29.0 in stage 4.0 (TID 102). 1180 bytes result sent to driver
15/06/03 17:18:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 105
15/06/03 17:18:22 INFO executor.Executor: Running task 32.0 in stage 4.0 (TID 105)
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:22 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:23 INFO executor.Executor: Finished task 27.0 in stage 4.0 (TID 100). 1180 bytes result sent to driver
15/06/03 17:18:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 106
15/06/03 17:18:23 INFO executor.Executor: Running task 33.0 in stage 4.0 (TID 106)
15/06/03 17:18:23 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:23 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:23 INFO executor.Executor: Finished task 30.0 in stage 4.0 (TID 103). 1180 bytes result sent to driver
15/06/03 17:18:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 107
15/06/03 17:18:23 INFO executor.Executor: Running task 34.0 in stage 4.0 (TID 107)
15/06/03 17:18:23 INFO executor.Executor: Finished task 31.0 in stage 4.0 (TID 104). 1180 bytes result sent to driver
15/06/03 17:18:23 INFO executor.Executor: Finished task 32.0 in stage 4.0 (TID 105). 1180 bytes result sent to driver
15/06/03 17:18:23 INFO executor.Executor: Finished task 18.0 in stage 4.0 (TID 91). 1180 bytes result sent to driver
15/06/03 17:18:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 108
15/06/03 17:18:23 INFO executor.Executor: Running task 35.0 in stage 4.0 (TID 108)
15/06/03 17:18:23 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:23 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:23 INFO executor.Executor: Finished task 17.0 in stage 4.0 (TID 90). 1180 bytes result sent to driver
15/06/03 17:18:23 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:23 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:23 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 36 blocks
15/06/03 17:18:23 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:23 INFO executor.Executor: Finished task 33.0 in stage 4.0 (TID 106). 1180 bytes result sent to driver
15/06/03 17:18:24 INFO executor.Executor: Finished task 26.0 in stage 4.0 (TID 99). 1180 bytes result sent to driver
15/06/03 17:18:24 INFO executor.Executor: Finished task 35.0 in stage 4.0 (TID 108). 1180 bytes result sent to driver
15/06/03 17:18:24 INFO executor.Executor: Finished task 34.0 in stage 4.0 (TID 107). 1180 bytes result sent to driver
15/06/03 17:18:24 INFO executor.Executor: Finished task 25.0 in stage 4.0 (TID 98). 1180 bytes result sent to driver
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 109
15/06/03 17:18:24 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 109)
15/06/03 17:18:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(9613) called with curMem=892139, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.4 KB, free 705.8 MB)
15/06/03 17:18:24 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 10 ms
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(21848) called with curMem=901752, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 21.3 KB, free 705.8 MB)
15/06/03 17:18:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/06/03 17:18:24 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@192.168.0.21:64531/user/MapOutputTracker#700156622]
15/06/03 17:18:24 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 36 blocks
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:24 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 109). 1026 bytes result sent to driver
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 110
15/06/03 17:18:24 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 110)
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(9613) called with curMem=923600, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.4 KB, free 705.7 MB)
15/06/03 17:18:24 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 12 ms
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(21848) called with curMem=933213, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.3 KB, free 705.7 MB)
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 36 blocks
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:24 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 110). 1021 bytes result sent to driver
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 111
15/06/03 17:18:24 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 111)
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(9613) called with curMem=955061, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.4 KB, free 705.7 MB)
15/06/03 17:18:24 INFO storage.BlockManagerMaster: Updated info of block broadcast_8_piece0
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 8 ms
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(21848) called with curMem=964674, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 21.3 KB, free 705.7 MB)
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 36 blocks
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:24 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 111). 1025 bytes result sent to driver
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 112
15/06/03 17:18:24 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 112)
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(9613) called with curMem=986522, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.4 KB, free 705.7 MB)
15/06/03 17:18:24 INFO storage.BlockManagerMaster: Updated info of block broadcast_9_piece0
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 11 ms
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(21848) called with curMem=996135, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 21.3 KB, free 705.7 MB)
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 36 blocks
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:18:24 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 112). 1026 bytes result sent to driver
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 113
15/06/03 17:18:24 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 113)
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(9613) called with curMem=1017983, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.4 KB, free 705.7 MB)
15/06/03 17:18:24 INFO storage.BlockManagerMaster: Updated info of block broadcast_10_piece0
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 10 ms
15/06/03 17:18:24 INFO storage.MemoryStore: ensureFreeSpace(21848) called with curMem=1027596, maxMem=740960501
15/06/03 17:18:24 INFO storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 21.3 KB, free 705.6 MB)
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 36 blocks
15/06/03 17:18:24 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:18:24 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 113). 920 bytes result sent to driver
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 114
15/06/03 17:18:24 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 114)
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 115
15/06/03 17:18:24 INFO executor.Executor: Running task 1.0 in stage 18.0 (TID 115)
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 116
15/06/03 17:18:24 INFO executor.Executor: Running task 2.0 in stage 18.0 (TID 116)
15/06/03 17:18:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 117
15/06/03 17:18:24 INFO executor.Executor: Running task 3.0 in stage 18.0 (TID 117)
15/06/03 17:18:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11
15/06/03 17:18:25 INFO storage.MemoryStore: ensureFreeSpace(15355) called with curMem=1049444, maxMem=740960501
15/06/03 17:18:25 INFO storage.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 15.0 KB, free 705.6 MB)
15/06/03 17:18:25 INFO storage.BlockManagerMaster: Updated info of block broadcast_11_piece0
15/06/03 17:18:25 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 10 ms
15/06/03 17:18:25 INFO storage.MemoryStore: ensureFreeSpace(62992) called with curMem=1064799, maxMem=740960501
15/06/03 17:18:25 INFO storage.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 61.5 KB, free 705.6 MB)
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 3.0 in stage 18.0 (TID 117)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 2.0 in stage 18.0 (TID 116)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 1.0 in stage 18.0 (TID 115)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 0.0 in stage 18.0 (TID 114)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 118
15/06/03 17:18:25 INFO executor.Executor: Running task 2.1 in stage 18.0 (TID 118)
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 119
15/06/03 17:18:25 INFO executor.Executor: Running task 0.1 in stage 18.0 (TID 119)
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 120
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 121
15/06/03 17:18:25 INFO executor.Executor: Running task 3.1 in stage 18.0 (TID 120)
15/06/03 17:18:25 INFO executor.Executor: Running task 1.1 in stage 18.0 (TID 121)
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 0.1 in stage 18.0 (TID 119)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 2.1 in stage 18.0 (TID 118)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 3.1 in stage 18.0 (TID 120)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 122
15/06/03 17:18:25 INFO executor.Executor: Running task 0.2 in stage 18.0 (TID 122)
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 123
15/06/03 17:18:25 INFO executor.Executor: Running task 2.2 in stage 18.0 (TID 123)
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 1.1 in stage 18.0 (TID 121)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 124
15/06/03 17:18:25 INFO executor.Executor: Running task 3.2 in stage 18.0 (TID 124)
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 125
15/06/03 17:18:25 INFO executor.Executor: Running task 1.2 in stage 18.0 (TID 125)
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 3.2 in stage 18.0 (TID 124)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 126
15/06/03 17:18:25 INFO executor.Executor: Running task 3.3 in stage 18.0 (TID 126)
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 1.2 in stage 18.0 (TID 125)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 127
15/06/03 17:18:25 INFO executor.Executor: Running task 1.3 in stage 18.0 (TID 127)
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 2.2 in stage 18.0 (TID 123)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 128
15/06/03 17:18:25 INFO executor.Executor: Running task 2.3 in stage 18.0 (TID 128)
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 0.2 in stage 18.0 (TID 122)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 129
15/06/03 17:18:25 INFO executor.Executor: Running task 0.3 in stage 18.0 (TID 129)
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 3.3 in stage 18.0 (TID 126)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 1.3 in stage 18.0 (TID 127)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 INFO executor.Executor: Executor is trying to kill task 0.3 in stage 18.0 (TID 129)
15/06/03 17:18:25 INFO executor.Executor: Executor is trying to kill task 2.3 in stage 18.0 (TID 128)
15/06/03 17:18:25 WARN spark.SpliceSpark: Initializing Spark with:
 master local[8]
 home null
 jars 
 environment 
15/06/03 17:18:25 WARN spark.SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
15/06/03 17:18:25 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
15/06/03 17:18:25 ERROR executor.Executor: Exception in task 2.3 in stage 18.0 (TID 128)
ERROR SE001: Splice Engine exception: unexpected exception
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:319)
	at com.splicemachine.pipeline.exception.ErrorState$12.newException(ErrorState.java:1860)
	at com.splicemachine.pipeline.exception.Exceptions.parseException(Exceptions.java:101)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:570)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:48)
	at com.splicemachine.derby.stream.function.NLJInnerJoinFunction.call(NLJInnerJoinFunction.java:21)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:111)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:614)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Found both spark.executor.extraJavaOptions and SPARK_JAVA_OPTS. Use only the former.
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:312)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5$$anonfun$apply$4.apply(SparkConf.scala:310)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:310)
	at org.apache.spark.SparkConf$$anonfun$validateSettings$5.apply(SparkConf.scala:296)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:296)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:176)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.splicemachine.derby.impl.spark.SpliceSpark.initializeSparkContext(SpliceSpark.java:137)
	at com.splicemachine.derby.impl.spark.SpliceSpark.getContext(SpliceSpark.java:54)
	at com.splicemachine.derby.stream.spark.SparkOperationContext.<init>(SparkOperationContext.java:47)
	at com.splicemachine.derby.stream.spark.SparkDataSetProcessor.createOperationContext(SparkDataSetProcessor.java:63)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:276)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.getDataSet(SpliceBaseOperation.java:555)
	at com.splicemachine.derby.impl.sql.execute.operations.MergeSortJoinOperation.getDataSet(MergeSortJoinOperation.java:175)
	at com.splicemachine.derby.impl.sql.execute.operations.ProjectRestrictOperation.getDataSet(ProjectRestrictOperation.java:277)
	at com.splicemachine.derby.impl.sql.execute.operations.SpliceBaseOperation.openCore(SpliceBaseOperation.java:568)
	... 23 more
15/06/03 17:18:25 INFO executor.Executor: Executor killed task 0.3 in stage 18.0 (TID 129)
15/06/03 17:18:28 INFO storage.BlockManager: Removing broadcast 5
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_5_piece0
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_5_piece0 of size 10271 dropped from memory (free 739842981)
15/06/03 17:18:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_5_piece0
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_5
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_5 of size 26080 dropped from memory (free 739869061)
15/06/03 17:18:28 INFO storage.BlockManager: Removing broadcast 6
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_6_piece0
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_6_piece0 of size 9613 dropped from memory (free 739878674)
15/06/03 17:18:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_6_piece0
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_6
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_6 of size 21848 dropped from memory (free 739900522)
15/06/03 17:18:28 INFO storage.BlockManager: Removing broadcast 7
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_7
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_7 of size 21848 dropped from memory (free 739922370)
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_7_piece0
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_7_piece0 of size 9613 dropped from memory (free 739931983)
15/06/03 17:18:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_7_piece0
15/06/03 17:18:28 INFO storage.BlockManager: Removing broadcast 8
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_8
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_8 of size 21848 dropped from memory (free 739953831)
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_8_piece0
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_8_piece0 of size 9613 dropped from memory (free 739963444)
15/06/03 17:18:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_8_piece0
15/06/03 17:18:28 INFO storage.BlockManager: Removing broadcast 9
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_9
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_9 of size 21848 dropped from memory (free 739985292)
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_9_piece0
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_9_piece0 of size 9613 dropped from memory (free 739994905)
15/06/03 17:18:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_9_piece0
15/06/03 17:18:28 INFO storage.BlockManager: Removing broadcast 10
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_10
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_10 of size 21848 dropped from memory (free 740016753)
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_10_piece0
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_10_piece0 of size 9613 dropped from memory (free 740026366)
15/06/03 17:18:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_10_piece0
15/06/03 17:18:28 INFO storage.BlockManager: Removing broadcast 11
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_11
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_11 of size 62992 dropped from memory (free 740089358)
15/06/03 17:18:28 INFO storage.BlockManager: Removing block broadcast_11_piece0
15/06/03 17:18:28 INFO storage.MemoryStore: Block broadcast_11_piece0 of size 15355 dropped from memory (free 740104713)
15/06/03 17:18:28 INFO storage.BlockManagerMaster: Updated info of block broadcast_11_piece0
15/06/03 17:34:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 130
15/06/03 17:34:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 131
15/06/03 17:34:52 INFO executor.Executor: Running task 0.0 in stage 19.0 (TID 130)
15/06/03 17:34:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 132
15/06/03 17:34:52 INFO executor.Executor: Running task 1.0 in stage 19.0 (TID 131)
15/06/03 17:34:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 133
15/06/03 17:34:52 INFO executor.Executor: Running task 2.0 in stage 19.0 (TID 132)
15/06/03 17:34:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 134
15/06/03 17:34:52 INFO executor.Executor: Running task 3.0 in stage 19.0 (TID 133)
15/06/03 17:34:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 135
15/06/03 17:34:52 INFO executor.Executor: Running task 4.0 in stage 19.0 (TID 134)
15/06/03 17:34:52 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
15/06/03 17:34:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 136
15/06/03 17:34:52 INFO executor.Executor: Running task 5.0 in stage 19.0 (TID 135)
15/06/03 17:34:52 INFO executor.Executor: Running task 6.0 in stage 19.0 (TID 136)
15/06/03 17:34:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 137
15/06/03 17:34:52 INFO executor.Executor: Running task 7.0 in stage 19.0 (TID 137)
15/06/03 17:34:52 INFO storage.MemoryStore: ensureFreeSpace(9387) called with curMem=855788, maxMem=740960501
15/06/03 17:34:52 INFO storage.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.2 KB, free 705.8 MB)
15/06/03 17:34:52 INFO storage.BlockManagerMaster: Updated info of block broadcast_14_piece0
15/06/03 17:34:52 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 14 ms
15/06/03 17:34:52 INFO storage.MemoryStore: ensureFreeSpace(22032) called with curMem=865175, maxMem=740960501
15/06/03 17:34:52 INFO storage.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 21.5 KB, free 705.8 MB)
15/06/03 17:34:52 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4=\xD3\xE7,\xE4I_'\x00\x84
15/06/03 17:34:52 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE46\xDE!\x00\x83,\xE4=\xD3\xE7
15/06/03 17:34:52 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x0B\xAA\xC1\x00\x84,\xE4\x1A\x15\x07
15/06/03 17:34:52 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4W\xCF\x00\x00\x84,\xE4f9\xC6
15/06/03 17:34:52 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x1A\x15\x07,\xE4(\x7F\xE1\x00\x85
15/06/03 17:34:52 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4(\x7F\xE1\x00\x85,\xE46\xDE!\x00\x83
15/06/03 17:34:52 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,\xE4\x0B\xAA\xC1\x00\x84
15/06/03 17:34:52 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4I_'\x00\x84,\xE4W\xCF\x00\x00\x84
15/06/03 17:34:52 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
15/06/03 17:34:52 INFO storage.MemoryStore: ensureFreeSpace(24580) called with curMem=887207, maxMem=740960501
15/06/03 17:34:52 INFO storage.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 24.0 KB, free 705.8 MB)
15/06/03 17:34:52 INFO storage.BlockManagerMaster: Updated info of block broadcast_13_piece0
15/06/03 17:34:52 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 7 ms
15/06/03 17:34:53 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=911787, maxMem=740960501
15/06/03 17:34:53 INFO storage.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 380.7 KB, free 705.4 MB)
15/06/03 17:34:53 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x22b3aa09 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 17:34:53 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x22b3aa09, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 17:34:53 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:34:53 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:34:53 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb001a, negotiated timeout = 60000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:34:53 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:35:00 INFO regionserver.HStore: Closed V
15/06/03 17:35:00 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:35:00 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:35:00 INFO executor.Executor: Finished task 4.0 in stage 19.0 (TID 134). 2212 bytes result sent to driver
15/06/03 17:35:00 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 138
15/06/03 17:35:00 INFO executor.Executor: Running task 8.0 in stage 19.0 (TID 138)
15/06/03 17:35:00 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4f9\xC6,\xE4t\xA2B\x00\x82
15/06/03 17:35:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:00 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:35:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:00 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:35:03 INFO regionserver.HStore: Closed V
15/06/03 17:35:03 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:35:03 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:35:03 INFO executor.Executor: Finished task 5.0 in stage 19.0 (TID 135). 2212 bytes result sent to driver
15/06/03 17:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 139
15/06/03 17:35:03 INFO executor.Executor: Running task 9.0 in stage 19.0 (TID 139)
15/06/03 17:35:03 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4t\xA2B\x00\x82,\xE4{\xA2C\x00\x83
15/06/03 17:35:03 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:03 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:35:03 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:03 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:35:03 INFO regionserver.HStore: Closed V
15/06/03 17:35:03 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:35:03 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:35:03 INFO executor.Executor: Finished task 0.0 in stage 19.0 (TID 130). 2212 bytes result sent to driver
15/06/03 17:35:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 140
15/06/03 17:35:03 INFO executor.Executor: Running task 10.0 in stage 19.0 (TID 140)
15/06/03 17:35:03 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4{\xA2C\x00\x83,\xE4\x80\xCB\xE7\x00\x83
15/06/03 17:35:03 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:03 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 17:35:03 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:03 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 17:35:05 INFO regionserver.HStore: Closed V
15/06/03 17:35:05 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:35:05 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:35:05 INFO executor.Executor: Finished task 3.0 in stage 19.0 (TID 133). 2212 bytes result sent to driver
15/06/03 17:35:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 141
15/06/03 17:35:05 INFO executor.Executor: Running task 11.0 in stage 19.0 (TID 141)
15/06/03 17:35:05 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x80\xCB\xE7\x00\x83,\xE4\x8CK\xE5\x00\x86
15/06/03 17:35:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:35:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:35:05 INFO regionserver.HStore: Closed V
15/06/03 17:35:05 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:35:05 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:35:05 INFO regionserver.HStore: Closed V
15/06/03 17:35:05 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:35:05 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:35:05 INFO executor.Executor: Finished task 6.0 in stage 19.0 (TID 136). 2212 bytes result sent to driver
15/06/03 17:35:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 142
15/06/03 17:35:05 INFO executor.Executor: Running task 12.0 in stage 19.0 (TID 142)
15/06/03 17:35:05 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x8CK\xE5\x00\x86,\xE4\x9A\xB2c\x00\x85
15/06/03 17:35:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:05 INFO executor.Executor: Finished task 1.0 in stage 19.0 (TID 131). 2212 bytes result sent to driver
15/06/03 17:35:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:35:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 143
15/06/03 17:35:05 INFO executor.Executor: Running task 13.0 in stage 19.0 (TID 143)
15/06/03 17:35:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:35:05 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x9A\xB2c\x00\x85,\xE4\xA7.\xA7\x00\x82
15/06/03 17:35:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:35:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:05 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:35:05 INFO regionserver.HStore: Closed V
15/06/03 17:35:05 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:35:05 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:35:05 INFO executor.Executor: Finished task 2.0 in stage 19.0 (TID 132). 2212 bytes result sent to driver
15/06/03 17:35:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 144
15/06/03 17:35:05 INFO executor.Executor: Running task 14.0 in stage 19.0 (TID 144)
15/06/03 17:35:05 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xA7.\xA7\x00\x82,\xE4\xB2\xBAC\x00\x82
15/06/03 17:35:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:05 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:05 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:06 INFO regionserver.HStore: Closed V
15/06/03 17:35:06 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:35:06 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:35:06 INFO executor.Executor: Finished task 7.0 in stage 19.0 (TID 137). 2212 bytes result sent to driver
15/06/03 17:35:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 145
15/06/03 17:35:06 INFO executor.Executor: Running task 15.0 in stage 19.0 (TID 145)
15/06/03 17:35:06 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xB2\xBAC\x00\x82,\xE4\xC1(\x85\x00\x85
15/06/03 17:35:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:06 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:06 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:07 INFO regionserver.HStore: Closed V
15/06/03 17:35:07 INFO regionserver.HRegion: Closed 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72.
15/06/03 17:35:07 WARN regionserver.HRegion: Region 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72. already closed
15/06/03 17:35:07 INFO executor.Executor: Finished task 10.0 in stage 19.0 (TID 140). 2049 bytes result sent to driver
15/06/03 17:35:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 146
15/06/03 17:35:07 INFO executor.Executor: Running task 16.0 in stage 19.0 (TID 146)
15/06/03 17:35:07 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xC1(\x85\x00\x85,\xE4\xCF\x8E\xE5\x00\x83
15/06/03 17:35:07 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:07 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:07 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:07 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:08 INFO regionserver.HStore: Closed V
15/06/03 17:35:08 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:35:08 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:35:08 INFO executor.Executor: Finished task 9.0 in stage 19.0 (TID 139). 2049 bytes result sent to driver
15/06/03 17:35:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 147
15/06/03 17:35:08 INFO executor.Executor: Running task 17.0 in stage 19.0 (TID 147)
15/06/03 17:35:08 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xCF\x8E\xE5\x00\x83,\xE4\xDD\xF5\xC2\x00\x85
15/06/03 17:35:08 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:08 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:08 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:08 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:10 INFO regionserver.HStore: Closed V
15/06/03 17:35:10 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:35:10 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:35:10 INFO executor.Executor: Finished task 8.0 in stage 19.0 (TID 138). 2049 bytes result sent to driver
15/06/03 17:35:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 148
15/06/03 17:35:10 INFO executor.Executor: Running task 18.0 in stage 19.0 (TID 148)
15/06/03 17:35:10 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xDD\xF5\xC2\x00\x85,\xE4\xECc\xA7
15/06/03 17:35:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:10 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:10 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:13 INFO regionserver.HStore: Closed V
15/06/03 17:35:13 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:35:13 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:35:13 INFO executor.Executor: Finished task 11.0 in stage 19.0 (TID 141). 2212 bytes result sent to driver
15/06/03 17:35:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 149
15/06/03 17:35:13 INFO executor.Executor: Running task 19.0 in stage 19.0 (TID 149)
15/06/03 17:35:13 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xECc\xA7,\xE4\xF7k\x86\x00\x85
15/06/03 17:35:13 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:13 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:13 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:13 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:35:14 INFO regionserver.HStore: Closed V
15/06/03 17:35:14 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:35:14 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:35:14 INFO executor.Executor: Finished task 14.0 in stage 19.0 (TID 144). 2212 bytes result sent to driver
15/06/03 17:35:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 150
15/06/03 17:35:14 INFO executor.Executor: Running task 20.0 in stage 19.0 (TID 150)
15/06/03 17:35:14 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xF7k\x86\x00\x85,\xE5\x02\xF4\x83\x00\x82
15/06/03 17:35:14 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:14 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:14 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:14 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:14 INFO regionserver.HStore: Closed V
15/06/03 17:35:14 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:35:14 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:35:14 INFO executor.Executor: Finished task 13.0 in stage 19.0 (TID 143). 2212 bytes result sent to driver
15/06/03 17:35:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 151
15/06/03 17:35:14 INFO executor.Executor: Running task 21.0 in stage 19.0 (TID 151)
15/06/03 17:35:14 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x02\xF4\x83\x00\x82,\xE5\x11ae
15/06/03 17:35:14 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:14 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:14 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:14 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:15 INFO regionserver.HStore: Closed V
15/06/03 17:35:15 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:35:15 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:35:15 INFO executor.Executor: Finished task 12.0 in stage 19.0 (TID 142). 2212 bytes result sent to driver
15/06/03 17:35:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 152
15/06/03 17:35:15 INFO executor.Executor: Running task 22.0 in stage 19.0 (TID 152)
15/06/03 17:35:15 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x11ae,\xE5\x1F\xCB\xA1\x00\x83
15/06/03 17:35:15 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:15 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:15 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:15 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:16 INFO regionserver.HStore: Closed V
15/06/03 17:35:16 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:35:16 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:35:16 INFO executor.Executor: Finished task 15.0 in stage 19.0 (TID 145). 2212 bytes result sent to driver
15/06/03 17:35:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 153
15/06/03 17:35:16 INFO executor.Executor: Running task 23.0 in stage 19.0 (TID 153)
15/06/03 17:35:16 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x1F\xCB\xA1\x00\x83,\xE5.:\x08
15/06/03 17:35:16 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:16 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:16 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:16 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:17 INFO regionserver.HStore: Closed V
15/06/03 17:35:17 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:35:17 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:35:17 INFO executor.Executor: Finished task 16.0 in stage 19.0 (TID 146). 2212 bytes result sent to driver
15/06/03 17:35:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 154
15/06/03 17:35:17 INFO executor.Executor: Running task 24.0 in stage 19.0 (TID 154)
15/06/03 17:35:17 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5.:\x08,\xE5<\xA2D\x00\x85
15/06/03 17:35:17 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:17 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:17 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:17 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:18 INFO regionserver.HStore: Closed V
15/06/03 17:35:18 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:35:18 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:35:18 INFO executor.Executor: Finished task 17.0 in stage 19.0 (TID 147). 2212 bytes result sent to driver
15/06/03 17:35:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 155
15/06/03 17:35:18 INFO executor.Executor: Running task 25.0 in stage 19.0 (TID 155)
15/06/03 17:35:18 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5<\xA2D\x00\x85,\xE5CNa\x00\x85
15/06/03 17:35:18 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:18 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:18 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:18 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:35:20 INFO regionserver.HStore: Closed V
15/06/03 17:35:20 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:35:20 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:35:20 INFO executor.Executor: Finished task 18.0 in stage 19.0 (TID 148). 2212 bytes result sent to driver
15/06/03 17:35:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 156
15/06/03 17:35:20 INFO executor.Executor: Running task 26.0 in stage 19.0 (TID 156)
15/06/03 17:35:20 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5CNa\x00\x85,\xE5N\xD3\x85\x00\x85
15/06/03 17:35:20 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:20 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:20 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:20 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:20 INFO regionserver.HStore: Closed V
15/06/03 17:35:20 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:35:20 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:35:20 INFO executor.Executor: Finished task 19.0 in stage 19.0 (TID 149). 2212 bytes result sent to driver
15/06/03 17:35:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 157
15/06/03 17:35:20 INFO executor.Executor: Running task 27.0 in stage 19.0 (TID 157)
15/06/03 17:35:20 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5N\xD3\x85\x00\x85,\xE5]:\xC7\x00\x82
15/06/03 17:35:20 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:20 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:21 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:21 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:22 INFO regionserver.HStore: Closed V
15/06/03 17:35:22 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:35:22 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:35:22 INFO executor.Executor: Finished task 20.0 in stage 19.0 (TID 150). 2212 bytes result sent to driver
15/06/03 17:35:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 158
15/06/03 17:35:22 INFO executor.Executor: Running task 28.0 in stage 19.0 (TID 158)
15/06/03 17:35:22 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5]:\xC7\x00\x82,\xE5k\xA2\xE1\x00\x83
15/06/03 17:35:22 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:22 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:22 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:22 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:23 INFO regionserver.HStore: Closed V
15/06/03 17:35:23 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:35:23 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:35:23 INFO executor.Executor: Finished task 25.0 in stage 19.0 (TID 155). 2212 bytes result sent to driver
15/06/03 17:35:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 159
15/06/03 17:35:23 INFO executor.Executor: Running task 29.0 in stage 19.0 (TID 159)
15/06/03 17:35:23 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5k\xA2\xE1\x00\x83,\xE5z\x00G\x00\x81
15/06/03 17:35:23 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:23 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:23 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:23 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:24 INFO regionserver.HStore: Closed V
15/06/03 17:35:24 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:35:24 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:35:24 INFO executor.Executor: Finished task 21.0 in stage 19.0 (TID 151). 2212 bytes result sent to driver
15/06/03 17:35:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 160
15/06/03 17:35:24 INFO executor.Executor: Running task 30.0 in stage 19.0 (TID 160)
15/06/03 17:35:24 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5z\x00G\x00\x81,\xE5\x86\x8B\x06
15/06/03 17:35:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:24 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:24 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:35:25 INFO regionserver.HStore: Closed V
15/06/03 17:35:25 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:35:25 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:35:25 INFO executor.Executor: Finished task 22.0 in stage 19.0 (TID 152). 2212 bytes result sent to driver
15/06/03 17:35:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 161
15/06/03 17:35:25 INFO executor.Executor: Running task 31.0 in stage 19.0 (TID 161)
15/06/03 17:35:25 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x86\x8B\x06,\xE5\x92\x0Cc\x00\x82
15/06/03 17:35:25 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:25 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:25 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:25 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:26 INFO regionserver.HStore: Closed V
15/06/03 17:35:26 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:35:26 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:35:26 INFO executor.Executor: Finished task 23.0 in stage 19.0 (TID 153). 2212 bytes result sent to driver
15/06/03 17:35:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 162
15/06/03 17:35:26 INFO executor.Executor: Running task 32.0 in stage 19.0 (TID 162)
15/06/03 17:35:26 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x92\x0Cc\x00\x82,\xE5\xA0q\x06\x00\x86
15/06/03 17:35:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:26 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:26 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:27 INFO regionserver.HStore: Closed V
15/06/03 17:35:27 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:35:27 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:35:27 INFO executor.Executor: Finished task 24.0 in stage 19.0 (TID 154). 2212 bytes result sent to driver
15/06/03 17:35:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 163
15/06/03 17:35:27 INFO executor.Executor: Running task 33.0 in stage 19.0 (TID 163)
15/06/03 17:35:27 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xA0q\x06\x00\x86,\xE5\xAE\xD6B\x00\x82
15/06/03 17:35:27 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:27 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:27 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:27 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:28 INFO regionserver.HStore: Closed V
15/06/03 17:35:28 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:35:28 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:35:28 INFO executor.Executor: Finished task 26.0 in stage 19.0 (TID 156). 2212 bytes result sent to driver
15/06/03 17:35:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 164
15/06/03 17:35:28 INFO executor.Executor: Running task 34.0 in stage 19.0 (TID 164)
15/06/03 17:35:28 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xAE\xD6B\x00\x82,\xE5\xBDA`\x00\x85
15/06/03 17:35:28 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:28 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:28 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:28 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:30 INFO regionserver.HStore: Closed V
15/06/03 17:35:30 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:35:30 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:35:31 INFO executor.Executor: Finished task 27.0 in stage 19.0 (TID 157). 2212 bytes result sent to driver
15/06/03 17:35:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 165
15/06/03 17:35:31 INFO executor.Executor: Running task 35.0 in stage 19.0 (TID 165)
15/06/03 17:35:31 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xBDA`\x00\x85,
15/06/03 17:35:31 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:31 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:31 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:31 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:35:33 INFO regionserver.HStore: Closed V
15/06/03 17:35:33 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:35:33 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:35:34 INFO executor.Executor: Finished task 28.0 in stage 19.0 (TID 158). 2212 bytes result sent to driver
15/06/03 17:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 166
15/06/03 17:35:34 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 166)
15/06/03 17:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
15/06/03 17:35:34 INFO storage.MemoryStore: ensureFreeSpace(9165) called with curMem=1301582, maxMem=740960501
15/06/03 17:35:34 INFO storage.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 9.0 KB, free 705.4 MB)
15/06/03 17:35:34 INFO storage.BlockManagerMaster: Updated info of block broadcast_15_piece0
15/06/03 17:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 15 ms
15/06/03 17:35:34 INFO storage.MemoryStore: ensureFreeSpace(20288) called with curMem=1310747, maxMem=740960501
15/06/03 17:35:34 INFO storage.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 19.8 KB, free 705.4 MB)
15/06/03 17:35:34 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,\xE42Y\x08
15/06/03 17:35:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12
15/06/03 17:35:34 INFO storage.MemoryStore: ensureFreeSpace(24633) called with curMem=1331035, maxMem=740960501
15/06/03 17:35:34 INFO storage.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 24.1 KB, free 705.3 MB)
15/06/03 17:35:34 INFO storage.BlockManagerMaster: Updated info of block broadcast_12_piece0
15/06/03 17:35:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 8 ms
15/06/03 17:35:34 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=1355668, maxMem=740960501
15/06/03 17:35:34 INFO storage.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 380.7 KB, free 705.0 MB)
15/06/03 17:35:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:34 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:35:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:34 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:35:34 INFO regionserver.HStore: Closed V
15/06/03 17:35:34 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:35:34 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:35:34 INFO executor.Executor: Finished task 29.0 in stage 19.0 (TID 159). 2212 bytes result sent to driver
15/06/03 17:35:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 167
15/06/03 17:35:34 INFO executor.Executor: Running task 1.0 in stage 20.0 (TID 167)
15/06/03 17:35:34 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE42Y\x08,\xE4q \x05
15/06/03 17:35:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:34 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:35:34 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:34 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:35:35 INFO regionserver.HStore: Closed V
15/06/03 17:35:35 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:35:35 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:35:35 INFO executor.Executor: Finished task 30.0 in stage 19.0 (TID 160). 2212 bytes result sent to driver
15/06/03 17:35:35 INFO regionserver.HStore: Closed V
15/06/03 17:35:35 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:35:35 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 168
15/06/03 17:35:35 INFO executor.Executor: Running task 2.0 in stage 20.0 (TID 168)
15/06/03 17:35:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4q \x05,\xE4\xAF\xEED
15/06/03 17:35:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:35 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:35:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:35 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:35:35 INFO executor.Executor: Finished task 31.0 in stage 19.0 (TID 161). 2212 bytes result sent to driver
15/06/03 17:35:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 169
15/06/03 17:35:35 INFO executor.Executor: Running task 3.0 in stage 20.0 (TID 169)
15/06/03 17:35:35 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xAF\xEED,\xE4\xCA[\x84
15/06/03 17:35:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:35 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:35:35 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:35 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:35:37 INFO regionserver.HStore: Closed V
15/06/03 17:35:37 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 17:35:37 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 17:35:37 INFO executor.Executor: Finished task 3.0 in stage 20.0 (TID 169). 1999 bytes result sent to driver
15/06/03 17:35:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 170
15/06/03 17:35:37 INFO executor.Executor: Running task 4.0 in stage 20.0 (TID 170)
15/06/03 17:35:37 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xCA[\x84,\xE4\xFC\x90'
15/06/03 17:35:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:37 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:35:37 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:37 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:35:38 INFO regionserver.HStore: Closed V
15/06/03 17:35:38 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:35:38 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:35:38 INFO executor.Executor: Finished task 32.0 in stage 19.0 (TID 162). 2212 bytes result sent to driver
15/06/03 17:35:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 171
15/06/03 17:35:38 INFO executor.Executor: Running task 5.0 in stage 20.0 (TID 171)
15/06/03 17:35:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xFC\x90',\xE5;h!
15/06/03 17:35:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:38 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:35:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:38 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:35:38 INFO regionserver.HStore: Closed V
15/06/03 17:35:38 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 17:35:38 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 17:35:38 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 166). 1999 bytes result sent to driver
15/06/03 17:35:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 172
15/06/03 17:35:38 INFO executor.Executor: Running task 6.0 in stage 20.0 (TID 172)
15/06/03 17:35:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5;h!,\xE5z)h
15/06/03 17:35:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:38 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:35:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:38 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:35:39 INFO regionserver.HStore: Closed V
15/06/03 17:35:39 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:35:39 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:35:39 INFO executor.Executor: Finished task 33.0 in stage 19.0 (TID 163). 2212 bytes result sent to driver
15/06/03 17:35:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 173
15/06/03 17:35:39 INFO executor.Executor: Running task 7.0 in stage 20.0 (TID 173)
15/06/03 17:35:39 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5z)h,\xE5\x94\xAE\x01
15/06/03 17:35:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:39 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:35:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:39 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:35:39 INFO regionserver.HStore: Closed V
15/06/03 17:35:39 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 17:35:39 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 17:35:39 INFO executor.Executor: Finished task 1.0 in stage 20.0 (TID 167). 1999 bytes result sent to driver
15/06/03 17:35:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 174
15/06/03 17:35:39 INFO executor.Executor: Running task 8.0 in stage 20.0 (TID 174)
15/06/03 17:35:39 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x94\xAE\x01,\xE5\xC6\xE6\x87
15/06/03 17:35:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:39 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 17:35:39 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:39 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 17:35:39 INFO regionserver.HStore: Closed V
15/06/03 17:35:39 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:35:39 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:35:39 INFO regionserver.HStore: Closed V
15/06/03 17:35:39 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 17:35:39 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 17:35:40 INFO executor.Executor: Finished task 34.0 in stage 19.0 (TID 164). 2212 bytes result sent to driver
15/06/03 17:35:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 175
15/06/03 17:35:40 INFO executor.Executor: Running task 9.0 in stage 20.0 (TID 175)
15/06/03 17:35:40 INFO executor.Executor: Finished task 2.0 in stage 20.0 (TID 168). 1999 bytes result sent to driver
15/06/03 17:35:40 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xC6\xE6\x87,
15/06/03 17:35:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:40 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 17:35:40 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:35:40 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 17:35:40 INFO regionserver.HStore: Closed V
15/06/03 17:35:40 INFO regionserver.HRegion: Closed 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893.
15/06/03 17:35:40 WARN regionserver.HRegion: Region 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893. already closed
15/06/03 17:35:40 INFO executor.Executor: Finished task 9.0 in stage 20.0 (TID 175). 1999 bytes result sent to driver
15/06/03 17:35:41 INFO regionserver.HStore: Closed V
15/06/03 17:35:41 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:35:41 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:35:41 INFO executor.Executor: Finished task 35.0 in stage 19.0 (TID 165). 2212 bytes result sent to driver
15/06/03 17:35:41 INFO regionserver.HStore: Closed V
15/06/03 17:35:41 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 17:35:41 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 17:35:41 INFO executor.Executor: Finished task 4.0 in stage 20.0 (TID 170). 2162 bytes result sent to driver
15/06/03 17:35:41 INFO regionserver.HStore: Closed V
15/06/03 17:35:41 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 17:35:41 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 17:35:41 INFO executor.Executor: Finished task 7.0 in stage 20.0 (TID 173). 2162 bytes result sent to driver
15/06/03 17:35:42 INFO regionserver.HStore: Closed V
15/06/03 17:35:42 INFO regionserver.HRegion: Closed 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893.
15/06/03 17:35:42 WARN regionserver.HRegion: Region 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893. already closed
15/06/03 17:35:42 INFO executor.Executor: Finished task 8.0 in stage 20.0 (TID 174). 2162 bytes result sent to driver
15/06/03 17:35:42 INFO regionserver.HStore: Closed V
15/06/03 17:35:42 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 17:35:42 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 17:35:42 INFO executor.Executor: Finished task 5.0 in stage 20.0 (TID 171). 2162 bytes result sent to driver
15/06/03 17:35:42 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 17:35:42 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb001a
15/06/03 17:35:42 INFO zookeeper.ZooKeeper: Session: 0x14dbb7837eb001a closed
15/06/03 17:35:42 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 17:35:42 INFO regionserver.HStore: Closed V
15/06/03 17:35:42 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 17:35:42 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 17:35:43 INFO executor.Executor: Finished task 6.0 in stage 20.0 (TID 172). 2162 bytes result sent to driver
15/06/03 17:35:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 176
15/06/03 17:35:43 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 176)
15/06/03 17:35:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 177
15/06/03 17:35:43 INFO executor.Executor: Running task 1.0 in stage 21.0 (TID 177)
15/06/03 17:35:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 178
15/06/03 17:35:43 INFO executor.Executor: Running task 2.0 in stage 21.0 (TID 178)
15/06/03 17:35:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 179
15/06/03 17:35:43 INFO executor.Executor: Running task 3.0 in stage 21.0 (TID 179)
15/06/03 17:35:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 180
15/06/03 17:35:43 INFO executor.Executor: Running task 4.0 in stage 21.0 (TID 180)
15/06/03 17:35:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 181
15/06/03 17:35:43 INFO executor.Executor: Running task 5.0 in stage 21.0 (TID 181)
15/06/03 17:35:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 182
15/06/03 17:35:43 INFO executor.Executor: Running task 6.0 in stage 21.0 (TID 182)
15/06/03 17:35:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 183
15/06/03 17:35:43 INFO executor.Executor: Running task 7.0 in stage 21.0 (TID 183)
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
15/06/03 17:35:43 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
15/06/03 17:35:43 INFO storage.MemoryStore: ensureFreeSpace(9698) called with curMem=1745463, maxMem=740960501
15/06/03 17:35:43 INFO storage.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 9.5 KB, free 705.0 MB)
15/06/03 17:35:43 INFO storage.BlockManagerMaster: Updated info of block broadcast_16_piece0
15/06/03 17:35:43 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 9 ms
15/06/03 17:35:43 INFO storage.MemoryStore: ensureFreeSpace(23080) called with curMem=1755161, maxMem=740960501
15/06/03 17:35:43 INFO storage.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 22.5 KB, free 704.9 MB)
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@192.168.0.21:64531/user/MapOutputTracker#700156622]
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@192.168.0.21:64531/user/MapOutputTracker#700156622]
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
15/06/03 17:35:43 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:43 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 32768, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 4.0 in stage 21.0 (TID 180)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 184
15/06/03 17:35:44 INFO executor.Executor: Running task 8.0 in stage 21.0 (TID 184)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 276065, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 2.0 in stage 21.0 (TID 178)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 185
15/06/03 17:35:44 INFO executor.Executor: Running task 4.1 in stage 21.0 (TID 185)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 405794, 1971-12-23, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 5.0 in stage 21.0 (TID 181)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 186
15/06/03 17:35:44 INFO executor.Executor: Running task 2.1 in stage 21.0 (TID 186)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 544581, 1994-06-11, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 6.0 in stage 21.0 (TID 182)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 187
15/06/03 17:35:44 INFO executor.Executor: Running task 5.1 in stage 21.0 (TID 187)
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 507394, 1970-01-19, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 1.0 in stage 21.0 (TID 177)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 566304, 1994-02-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 188
15/06/03 17:35:44 INFO executor.Executor: Running task 6.1 in stage 21.0 (TID 188)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 3.0 in stage 21.0 (TID 179)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 571431, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 0.0 in stage 21.0 (TID 176)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 189
15/06/03 17:35:44 INFO executor.Executor: Running task 1.1 in stage 21.0 (TID 189)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 190
15/06/03 17:35:44 INFO executor.Executor: Running task 3.1 in stage 21.0 (TID 190)
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 559744, 1992-10-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 7.0 in stage 21.0 (TID 183)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { -323606302, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 8.0 in stage 21.0 (TID 184)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 191
15/06/03 17:35:44 INFO executor.Executor: Running task 0.1 in stage 21.0 (TID 191)
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 192
15/06/03 17:35:44 INFO executor.Executor: Running task 8.1 in stage 21.0 (TID 192)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 32768, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 4.1 in stage 21.0 (TID 185)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 193
15/06/03 17:35:44 INFO executor.Executor: Running task 7.1 in stage 21.0 (TID 193)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 276065, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 2.1 in stage 21.0 (TID 186)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 194
15/06/03 17:35:44 INFO executor.Executor: Running task 4.2 in stage 21.0 (TID 194)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 507394, 1970-01-19, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 1.1 in stage 21.0 (TID 189)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 195
15/06/03 17:35:44 INFO executor.Executor: Running task 2.2 in stage 21.0 (TID 195)
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 566304, 1994-02-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 3.1 in stage 21.0 (TID 190)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 544581, 1994-06-11, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 6.1 in stage 21.0 (TID 188)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 196
15/06/03 17:35:44 INFO executor.Executor: Running task 1.2 in stage 21.0 (TID 196)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 197
15/06/03 17:35:44 INFO executor.Executor: Running task 3.2 in stage 21.0 (TID 197)
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 405794, 1971-12-23, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 571431, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 5.1 in stage 21.0 (TID 187)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 0.1 in stage 21.0 (TID 191)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 198
15/06/03 17:35:44 INFO executor.Executor: Running task 6.2 in stage 21.0 (TID 198)
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 199
15/06/03 17:35:44 INFO executor.Executor: Running task 9.0 in stage 21.0 (TID 199)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { -323606302, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 8.1 in stage 21.0 (TID 192)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 200
15/06/03 17:35:44 INFO executor.Executor: Running task 0.2 in stage 21.0 (TID 200)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 32768, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 4.2 in stage 21.0 (TID 194)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 201
15/06/03 17:35:44 INFO executor.Executor: Running task 8.2 in stage 21.0 (TID 201)
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 559744, 1992-10-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 7.1 in stage 21.0 (TID 193)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 276065, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 2.2 in stage 21.0 (TID 195)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 202
15/06/03 17:35:44 INFO executor.Executor: Running task 4.3 in stage 21.0 (TID 202)
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 203
15/06/03 17:35:44 INFO executor.Executor: Running task 7.2 in stage 21.0 (TID 203)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 507394, 1970-01-19, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 1.2 in stage 21.0 (TID 196)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { -1920, 1995-09-20, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 9.0 in stage 21.0 (TID 199)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 204
15/06/03 17:35:44 INFO executor.Executor: Running task 2.3 in stage 21.0 (TID 204)
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 205
15/06/03 17:35:44 INFO executor.Executor: Running task 1.3 in stage 21.0 (TID 205)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 566304, 1994-02-13, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 3.2 in stage 21.0 (TID 197)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 206
15/06/03 17:35:44 INFO executor.Executor: Running task 9.1 in stage 21.0 (TID 206)
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 544581, 1994-06-11, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 6.2 in stage 21.0 (TID 198)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 207
15/06/03 17:35:44 INFO executor.Executor: Running task 3.3 in stage 21.0 (TID 207)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { -323606302, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 8.2 in stage 21.0 (TID 201)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1102)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1090)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 208
15/06/03 17:35:44 INFO executor.Executor: Running task 6.3 in stage 21.0 (TID 208)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 32768, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 4.3 in stage 21.0 (TID 202)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 209
15/06/03 17:35:44 INFO executor.Executor: Running task 8.3 in stage 21.0 (TID 209)
15/06/03 17:35:44 ERROR kryo.SparkValueRowSerializer: Exception while deserializing row with template { 571431, 1969-12-31, NULL }
ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:35:44 ERROR executor.Executor: Exception in task 0.2 in stage 21.0 (TID 200)
java.lang.RuntimeException: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.utils.SpliceLogUtils.logAndThrowRuntime(SpliceLogUtils.java:67)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:120)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22007: The string representation of a datetime value is out of range.
Splice Machine Release: 1.1.1-SNAPSHOT
Splice Machine Version Hash: 08c20a115b
Splice Machine Build Time: 2015-06-03 17:28 +0000
	at com.splicemachine.db.iapi.error.StandardException.newException(StandardException.java:299)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:976)
	at com.splicemachine.db.iapi.types.SQLDate.computeEncodedDate(SQLDate.java:1088)
	at com.splicemachine.db.iapi.types.SQLDate.setValue(SQLDate.java:555)
	at com.splicemachine.db.iapi.types.DataType.setValue(DataType.java:361)
	at com.splicemachine.derby.utils.marshall.dvd.DateDescriptorSerializer.decode(DateDescriptorSerializer.java:41)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	... 51 more
15/06/03 17:35:44 INFO executor.Executor: Executor is trying to kill task 2.3 in stage 21.0 (TID 204)
15/06/03 17:35:44 INFO executor.Executor: Executor is trying to kill task 6.3 in stage 21.0 (TID 208)
15/06/03 17:35:44 INFO executor.Executor: Executor is trying to kill task 1.3 in stage 21.0 (TID 205)
15/06/03 17:35:44 INFO executor.Executor: Executor is trying to kill task 8.3 in stage 21.0 (TID 209)
15/06/03 17:35:44 INFO executor.Executor: Executor is trying to kill task 9.1 in stage 21.0 (TID 206)
15/06/03 17:35:44 INFO executor.Executor: Executor killed task 1.3 in stage 21.0 (TID 205)
15/06/03 17:35:44 INFO executor.Executor: Executor killed task 9.1 in stage 21.0 (TID 206)
15/06/03 17:35:44 INFO executor.Executor: Executor killed task 2.3 in stage 21.0 (TID 204)
15/06/03 17:35:44 INFO executor.Executor: Executor is trying to kill task 7.2 in stage 21.0 (TID 203)
15/06/03 17:35:44 INFO executor.Executor: Executor is trying to kill task 3.3 in stage 21.0 (TID 207)
15/06/03 17:35:44 INFO executor.Executor: Executor killed task 7.2 in stage 21.0 (TID 203)
15/06/03 17:35:44 INFO executor.Executor: Executor killed task 3.3 in stage 21.0 (TID 207)
15/06/03 17:35:44 INFO executor.Executor: Executor killed task 6.3 in stage 21.0 (TID 208)
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:35:44 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:35:44 INFO executor.Executor: Executor killed task 8.3 in stage 21.0 (TID 209)
15/06/03 17:37:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 210
15/06/03 17:37:17 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 210)
15/06/03 17:37:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
15/06/03 17:37:17 INFO storage.MemoryStore: ensureFreeSpace(10656) called with curMem=1778241, maxMem=740960501
15/06/03 17:37:17 INFO storage.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 10.4 KB, free 704.9 MB)
15/06/03 17:37:17 INFO storage.BlockManagerMaster: Updated info of block broadcast_19_piece0
15/06/03 17:37:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 8 ms
15/06/03 17:37:17 INFO storage.MemoryStore: ensureFreeSpace(24448) called with curMem=1788897, maxMem=740960501
15/06/03 17:37:17 INFO storage.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 23.9 KB, free 704.9 MB)
15/06/03 17:37:17 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,
15/06/03 17:37:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
15/06/03 17:37:17 INFO storage.MemoryStore: ensureFreeSpace(24490) called with curMem=1813345, maxMem=740960501
15/06/03 17:37:17 INFO storage.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 23.9 KB, free 704.9 MB)
15/06/03 17:37:17 INFO storage.BlockManagerMaster: Updated info of block broadcast_18_piece0
15/06/03 17:37:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 7 ms
15/06/03 17:37:17 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=1837835, maxMem=740960501
15/06/03 17:37:17 INFO storage.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 380.7 KB, free 704.5 MB)
15/06/03 17:37:17 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x652bd380 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 17:37:17 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x652bd380, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 17:37:17 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:37:17 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:37:17 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb001b, negotiated timeout = 60000
15/06/03 17:37:17 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:17 INFO regionserver.HRegion: Onlined 7faeee339b2231d747e63345d70c6bfc; next sequenceid=156
15/06/03 17:37:18 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:18 INFO regionserver.HRegion: Onlined 7faeee339b2231d747e63345d70c6bfc; next sequenceid=156
15/06/03 17:37:19 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 17:37:19 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb001b
15/06/03 17:37:19 INFO zookeeper.ZooKeeper: Session: 0x14dbb7837eb001b closed
15/06/03 17:37:19 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 17:37:19 INFO regionserver.HStore: Closed V
15/06/03 17:37:19 INFO regionserver.HRegion: Closed 1392,,1433352630566.7faeee339b2231d747e63345d70c6bfc.
15/06/03 17:37:19 WARN regionserver.HRegion: Region 1392,,1433352630566.7faeee339b2231d747e63345d70c6bfc. already closed
15/06/03 17:37:20 INFO storage.MemoryStore: ensureFreeSpace(23093604) called with curMem=2227630, maxMem=740960501
15/06/03 17:37:20 INFO storage.MemoryStore: Block taskresult_210 stored as bytes in memory (estimated size 22.0 MB, free 682.5 MB)
15/06/03 17:37:20 INFO storage.BlockManagerMaster: Updated info of block taskresult_210
15/06/03 17:37:20 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 210). 23093604 bytes result sent via BlockManager)
15/06/03 17:37:20 INFO storage.BlockManager: Removing block taskresult_210
15/06/03 17:37:20 INFO storage.MemoryStore: Block taskresult_210 of size 23093604 dropped from memory (free 738732871)
15/06/03 17:37:20 INFO storage.BlockManagerMaster: Updated info of block taskresult_210
15/06/03 17:37:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 211
15/06/03 17:37:23 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 211)
15/06/03 17:37:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22
15/06/03 17:37:23 INFO storage.MemoryStore: ensureFreeSpace(10775) called with curMem=2227630, maxMem=740960501
15/06/03 17:37:23 INFO storage.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 10.5 KB, free 704.5 MB)
15/06/03 17:37:23 INFO storage.BlockManagerMaster: Updated info of block broadcast_22_piece0
15/06/03 17:37:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 9 ms
15/06/03 17:37:23 INFO storage.MemoryStore: ensureFreeSpace(24704) called with curMem=2238405, maxMem=740960501
15/06/03 17:37:23 INFO storage.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 24.1 KB, free 704.5 MB)
15/06/03 17:37:23 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,
15/06/03 17:37:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
15/06/03 17:37:23 INFO storage.MemoryStore: ensureFreeSpace(24512) called with curMem=2263109, maxMem=740960501
15/06/03 17:37:23 INFO storage.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 23.9 KB, free 704.5 MB)
15/06/03 17:37:23 INFO storage.BlockManagerMaster: Updated info of block broadcast_21_piece0
15/06/03 17:37:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 6 ms
15/06/03 17:37:23 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=2287621, maxMem=740960501
15/06/03 17:37:23 INFO storage.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 380.7 KB, free 704.1 MB)
15/06/03 17:37:23 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6d3d080 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 17:37:23 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x6d3d080, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 17:37:23 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:37:23 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:37:23 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb001c, negotiated timeout = 60000
15/06/03 17:37:23 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:23 INFO regionserver.HRegion: Onlined d65bd0417f9cf02dd3f1226e5df1e576; next sequenceid=4
15/06/03 17:37:23 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:23 INFO regionserver.HRegion: Onlined d65bd0417f9cf02dd3f1226e5df1e576; next sequenceid=4
15/06/03 17:37:23 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 17:37:23 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb001c
15/06/03 17:37:23 INFO zookeeper.ZooKeeper: Session: 0x14dbb7837eb001c closed
15/06/03 17:37:23 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 17:37:23 INFO regionserver.HStore: Closed V
15/06/03 17:37:23 INFO regionserver.HRegion: Closed 1472,,1433352632097.d65bd0417f9cf02dd3f1226e5df1e576.
15/06/03 17:37:23 WARN regionserver.HRegion: Region 1472,,1433352632097.d65bd0417f9cf02dd3f1226e5df1e576. already closed
15/06/03 17:37:23 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 211). 2547 bytes result sent to driver
15/06/03 17:37:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 212
15/06/03 17:37:24 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 212)
15/06/03 17:37:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
15/06/03 17:37:24 INFO storage.MemoryStore: ensureFreeSpace(10866) called with curMem=2677416, maxMem=740960501
15/06/03 17:37:24 INFO storage.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.6 KB, free 704.1 MB)
15/06/03 17:37:24 INFO storage.BlockManagerMaster: Updated info of block broadcast_25_piece0
15/06/03 17:37:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 9 ms
15/06/03 17:37:24 INFO storage.MemoryStore: ensureFreeSpace(24960) called with curMem=2688282, maxMem=740960501
15/06/03 17:37:24 INFO storage.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 24.4 KB, free 704.0 MB)
15/06/03 17:37:24 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,
15/06/03 17:37:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
15/06/03 17:37:24 INFO storage.MemoryStore: ensureFreeSpace(24488) called with curMem=2713242, maxMem=740960501
15/06/03 17:37:24 INFO storage.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 23.9 KB, free 704.0 MB)
15/06/03 17:37:24 INFO storage.BlockManagerMaster: Updated info of block broadcast_24_piece0
15/06/03 17:37:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 7 ms
15/06/03 17:37:24 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=2737730, maxMem=740960501
15/06/03 17:37:24 INFO storage.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 380.7 KB, free 703.7 MB)
15/06/03 17:37:24 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x16b00856 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 17:37:24 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x16b00856, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 17:37:24 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:37:24 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:37:24 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb001d, negotiated timeout = 60000
15/06/03 17:37:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:24 INFO regionserver.HRegion: Onlined 12bec0d4e38b2e121553db95fd50c0e4; next sequenceid=13
15/06/03 17:37:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:24 INFO regionserver.HRegion: Onlined 12bec0d4e38b2e121553db95fd50c0e4; next sequenceid=13
15/06/03 17:37:24 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 17:37:24 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb001d
15/06/03 17:37:24 INFO zookeeper.ZooKeeper: Session: 0x14dbb7837eb001d closed
15/06/03 17:37:24 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 17:37:25 INFO regionserver.HStore: Closed V
15/06/03 17:37:25 INFO regionserver.HRegion: Closed 1424,,1433352631192.12bec0d4e38b2e121553db95fd50c0e4.
15/06/03 17:37:25 WARN regionserver.HRegion: Region 1424,,1433352631192.12bec0d4e38b2e121553db95fd50c0e4. already closed
15/06/03 17:37:25 INFO executor.Executor: Finished task 0.0 in stage 25.0 (TID 212). 1500825 bytes result sent to driver
15/06/03 17:37:25 INFO storage.BlockManager: Removing broadcast 21
15/06/03 17:37:25 INFO storage.BlockManager: Removing block broadcast_21_piece0
15/06/03 17:37:25 INFO storage.MemoryStore: Block broadcast_21_piece0 of size 24512 dropped from memory (free 737857488)
15/06/03 17:37:25 INFO storage.BlockManagerMaster: Updated info of block broadcast_21_piece0
15/06/03 17:37:25 INFO storage.BlockManager: Removing block broadcast_21
15/06/03 17:37:25 INFO storage.MemoryStore: Block broadcast_21 of size 389795 dropped from memory (free 738247283)
15/06/03 17:37:25 INFO storage.BlockManager: Removing broadcast 22
15/06/03 17:37:25 INFO storage.BlockManager: Removing block broadcast_22_piece0
15/06/03 17:37:25 INFO storage.MemoryStore: Block broadcast_22_piece0 of size 10775 dropped from memory (free 738258058)
15/06/03 17:37:25 INFO storage.BlockManagerMaster: Updated info of block broadcast_22_piece0
15/06/03 17:37:25 INFO storage.BlockManager: Removing block broadcast_22
15/06/03 17:37:25 INFO storage.MemoryStore: Block broadcast_22 of size 24704 dropped from memory (free 738282762)
15/06/03 17:37:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 213
15/06/03 17:37:26 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 213)
15/06/03 17:37:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
15/06/03 17:37:26 INFO storage.MemoryStore: ensureFreeSpace(10949) called with curMem=2677739, maxMem=740960501
15/06/03 17:37:26 INFO storage.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 10.7 KB, free 704.1 MB)
15/06/03 17:37:26 INFO storage.BlockManagerMaster: Updated info of block broadcast_28_piece0
15/06/03 17:37:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 8 ms
15/06/03 17:37:26 INFO storage.MemoryStore: ensureFreeSpace(25216) called with curMem=2688688, maxMem=740960501
15/06/03 17:37:26 INFO storage.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 24.6 KB, free 704.0 MB)
15/06/03 17:37:26 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,
15/06/03 17:37:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
15/06/03 17:37:26 INFO storage.MemoryStore: ensureFreeSpace(24546) called with curMem=2713904, maxMem=740960501
15/06/03 17:37:26 INFO storage.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 24.0 KB, free 704.0 MB)
15/06/03 17:37:26 INFO storage.BlockManagerMaster: Updated info of block broadcast_27_piece0
15/06/03 17:37:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 7 ms
15/06/03 17:37:26 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=2738450, maxMem=740960501
15/06/03 17:37:26 INFO storage.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 380.7 KB, free 703.7 MB)
15/06/03 17:37:26 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x578e1eeb connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 17:37:26 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x578e1eeb, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 17:37:26 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:37:26 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:37:26 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb001e, negotiated timeout = 60000
15/06/03 17:37:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:26 INFO regionserver.HRegion: Onlined d733d18772687ae27aba4ee87ef6e01c; next sequenceid=4
15/06/03 17:37:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:26 INFO regionserver.HRegion: Onlined d733d18772687ae27aba4ee87ef6e01c; next sequenceid=4
15/06/03 17:37:26 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 17:37:26 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb001e
15/06/03 17:37:26 INFO zookeeper.ZooKeeper: Session: 0x14dbb7837eb001e closed
15/06/03 17:37:26 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 17:37:26 INFO regionserver.HStore: Closed V
15/06/03 17:37:26 INFO regionserver.HRegion: Closed 1456,,1433352631799.d733d18772687ae27aba4ee87ef6e01c.
15/06/03 17:37:26 WARN regionserver.HRegion: Region 1456,,1433352631799.d733d18772687ae27aba4ee87ef6e01c. already closed
15/06/03 17:37:26 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 213). 1819 bytes result sent to driver
15/06/03 17:37:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 214
15/06/03 17:37:38 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 214)
15/06/03 17:37:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 215
15/06/03 17:37:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 216
15/06/03 17:37:38 INFO executor.Executor: Running task 1.0 in stage 27.0 (TID 215)
15/06/03 17:37:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31
15/06/03 17:37:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 217
15/06/03 17:37:38 INFO executor.Executor: Running task 2.0 in stage 27.0 (TID 216)
15/06/03 17:37:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 218
15/06/03 17:37:38 INFO executor.Executor: Running task 3.0 in stage 27.0 (TID 217)
15/06/03 17:37:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 219
15/06/03 17:37:38 INFO executor.Executor: Running task 4.0 in stage 27.0 (TID 218)
15/06/03 17:37:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 220
15/06/03 17:37:38 INFO executor.Executor: Running task 5.0 in stage 27.0 (TID 219)
15/06/03 17:37:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 221
15/06/03 17:37:38 INFO executor.Executor: Running task 6.0 in stage 27.0 (TID 220)
15/06/03 17:37:38 INFO executor.Executor: Running task 7.0 in stage 27.0 (TID 221)
15/06/03 17:37:38 INFO storage.MemoryStore: ensureFreeSpace(11300) called with curMem=3128245, maxMem=740960501
15/06/03 17:37:38 INFO storage.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 11.0 KB, free 703.6 MB)
15/06/03 17:37:38 INFO storage.BlockManagerMaster: Updated info of block broadcast_31_piece0
15/06/03 17:37:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 10 ms
15/06/03 17:37:38 INFO storage.MemoryStore: ensureFreeSpace(25992) called with curMem=3139545, maxMem=740960501
15/06/03 17:37:38 INFO storage.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 25.4 KB, free 703.6 MB)
15/06/03 17:37:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4=\xD3\xE7,\xE4I_'\x00\x84
15/06/03 17:37:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
15/06/03 17:37:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x1A\x15\x07,\xE4(\x7F\xE1\x00\x85
15/06/03 17:37:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE46\xDE!\x00\x83,\xE4=\xD3\xE7
15/06/03 17:37:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4I_'\x00\x84,\xE4W\xCF\x00\x00\x84
15/06/03 17:37:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4(\x7F\xE1\x00\x85,\xE46\xDE!\x00\x83
15/06/03 17:37:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,\xE4\x0B\xAA\xC1\x00\x84
15/06/03 17:37:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4W\xCF\x00\x00\x84,\xE4f9\xC6
15/06/03 17:37:38 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x0B\xAA\xC1\x00\x84,\xE4\x1A\x15\x07
15/06/03 17:37:38 INFO storage.MemoryStore: ensureFreeSpace(24569) called with curMem=3165537, maxMem=740960501
15/06/03 17:37:38 INFO storage.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 24.0 KB, free 703.6 MB)
15/06/03 17:37:38 INFO storage.BlockManagerMaster: Updated info of block broadcast_30_piece0
15/06/03 17:37:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 6 ms
15/06/03 17:37:38 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=3190106, maxMem=740960501
15/06/03 17:37:38 INFO storage.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 380.7 KB, free 703.2 MB)
15/06/03 17:37:38 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x26c61431 connecting to ZooKeeper ensemble=127.0.0.1:2181
15/06/03 17:37:38 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x26c61431, quorum=127.0.0.1:2181, baseZNode=/hbase
15/06/03 17:37:38 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 17:37:38 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 17:37:38 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb001f, negotiated timeout = 60000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined cc0da1066d3a8aaeca2c1d28ae422956; next sequenceid=1697
15/06/03 17:37:38 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:44 INFO regionserver.HStore: Closed V
15/06/03 17:37:44 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:37:44 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:37:44 INFO executor.Executor: Finished task 4.0 in stage 27.0 (TID 218). 2162 bytes result sent to driver
15/06/03 17:37:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 222
15/06/03 17:37:44 INFO executor.Executor: Running task 8.0 in stage 27.0 (TID 222)
15/06/03 17:37:44 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4f9\xC6,\xE4t\xA2B\x00\x82
15/06/03 17:37:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:44 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:44 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:44 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:45 INFO storage.BlockManager: Removing broadcast 28
15/06/03 17:37:45 INFO storage.BlockManager: Removing block broadcast_28
15/06/03 17:37:45 INFO storage.MemoryStore: Block broadcast_28 of size 25216 dropped from memory (free 737405816)
15/06/03 17:37:45 INFO storage.BlockManager: Removing block broadcast_28_piece0
15/06/03 17:37:45 INFO storage.MemoryStore: Block broadcast_28_piece0 of size 10949 dropped from memory (free 737416765)
15/06/03 17:37:45 INFO storage.BlockManagerMaster: Updated info of block broadcast_28_piece0
15/06/03 17:37:45 INFO storage.BlockManager: Removing broadcast 27
15/06/03 17:37:45 INFO storage.BlockManager: Removing block broadcast_27
15/06/03 17:37:45 INFO storage.MemoryStore: Block broadcast_27 of size 389795 dropped from memory (free 737806560)
15/06/03 17:37:45 INFO storage.BlockManager: Removing block broadcast_27_piece0
15/06/03 17:37:45 INFO storage.MemoryStore: Block broadcast_27_piece0 of size 24546 dropped from memory (free 737831106)
15/06/03 17:37:45 INFO storage.BlockManagerMaster: Updated info of block broadcast_27_piece0
15/06/03 17:37:48 INFO regionserver.HStore: Closed V
15/06/03 17:37:48 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:37:48 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:37:48 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 214). 2162 bytes result sent to driver
15/06/03 17:37:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 223
15/06/03 17:37:48 INFO executor.Executor: Running task 9.0 in stage 27.0 (TID 223)
15/06/03 17:37:48 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4t\xA2B\x00\x82,\xE4{\xA2C\x00\x83
15/06/03 17:37:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:48 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:48 INFO regionserver.HRegion: Onlined 923bbab361cd623ce638fbea6d814952; next sequenceid=1698
15/06/03 17:37:48 INFO regionserver.HStore: Closed V
15/06/03 17:37:48 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:37:48 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:37:48 INFO executor.Executor: Finished task 5.0 in stage 27.0 (TID 219). 2162 bytes result sent to driver
15/06/03 17:37:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 224
15/06/03 17:37:48 INFO executor.Executor: Running task 10.0 in stage 27.0 (TID 224)
15/06/03 17:37:48 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4{\xA2C\x00\x83,\xE4\x80\xCB\xE7\x00\x83
15/06/03 17:37:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:48 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 17:37:48 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:48 INFO regionserver.HRegion: Onlined f0f26897a5648fe2d9ae0e1138fe7f72; next sequenceid=1777
15/06/03 17:37:50 INFO regionserver.HStore: Closed V
15/06/03 17:37:50 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:37:50 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:37:50 INFO executor.Executor: Finished task 7.0 in stage 27.0 (TID 221). 2162 bytes result sent to driver
15/06/03 17:37:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 225
15/06/03 17:37:50 INFO executor.Executor: Running task 11.0 in stage 27.0 (TID 225)
15/06/03 17:37:50 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x80\xCB\xE7\x00\x83,\xE4\x8CK\xE5\x00\x86
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:37:50 INFO regionserver.HStore: Closed V
15/06/03 17:37:50 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:37:50 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:37:50 INFO executor.Executor: Finished task 3.0 in stage 27.0 (TID 217). 2162 bytes result sent to driver
15/06/03 17:37:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 226
15/06/03 17:37:50 INFO executor.Executor: Running task 12.0 in stage 27.0 (TID 226)
15/06/03 17:37:50 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x8CK\xE5\x00\x86,\xE4\x9A\xB2c\x00\x85
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:37:50 INFO regionserver.HStore: Closed V
15/06/03 17:37:50 INFO regionserver.HStore: Closed V
15/06/03 17:37:50 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:37:50 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:37:50 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:37:50 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:37:50 INFO executor.Executor: Finished task 6.0 in stage 27.0 (TID 220). 2162 bytes result sent to driver
15/06/03 17:37:50 INFO executor.Executor: Finished task 1.0 in stage 27.0 (TID 215). 2162 bytes result sent to driver
15/06/03 17:37:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 227
15/06/03 17:37:50 INFO executor.Executor: Running task 13.0 in stage 27.0 (TID 227)
15/06/03 17:37:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 228
15/06/03 17:37:50 INFO executor.Executor: Running task 14.0 in stage 27.0 (TID 228)
15/06/03 17:37:50 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xA7.\xA7\x00\x82,\xE4\xB2\xBAC\x00\x82
15/06/03 17:37:50 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\x9A\xB2c\x00\x85,\xE4\xA7.\xA7\x00\x82
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 2d3265a729c7a00e07ac954027bca4d3; next sequenceid=2727
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:50 INFO regionserver.HStore: Closed V
15/06/03 17:37:50 INFO regionserver.HRegion: Closed 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956.
15/06/03 17:37:50 WARN regionserver.HRegion: Region 1360,,1433353618381.cc0da1066d3a8aaeca2c1d28ae422956. already closed
15/06/03 17:37:50 INFO executor.Executor: Finished task 2.0 in stage 27.0 (TID 216). 2162 bytes result sent to driver
15/06/03 17:37:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 229
15/06/03 17:37:50 INFO executor.Executor: Running task 15.0 in stage 27.0 (TID 229)
15/06/03 17:37:50 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xB2\xBAC\x00\x82,\xE4\xC1(\x85\x00\x85
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:50 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:50 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:51 INFO regionserver.HStore: Closed V
15/06/03 17:37:51 INFO regionserver.HRegion: Closed 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72.
15/06/03 17:37:51 WARN regionserver.HRegion: Region 1360,\xE4{\xA2C\x00\x83,1433352934349.f0f26897a5648fe2d9ae0e1138fe7f72. already closed
15/06/03 17:37:51 INFO executor.Executor: Finished task 10.0 in stage 27.0 (TID 224). 1999 bytes result sent to driver
15/06/03 17:37:51 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 230
15/06/03 17:37:51 INFO executor.Executor: Running task 16.0 in stage 27.0 (TID 230)
15/06/03 17:37:51 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xC1(\x85\x00\x85,\xE4\xCF\x8E\xE5\x00\x83
15/06/03 17:37:51 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:51 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:51 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:51 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:53 INFO regionserver.HStore: Closed V
15/06/03 17:37:53 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:37:53 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:37:53 INFO executor.Executor: Finished task 9.0 in stage 27.0 (TID 223). 1999 bytes result sent to driver
15/06/03 17:37:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 231
15/06/03 17:37:53 INFO executor.Executor: Running task 17.0 in stage 27.0 (TID 231)
15/06/03 17:37:53 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xCF\x8E\xE5\x00\x83,\xE4\xDD\xF5\xC2\x00\x85
15/06/03 17:37:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:53 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:53 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:53 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:55 INFO regionserver.HStore: Closed V
15/06/03 17:37:55 INFO regionserver.HRegion: Closed 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952.
15/06/03 17:37:55 WARN regionserver.HRegion: Region 1360,\xE4=\xD3\xE7,1433353618381.923bbab361cd623ce638fbea6d814952. already closed
15/06/03 17:37:55 INFO executor.Executor: Finished task 8.0 in stage 27.0 (TID 222). 2162 bytes result sent to driver
15/06/03 17:37:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 232
15/06/03 17:37:55 INFO executor.Executor: Running task 18.0 in stage 27.0 (TID 232)
15/06/03 17:37:55 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xDD\xF5\xC2\x00\x85,\xE4\xECc\xA7
15/06/03 17:37:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:55 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:55 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:55 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:58 INFO regionserver.HStore: Closed V
15/06/03 17:37:58 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:37:58 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:37:58 INFO executor.Executor: Finished task 11.0 in stage 27.0 (TID 225). 2162 bytes result sent to driver
15/06/03 17:37:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 233
15/06/03 17:37:58 INFO executor.Executor: Running task 19.0 in stage 27.0 (TID 233)
15/06/03 17:37:58 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xECc\xA7,\xE4\xF7k\x86\x00\x85
15/06/03 17:37:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:58 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:58 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:58 INFO regionserver.HRegion: Onlined 4ff9a2d55537ac9f127d8c05431c5b79; next sequenceid=4349
15/06/03 17:37:59 INFO regionserver.HStore: Closed V
15/06/03 17:37:59 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:37:59 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:37:59 INFO executor.Executor: Finished task 14.0 in stage 27.0 (TID 228). 2162 bytes result sent to driver
15/06/03 17:37:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 234
15/06/03 17:37:59 INFO executor.Executor: Running task 20.0 in stage 27.0 (TID 234)
15/06/03 17:37:59 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xF7k\x86\x00\x85,\xE5\x02\xF4\x83\x00\x82
15/06/03 17:37:59 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:59 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:37:59 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:37:59 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:00 INFO regionserver.HStore: Closed V
15/06/03 17:38:00 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:38:00 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:38:00 INFO executor.Executor: Finished task 13.0 in stage 27.0 (TID 227). 2162 bytes result sent to driver
15/06/03 17:38:00 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 235
15/06/03 17:38:00 INFO executor.Executor: Running task 21.0 in stage 27.0 (TID 235)
15/06/03 17:38:00 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x02\xF4\x83\x00\x82,\xE5\x11ae
15/06/03 17:38:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:00 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:00 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:00 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:00 INFO regionserver.HStore: Closed V
15/06/03 17:38:00 INFO regionserver.HRegion: Closed 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3.
15/06/03 17:38:00 WARN regionserver.HRegion: Region 1360,\xE4\x80\xCB\xE7\x00\x83,1433353018488.2d3265a729c7a00e07ac954027bca4d3. already closed
15/06/03 17:38:01 INFO executor.Executor: Finished task 12.0 in stage 27.0 (TID 226). 2162 bytes result sent to driver
15/06/03 17:38:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 236
15/06/03 17:38:01 INFO executor.Executor: Running task 22.0 in stage 27.0 (TID 236)
15/06/03 17:38:01 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x11ae,\xE5\x1F\xCB\xA1\x00\x83
15/06/03 17:38:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:01 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:01 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:01 INFO regionserver.HStore: Closed V
15/06/03 17:38:01 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:38:01 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:38:01 INFO executor.Executor: Finished task 15.0 in stage 27.0 (TID 229). 2162 bytes result sent to driver
15/06/03 17:38:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 237
15/06/03 17:38:01 INFO executor.Executor: Running task 23.0 in stage 27.0 (TID 237)
15/06/03 17:38:01 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x1F\xCB\xA1\x00\x83,\xE5.:\x08
15/06/03 17:38:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:01 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:01 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:01 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:02 INFO regionserver.HStore: Closed V
15/06/03 17:38:02 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:38:02 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:38:02 INFO executor.Executor: Finished task 16.0 in stage 27.0 (TID 230). 2162 bytes result sent to driver
15/06/03 17:38:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 238
15/06/03 17:38:02 INFO executor.Executor: Running task 24.0 in stage 27.0 (TID 238)
15/06/03 17:38:02 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5.:\x08,\xE5<\xA2D\x00\x85
15/06/03 17:38:02 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:02 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:02 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:02 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:04 INFO regionserver.HStore: Closed V
15/06/03 17:38:04 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:38:04 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:38:04 INFO executor.Executor: Finished task 17.0 in stage 27.0 (TID 231). 2162 bytes result sent to driver
15/06/03 17:38:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 239
15/06/03 17:38:04 INFO executor.Executor: Running task 25.0 in stage 27.0 (TID 239)
15/06/03 17:38:04 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5<\xA2D\x00\x85,\xE5CNa\x00\x85
15/06/03 17:38:04 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:04 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:04 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:04 INFO regionserver.HRegion: Onlined f6b425b8ae9ca19d5b3e66c328d168d5; next sequenceid=5297
15/06/03 17:38:05 INFO regionserver.HStore: Closed V
15/06/03 17:38:05 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:38:05 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:38:05 INFO executor.Executor: Finished task 18.0 in stage 27.0 (TID 232). 2162 bytes result sent to driver
15/06/03 17:38:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 240
15/06/03 17:38:05 INFO executor.Executor: Running task 26.0 in stage 27.0 (TID 240)
15/06/03 17:38:05 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5CNa\x00\x85,\xE5N\xD3\x85\x00\x85
15/06/03 17:38:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:05 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:05 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:05 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:05 INFO regionserver.HStore: Closed V
15/06/03 17:38:05 INFO regionserver.HRegion: Closed 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79.
15/06/03 17:38:05 WARN regionserver.HRegion: Region 1360,\xE4\xA7.\xA7\x00\x82,1433353107608.4ff9a2d55537ac9f127d8c05431c5b79. already closed
15/06/03 17:38:05 INFO executor.Executor: Finished task 19.0 in stage 27.0 (TID 233). 2162 bytes result sent to driver
15/06/03 17:38:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 241
15/06/03 17:38:05 INFO executor.Executor: Running task 27.0 in stage 27.0 (TID 241)
15/06/03 17:38:06 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5N\xD3\x85\x00\x85,\xE5]:\xC7\x00\x82
15/06/03 17:38:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:06 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:06 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:06 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:07 INFO regionserver.HStore: Closed V
15/06/03 17:38:07 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:38:07 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:38:08 INFO executor.Executor: Finished task 20.0 in stage 27.0 (TID 234). 2162 bytes result sent to driver
15/06/03 17:38:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 242
15/06/03 17:38:08 INFO executor.Executor: Running task 28.0 in stage 27.0 (TID 242)
15/06/03 17:38:08 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5]:\xC7\x00\x82,\xE5k\xA2\xE1\x00\x83
15/06/03 17:38:08 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:08 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:08 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:08 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:09 INFO regionserver.HStore: Closed V
15/06/03 17:38:09 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:38:09 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:38:10 INFO executor.Executor: Finished task 25.0 in stage 27.0 (TID 239). 1999 bytes result sent to driver
15/06/03 17:38:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 243
15/06/03 17:38:10 INFO executor.Executor: Running task 29.0 in stage 27.0 (TID 243)
15/06/03 17:38:10 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5k\xA2\xE1\x00\x83,\xE5z\x00G\x00\x81
15/06/03 17:38:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:10 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:10 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:10 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:11 INFO regionserver.HStore: Closed V
15/06/03 17:38:11 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:38:11 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:38:11 INFO executor.Executor: Finished task 21.0 in stage 27.0 (TID 235). 2162 bytes result sent to driver
15/06/03 17:38:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 244
15/06/03 17:38:11 INFO executor.Executor: Running task 30.0 in stage 27.0 (TID 244)
15/06/03 17:38:11 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5z\x00G\x00\x81,\xE5\x86\x8B\x06
15/06/03 17:38:11 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:11 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:11 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:11 INFO regionserver.HRegion: Onlined af91a06d02c07e393d45275a8cee2ed8; next sequenceid=6081
15/06/03 17:38:11 INFO regionserver.HStore: Closed V
15/06/03 17:38:11 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:38:11 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:38:11 INFO executor.Executor: Finished task 22.0 in stage 27.0 (TID 236). 2162 bytes result sent to driver
15/06/03 17:38:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 245
15/06/03 17:38:11 INFO executor.Executor: Running task 31.0 in stage 27.0 (TID 245)
15/06/03 17:38:11 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x86\x8B\x06,\xE5\x92\x0Cc\x00\x82
15/06/03 17:38:11 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:11 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:11 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:11 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:12 INFO regionserver.HStore: Closed V
15/06/03 17:38:12 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:38:12 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:38:12 INFO executor.Executor: Finished task 23.0 in stage 27.0 (TID 237). 2162 bytes result sent to driver
15/06/03 17:38:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 246
15/06/03 17:38:12 INFO executor.Executor: Running task 32.0 in stage 27.0 (TID 246)
15/06/03 17:38:12 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x92\x0Cc\x00\x82,\xE5\xA0q\x06\x00\x86
15/06/03 17:38:12 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:12 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:12 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:12 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:13 INFO regionserver.HStore: Closed V
15/06/03 17:38:13 INFO regionserver.HRegion: Closed 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5.
15/06/03 17:38:13 WARN regionserver.HRegion: Region 1360,\xE4\xF7k\x86\x00\x85,1433353203344.f6b425b8ae9ca19d5b3e66c328d168d5. already closed
15/06/03 17:38:13 INFO executor.Executor: Finished task 24.0 in stage 27.0 (TID 238). 2162 bytes result sent to driver
15/06/03 17:38:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 247
15/06/03 17:38:13 INFO executor.Executor: Running task 33.0 in stage 27.0 (TID 247)
15/06/03 17:38:13 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xA0q\x06\x00\x86,\xE5\xAE\xD6B\x00\x82
15/06/03 17:38:13 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:13 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:13 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:13 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:14 INFO regionserver.HStore: Closed V
15/06/03 17:38:14 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:38:14 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:38:15 INFO executor.Executor: Finished task 26.0 in stage 27.0 (TID 240). 2162 bytes result sent to driver
15/06/03 17:38:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 248
15/06/03 17:38:15 INFO executor.Executor: Running task 34.0 in stage 27.0 (TID 248)
15/06/03 17:38:15 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xAE\xD6B\x00\x82,\xE5\xBDA`\x00\x85
15/06/03 17:38:15 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:15 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:15 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:15 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:16 INFO regionserver.HStore: Closed V
15/06/03 17:38:16 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:38:16 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:38:17 INFO executor.Executor: Finished task 27.0 in stage 27.0 (TID 241). 2162 bytes result sent to driver
15/06/03 17:38:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 249
15/06/03 17:38:17 INFO executor.Executor: Running task 35.0 in stage 27.0 (TID 249)
15/06/03 17:38:17 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xBDA`\x00\x85,
15/06/03 17:38:17 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:17 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:17 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:17 INFO regionserver.HRegion: Onlined e997096226a6761f8a165c700f026167; next sequenceid=6082
15/06/03 17:38:19 INFO regionserver.HStore: Closed V
15/06/03 17:38:19 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:38:19 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:38:19 INFO executor.Executor: Finished task 28.0 in stage 27.0 (TID 242). 2162 bytes result sent to driver
15/06/03 17:38:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 250
15/06/03 17:38:19 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 250)
15/06/03 17:38:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32
15/06/03 17:38:19 INFO storage.MemoryStore: ensureFreeSpace(13285) called with curMem=3129395, maxMem=740960501
15/06/03 17:38:19 INFO storage.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 13.0 KB, free 703.6 MB)
15/06/03 17:38:19 INFO storage.BlockManagerMaster: Updated info of block broadcast_32_piece0
15/06/03 17:38:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 17 ms
15/06/03 17:38:19 INFO storage.MemoryStore: ensureFreeSpace(35616) called with curMem=3142680, maxMem=740960501
15/06/03 17:38:19 INFO storage.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 34.8 KB, free 703.6 MB)
15/06/03 17:38:19 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:,\xE42Y\x08
15/06/03 17:38:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
15/06/03 17:38:19 INFO storage.MemoryStore: ensureFreeSpace(24570) called with curMem=3178296, maxMem=740960501
15/06/03 17:38:19 INFO storage.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 24.0 KB, free 703.6 MB)
15/06/03 17:38:19 INFO storage.BlockManagerMaster: Updated info of block broadcast_17_piece0
15/06/03 17:38:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 10 ms
15/06/03 17:38:19 INFO storage.MemoryStore: ensureFreeSpace(389795) called with curMem=3202866, maxMem=740960501
15/06/03 17:38:19 INFO storage.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 380.7 KB, free 703.2 MB)
15/06/03 17:38:19 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:19 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:38:19 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:19 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:38:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
15/06/03 17:38:19 INFO storage.MemoryStore: ensureFreeSpace(99) called with curMem=3592661, maxMem=740960501
15/06/03 17:38:19 INFO storage.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 99.0 B, free 703.2 MB)
15/06/03 17:38:19 INFO storage.BlockManagerMaster: Updated info of block broadcast_20_piece0
15/06/03 17:38:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 10 ms
15/06/03 17:38:19 INFO storage.MemoryStore: ensureFreeSpace(144) called with curMem=3592760, maxMem=740960501
15/06/03 17:38:19 INFO storage.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 144.0 B, free 703.2 MB)
15/06/03 17:38:21 INFO regionserver.HStore: Closed V
15/06/03 17:38:21 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:38:21 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:38:21 INFO executor.Executor: Finished task 31.0 in stage 27.0 (TID 245). 2162 bytes result sent to driver
15/06/03 17:38:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 251
15/06/03 17:38:21 INFO executor.Executor: Running task 1.0 in stage 28.0 (TID 251)
15/06/03 17:38:21 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE42Y\x08,\xE4q \x05
15/06/03 17:38:21 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:21 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:38:21 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:21 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:38:21 INFO regionserver.HStore: Closed V
15/06/03 17:38:21 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:38:21 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:38:21 INFO executor.Executor: Finished task 30.0 in stage 27.0 (TID 244). 2162 bytes result sent to driver
15/06/03 17:38:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 252
15/06/03 17:38:21 INFO executor.Executor: Running task 2.0 in stage 28.0 (TID 252)
15/06/03 17:38:21 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4q \x05,\xE4\xAF\xEED
15/06/03 17:38:21 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:21 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:38:21 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:21 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:38:22 INFO regionserver.HStore: Closed V
15/06/03 17:38:22 INFO regionserver.HRegion: Closed 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8.
15/06/03 17:38:22 WARN regionserver.HRegion: Region 1360,\xE5CNa\x00\x85,1433353279839.af91a06d02c07e393d45275a8cee2ed8. already closed
15/06/03 17:38:22 INFO executor.Executor: Finished task 29.0 in stage 27.0 (TID 243). 2162 bytes result sent to driver
15/06/03 17:38:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 253
15/06/03 17:38:22 INFO executor.Executor: Running task 3.0 in stage 28.0 (TID 253)
15/06/03 17:38:22 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xAF\xEED,\xE4\xCA[\x84
15/06/03 17:38:22 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:22 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:38:22 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:22 INFO regionserver.HRegion: Onlined 714d358f94bd001789376ca6aa4ec100; next sequenceid=1522
15/06/03 17:38:24 INFO regionserver.HStore: Closed V
15/06/03 17:38:24 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 17:38:24 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 17:38:24 INFO regionserver.HStore: Closed V
15/06/03 17:38:24 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 17:38:24 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 17:38:24 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 250). 2342 bytes result sent to driver
15/06/03 17:38:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 254
15/06/03 17:38:24 INFO executor.Executor: Running task 4.0 in stage 28.0 (TID 254)
15/06/03 17:38:24 INFO executor.Executor: Finished task 3.0 in stage 28.0 (TID 253). 2342 bytes result sent to driver
15/06/03 17:38:24 INFO regionserver.HStore: Closed V
15/06/03 17:38:24 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:38:24 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:38:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 255
15/06/03 17:38:24 INFO executor.Executor: Running task 5.0 in stage 28.0 (TID 255)
15/06/03 17:38:24 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xCA[\x84,\xE4\xFC\x90'
15/06/03 17:38:24 INFO executor.Executor: Finished task 32.0 in stage 27.0 (TID 246). 2162 bytes result sent to driver
15/06/03 17:38:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:24 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:38:24 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE4\xFC\x90',\xE5;h!
15/06/03 17:38:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 256
15/06/03 17:38:24 INFO executor.Executor: Running task 6.0 in stage 28.0 (TID 256)
15/06/03 17:38:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:24 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:38:24 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:38:24 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5;h!,\xE5z)h
15/06/03 17:38:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:24 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:38:24 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:38:24 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:24 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:38:25 INFO regionserver.HStore: Closed V
15/06/03 17:38:25 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:38:25 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:38:25 INFO executor.Executor: Finished task 33.0 in stage 27.0 (TID 247). 2162 bytes result sent to driver
15/06/03 17:38:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 257
15/06/03 17:38:25 INFO executor.Executor: Running task 7.0 in stage 28.0 (TID 257)
15/06/03 17:38:25 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5z)h,\xE5\x94\xAE\x01
15/06/03 17:38:25 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:25 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:38:25 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:25 INFO regionserver.HRegion: Onlined 0381536cccaa8b72eab3fe116f3667cf; next sequenceid=1523
15/06/03 17:38:26 INFO regionserver.HStore: Closed V
15/06/03 17:38:26 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:38:26 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:38:26 INFO executor.Executor: Finished task 35.0 in stage 27.0 (TID 249). 2162 bytes result sent to driver
15/06/03 17:38:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 258
15/06/03 17:38:26 INFO executor.Executor: Running task 8.0 in stage 28.0 (TID 258)
15/06/03 17:38:26 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\x94\xAE\x01,\xE5\xC6\xE6\x87
15/06/03 17:38:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:26 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 17:38:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:26 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 17:38:26 INFO regionserver.HStore: Closed V
15/06/03 17:38:26 INFO regionserver.HRegion: Closed 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167.
15/06/03 17:38:26 WARN regionserver.HRegion: Region 1360,\xE5\x86\x8B\x06,1433353279839.e997096226a6761f8a165c700f026167. already closed
15/06/03 17:38:26 INFO executor.Executor: Finished task 34.0 in stage 27.0 (TID 248). 2162 bytes result sent to driver
15/06/03 17:38:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 259
15/06/03 17:38:26 INFO executor.Executor: Running task 9.0 in stage 28.0 (TID 259)
15/06/03 17:38:26 INFO regionserver.HStore: Closed V
15/06/03 17:38:26 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 17:38:26 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 17:38:26 INFO rdd.NewHadoopRDD: Input split: 192.168.0.21:\xE5\xC6\xE6\x87,
15/06/03 17:38:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:26 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 17:38:26 INFO compactions.CompactionConfiguration: size [16777216, 260046848); files [5, 10); ratio 1.250000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
15/06/03 17:38:26 INFO regionserver.HRegion: Onlined 402f675264cb4376ccad012131696893; next sequenceid=1520
15/06/03 17:38:26 INFO executor.Executor: Finished task 2.0 in stage 28.0 (TID 252). 2342 bytes result sent to driver
15/06/03 17:38:26 INFO regionserver.HStore: Closed V
15/06/03 17:38:26 INFO regionserver.HRegion: Closed 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100.
15/06/03 17:38:26 WARN regionserver.HRegion: Region 1376,,1433353730537.714d358f94bd001789376ca6aa4ec100. already closed
15/06/03 17:38:26 INFO executor.Executor: Finished task 1.0 in stage 28.0 (TID 251). 2342 bytes result sent to driver
15/06/03 17:38:27 INFO regionserver.HStore: Closed V
15/06/03 17:38:27 INFO regionserver.HRegion: Closed 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893.
15/06/03 17:38:27 WARN regionserver.HRegion: Region 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893. already closed
15/06/03 17:38:27 INFO executor.Executor: Finished task 9.0 in stage 28.0 (TID 259). 2179 bytes result sent to driver
15/06/03 17:38:27 INFO regionserver.HStore: Closed V
15/06/03 17:38:27 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 17:38:27 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 17:38:27 INFO executor.Executor: Finished task 7.0 in stage 28.0 (TID 257). 2179 bytes result sent to driver
15/06/03 17:38:28 INFO regionserver.HStore: Closed V
15/06/03 17:38:28 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 17:38:28 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 17:38:28 INFO executor.Executor: Finished task 4.0 in stage 28.0 (TID 254). 2179 bytes result sent to driver
15/06/03 17:38:28 INFO regionserver.HStore: Closed V
15/06/03 17:38:28 INFO regionserver.HRegion: Closed 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893.
15/06/03 17:38:28 WARN regionserver.HRegion: Region 1376,\xE5\x94\xAE\x01,1433353508246.402f675264cb4376ccad012131696893. already closed
15/06/03 17:38:28 INFO executor.Executor: Finished task 8.0 in stage 28.0 (TID 258). 2179 bytes result sent to driver
15/06/03 17:38:28 INFO regionserver.HStore: Closed V
15/06/03 17:38:28 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 17:38:28 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 17:38:29 INFO executor.Executor: Finished task 5.0 in stage 28.0 (TID 255). 2179 bytes result sent to driver
15/06/03 17:38:29 INFO client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
15/06/03 17:38:29 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb001f
15/06/03 17:38:29 INFO zookeeper.ZooKeeper: Session: 0x14dbb7837eb001f closed
15/06/03 17:38:29 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 17:38:29 INFO regionserver.HStore: Closed V
15/06/03 17:38:29 INFO regionserver.HRegion: Closed 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf.
15/06/03 17:38:29 WARN regionserver.HRegion: Region 1376,\xE4\xCA[\x84,1433353730537.0381536cccaa8b72eab3fe116f3667cf. already closed
15/06/03 17:38:29 INFO executor.Executor: Finished task 6.0 in stage 28.0 (TID 256). 2179 bytes result sent to driver
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 260
15/06/03 17:38:29 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 260)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 261
15/06/03 17:38:29 INFO executor.Executor: Running task 1.0 in stage 29.0 (TID 261)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 262
15/06/03 17:38:29 INFO executor.Executor: Running task 2.0 in stage 29.0 (TID 262)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 263
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache
15/06/03 17:38:29 INFO executor.Executor: Running task 3.0 in stage 29.0 (TID 263)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 264
15/06/03 17:38:29 INFO executor.Executor: Running task 4.0 in stage 29.0 (TID 264)
15/06/03 17:38:29 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 33
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 265
15/06/03 17:38:29 INFO executor.Executor: Running task 5.0 in stage 29.0 (TID 265)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 266
15/06/03 17:38:29 INFO executor.Executor: Running task 6.0 in stage 29.0 (TID 266)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 267
15/06/03 17:38:29 INFO executor.Executor: Running task 7.0 in stage 29.0 (TID 267)
15/06/03 17:38:29 INFO storage.MemoryStore: ensureFreeSpace(12005) called with curMem=3592904, maxMem=740960501
15/06/03 17:38:29 INFO storage.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 11.7 KB, free 703.2 MB)
15/06/03 17:38:29 INFO storage.BlockManagerMaster: Updated info of block broadcast_33_piece0
15/06/03 17:38:29 INFO broadcast.TorrentBroadcast: Reading broadcast variable 33 took 11 ms
15/06/03 17:38:29 INFO storage.MemoryStore: ensureFreeSpace(29912) called with curMem=3604909, maxMem=740960501
15/06/03 17:38:29 INFO storage.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 29.2 KB, free 703.2 MB)
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@192.168.0.21:64531/user/MapOutputTracker#700156622]
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@192.168.0.21:64531/user/MapOutputTracker#700156622]
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them
15/06/03 17:38:29 INFO spark.MapOutputTrackerWorker: Got the output locations
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 0.0 in stage 29.0 (TID 260)
java.lang.NumberFormatException: For input string: "7>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 268
15/06/03 17:38:29 INFO executor.Executor: Running task 8.0 in stage 29.0 (TID 268)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 7.0 in stage 29.0 (TID 267)
java.lang.NumberFormatException: Zero length BigInteger
	at java.math.BigInteger.<init>(BigInteger.java:296)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 269
15/06/03 17:38:29 INFO executor.Executor: Running task 0.1 in stage 29.0 (TID 269)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 2.0 in stage 29.0 (TID 262)
java.lang.NumberFormatException: For input string: "71;7 "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 270
15/06/03 17:38:29 INFO executor.Executor: Running task 7.1 in stage 29.0 (TID 270)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 4.0 in stage 29.0 (TID 264)
java.lang.NumberFormatException: For input string: ";"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 271
15/06/03 17:38:29 INFO executor.Executor: Running task 2.1 in stage 29.0 (TID 271)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 3.0 in stage 29.0 (TID 263)
java.lang.NumberFormatException: For input string: "9;"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 272
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO executor.Executor: Running task 4.1 in stage 29.0 (TID 272)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 1.0 in stage 29.0 (TID 261)
java.lang.ArrayIndexOutOfBoundsException: 1
	at com.splicemachine.encoding.ScalarEncoding.toLongWithOffset(ScalarEncoding.java:230)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:202)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 273
15/06/03 17:38:29 INFO executor.Executor: Running task 3.1 in stage 29.0 (TID 273)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 5.0 in stage 29.0 (TID 265)
java.lang.NumberFormatException: For input string: "6        "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:347)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 274
15/06/03 17:38:29 INFO executor.Executor: Running task 1.1 in stage 29.0 (TID 274)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 0.1 in stage 29.0 (TID 269)
java.lang.NumberFormatException: For input string: "7>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 275
15/06/03 17:38:29 INFO executor.Executor: Running task 5.1 in stage 29.0 (TID 275)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 7.1 in stage 29.0 (TID 270)
java.lang.NumberFormatException: Zero length BigInteger
	at java.math.BigInteger.<init>(BigInteger.java:296)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 276
15/06/03 17:38:29 INFO executor.Executor: Running task 0.2 in stage 29.0 (TID 276)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 6.0 in stage 29.0 (TID 266)
java.lang.NumberFormatException: For input string: "8=29:"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 277
15/06/03 17:38:29 INFO executor.Executor: Running task 7.2 in stage 29.0 (TID 277)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 8.0 in stage 29.0 (TID 268)
java.lang.NegativeArraySizeException
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:216)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 278
15/06/03 17:38:29 INFO executor.Executor: Running task 6.1 in stage 29.0 (TID 278)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 2.1 in stage 29.0 (TID 271)
java.lang.NumberFormatException: For input string: "71;7 "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 279
15/06/03 17:38:29 INFO executor.Executor: Running task 8.1 in stage 29.0 (TID 279)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 4.1 in stage 29.0 (TID 272)
java.lang.NumberFormatException: For input string: ";"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 280
15/06/03 17:38:29 INFO executor.Executor: Running task 2.2 in stage 29.0 (TID 280)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 3.1 in stage 29.0 (TID 273)
java.lang.NumberFormatException: For input string: "9;"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 281
15/06/03 17:38:29 INFO executor.Executor: Running task 4.2 in stage 29.0 (TID 281)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 0.2 in stage 29.0 (TID 276)
java.lang.NumberFormatException: For input string: "7>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 282
15/06/03 17:38:29 INFO executor.Executor: Running task 3.2 in stage 29.0 (TID 282)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 1.1 in stage 29.0 (TID 274)
java.lang.ArrayIndexOutOfBoundsException
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 7.2 in stage 29.0 (TID 277)
java.lang.NumberFormatException: Zero length BigInteger
	at java.math.BigInteger.<init>(BigInteger.java:296)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 283
15/06/03 17:38:29 INFO executor.Executor: Running task 0.3 in stage 29.0 (TID 283)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 284
15/06/03 17:38:29 INFO executor.Executor: Running task 1.2 in stage 29.0 (TID 284)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 8.1 in stage 29.0 (TID 279)
java.lang.NegativeArraySizeException
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:216)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 285
15/06/03 17:38:29 INFO executor.Executor: Running task 7.3 in stage 29.0 (TID 285)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 5.1 in stage 29.0 (TID 275)
java.lang.NumberFormatException: For input string: "6        "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:347)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 0.3 in stage 29.0 (TID 283)
java.lang.NumberFormatException: For input string: "7>"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 286
15/06/03 17:38:29 INFO executor.Executor: Running task 8.2 in stage 29.0 (TID 286)
15/06/03 17:38:29 ERROR executor.Executor: Exception in task 2.2 in stage 29.0 (TID 280)
java.lang.NumberFormatException: For input string: "71;7 "
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:492)
	at java.math.BigInteger.<init>(BigInteger.java:338)
	at java.math.BigInteger.<init>(BigInteger.java:476)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:238)
	at com.splicemachine.encoding.BigDecimalEncoding.toBigDecimal(BigDecimalEncoding.java:180)
	at com.splicemachine.encoding.Encoding.decodeBigDecimal(Encoding.java:585)
	at com.splicemachine.encoding.MultiFieldDecoder.decodeNextBigDecimal(MultiFieldDecoder.java:155)
	at com.splicemachine.derby.utils.marshall.dvd.DecimalDescriptorSerializer.decode(DecimalDescriptorSerializer.java:47)
	at com.splicemachine.derby.utils.marshall.dvd.LazyDescriptorSerializer.decode(LazyDescriptorSerializer.java:71)
	at com.splicemachine.derby.utils.marshall.dvd.NullDescriptorSerializer.decode(NullDescriptorSerializer.java:116)
	at com.splicemachine.derby.utils.marshall.BareKeyHash.unpack(BareKeyHash.java:99)
	at com.splicemachine.derby.utils.marshall.BareKeyHash$Decoder.decode(BareKeyHash.java:134)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:118)
	at com.splicemachine.derby.impl.spark.kryo.SparkValueRowSerializer.read(SparkValueRowSerializer.java:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.splicemachine.utils.kryo.KryoObjectInput.readObject(KryoObjectInput.java:23)
	at com.splicemachine.derby.impl.sql.execute.operations.LocatedRow.readExternal(LocatedRow.java:102)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:32)
	at com.splicemachine.utils.kryo.ExternalizableSerializer.read(ExternalizableSerializer.java:15)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:142)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 17:38:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 287
15/06/03 17:38:29 INFO executor.Executor: Running task 5.2 in stage 29.0 (TID 287)
15/06/03 17:38:29 INFO executor.Executor: Executor is trying to kill task 5.2 in stage 29.0 (TID 287)
15/06/03 17:38:29 INFO executor.Executor: Executor is trying to kill task 1.2 in stage 29.0 (TID 284)
15/06/03 17:38:29 INFO executor.Executor: Executor is trying to kill task 4.2 in stage 29.0 (TID 281)
15/06/03 17:38:29 INFO executor.Executor: Executor is trying to kill task 7.3 in stage 29.0 (TID 285)
15/06/03 17:38:29 INFO executor.Executor: Executor is trying to kill task 8.2 in stage 29.0 (TID 286)
15/06/03 17:38:29 INFO executor.Executor: Executor is trying to kill task 6.1 in stage 29.0 (TID 278)
15/06/03 17:38:29 INFO executor.Executor: Executor killed task 7.3 in stage 29.0 (TID 285)
15/06/03 17:38:29 INFO executor.Executor: Executor is trying to kill task 3.2 in stage 29.0 (TID 282)
15/06/03 17:38:29 INFO executor.Executor: Executor killed task 3.2 in stage 29.0 (TID 282)
15/06/03 17:38:29 INFO executor.Executor: Executor killed task 1.2 in stage 29.0 (TID 284)
15/06/03 17:38:29 INFO executor.Executor: Executor killed task 6.1 in stage 29.0 (TID 278)
15/06/03 17:38:29 INFO executor.Executor: Executor killed task 4.2 in stage 29.0 (TID 281)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO executor.Executor: Executor killed task 8.2 in stage 29.0 (TID 286)
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 10 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Getting 36 non-empty blocks out of 36 blocks
15/06/03 17:38:29 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/06/03 17:38:29 INFO executor.Executor: Executor killed task 5.2 in stage 29.0 (TID 287)
15/06/03 17:39:21 INFO storage.BlockManager: Removing broadcast 32
15/06/03 17:39:21 INFO storage.BlockManager: Removing block broadcast_32
15/06/03 17:39:21 INFO storage.MemoryStore: Block broadcast_32 of size 35616 dropped from memory (free 737361296)
15/06/03 17:39:21 INFO storage.BlockManager: Removing block broadcast_32_piece0
15/06/03 17:39:21 INFO storage.MemoryStore: Block broadcast_32_piece0 of size 13285 dropped from memory (free 737374581)
15/06/03 17:39:21 INFO storage.BlockManagerMaster: Updated info of block broadcast_32_piece0
15/06/03 17:39:21 INFO storage.BlockManager: Removing broadcast 29
15/06/03 17:39:21 INFO storage.BlockManager: Removing broadcast 30
15/06/03 17:39:21 INFO storage.BlockManager: Removing block broadcast_30_piece0
15/06/03 17:39:21 INFO storage.MemoryStore: Block broadcast_30_piece0 of size 24569 dropped from memory (free 737399150)
15/06/03 17:39:21 INFO storage.BlockManagerMaster: Updated info of block broadcast_30_piece0
15/06/03 17:39:21 INFO storage.BlockManager: Removing block broadcast_30
15/06/03 17:39:21 INFO storage.MemoryStore: Block broadcast_30 of size 389795 dropped from memory (free 737788945)
15/06/03 17:39:21 INFO storage.BlockManager: Removing broadcast 33
15/06/03 17:39:21 INFO storage.BlockManager: Removing block broadcast_33_piece0
15/06/03 17:39:21 INFO storage.MemoryStore: Block broadcast_33_piece0 of size 12005 dropped from memory (free 737800950)
15/06/03 17:39:21 INFO storage.BlockManagerMaster: Updated info of block broadcast_33_piece0
15/06/03 17:39:21 INFO storage.BlockManager: Removing block broadcast_33
15/06/03 17:39:21 INFO storage.MemoryStore: Block broadcast_33 of size 29912 dropped from memory (free 737830862)
15/06/03 17:39:21 INFO storage.BlockManager: Removing broadcast 31
15/06/03 17:39:21 INFO storage.BlockManager: Removing block broadcast_31
15/06/03 17:39:21 INFO storage.MemoryStore: Block broadcast_31 of size 25992 dropped from memory (free 737856854)
15/06/03 17:39:21 INFO storage.BlockManager: Removing block broadcast_31_piece0
15/06/03 17:39:21 INFO storage.MemoryStore: Block broadcast_31_piece0 of size 11300 dropped from memory (free 737868154)
15/06/03 17:39:21 INFO storage.BlockManagerMaster: Updated info of block broadcast_31_piece0
15/06/03 17:39:21 INFO storage.BlockManager: Removing broadcast 26
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dbb7837eb0016, likely server has closed socket, closing socket connection and attempting reconnect
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dbb7837eb0014, likely server has closed socket, closing socket connection and attempting reconnect
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dbb7837eb0015, likely server has closed socket, closing socket connection and attempting reconnect
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dbb7837eb0011, likely server has closed socket, closing socket connection and attempting reconnect
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dbb7837eb0012, likely server has closed socket, closing socket connection and attempting reconnect
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dbb7837eb0013, likely server has closed socket, closing socket connection and attempting reconnect
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dbb7837eb000f, likely server has closed socket, closing socket connection and attempting reconnect
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x14dbb7837eb0010, likely server has closed socket, closing socket connection and attempting reconnect
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:30 WARN zookeeper.ClientCnxn: Session 0x14dbb7837eb0014 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:30 WARN zookeeper.ClientCnxn: Session 0x14dbb7837eb0015 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:30 WARN zookeeper.ClientCnxn: Session 0x14dbb7837eb0013 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:14:30 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:30 WARN zookeeper.ClientCnxn: Session 0x14dbb7837eb000f for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:14:31 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:31 WARN zookeeper.ClientCnxn: Session 0x14dbb7837eb0012 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:14:31 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:31 WARN zookeeper.ClientCnxn: Session 0x14dbb7837eb0010 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:14:31 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:31 WARN zookeeper.ClientCnxn: Session 0x14dbb7837eb0016 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:14:31 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:31 WARN zookeeper.ClientCnxn: Session 0x14dbb7837eb0011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14dbb7837eb0013 has expired, closing socket connection
15/06/03 18:14:32 WARN client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
15/06/03 18:14:32 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb0013
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14dbb7837eb0014 has expired, closing socket connection
15/06/03 18:14:32 INFO utils.SpliceZooKeeperManager: Lost connection with ZooKeeper, attempting reconnect
15/06/03 18:14:32 INFO utils.SpliceZooKeeperManager: Successfully reconnected to ZooKeeper
15/06/03 18:14:32 ERROR utils.SpliceZooKeeperManager: spliceconnection-0x14dbb7837eb0014, quorum=localhost:2181, baseZNode=/hbase spliceconnection-0x14dbb7837eb0014 received expired from ZooKeeper, aborting
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14dbb7837eb000f has expired, closing socket connection
15/06/03 18:14:32 WARN client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
15/06/03 18:14:32 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb000f
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14dbb7837eb0011 has expired, closing socket connection
15/06/03 18:14:32 WARN client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
15/06/03 18:14:32 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb0011
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14dbb7837eb0012 has expired, closing socket connection
15/06/03 18:14:32 WARN client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
15/06/03 18:14:32 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb0012
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14dbb7837eb0015 has expired, closing socket connection
15/06/03 18:14:32 WARN client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
15/06/03 18:14:32 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb0015
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14dbb7837eb0010 has expired, closing socket connection
15/06/03 18:14:32 WARN client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
15/06/03 18:14:32 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb0010
15/06/03 18:14:32 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 18:14:33 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:14:33 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:14:33 INFO zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14dbb7837eb0016 has expired, closing socket connection
15/06/03 18:14:33 WARN client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
15/06/03 18:14:33 INFO client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14dbb7837eb0016
15/06/03 18:14:33 INFO zookeeper.ClientCnxn: EventThread shut down
15/06/03 18:44:41 WARN util.AkkaUtils: Error sending message in 1 attempts
akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://sparkDriver@192.168.0.21:64531/user/HeartbeatReceiver#1276231993]] after [30000 ms]
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:333)
	at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)
	at java.lang.Thread.run(Thread.java:745)
15/06/03 18:45:11 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x103fecbf connecting to ZooKeeper ensemble=localhost:2181
15/06/03 18:45:11 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x103fecbf, quorum=localhost:2181, baseZNode=/hbase
15/06/03 18:45:11 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:45:11 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
15/06/03 18:45:11 WARN zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/meta-region-server
15/06/03 18:45:11 INFO util.RetryCounter: Sleeping 1000ms before retry #0...
15/06/03 18:45:11 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/06/03 18:45:11 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/06/03 18:45:11 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14dbb7837eb0025, negotiated timeout = 60000
15/06/03 18:45:14 WARN util.AkkaUtils: Error sending message in 2 attempts
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:187)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:398)
15/06/03 18:45:22 ERROR executor.CoarseGrainedExecutorBackend: Driver Disassociated [akka.tcp://sparkExecutor@192.168.0.21:64537] -> [akka.tcp://sparkDriver@192.168.0.21:64531] disassociated! Shutting down.
15/06/03 18:45:22 WARN remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkDriver@192.168.0.21:64531] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
