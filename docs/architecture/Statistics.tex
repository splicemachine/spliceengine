Understanding the role of statistics in a relational database is typically tightly connected to undertstanding the role of the query optimizer. In most databases\footnote{See Appendix-\ref{sec:OtherDBs}}, Statistics information is a tool which is used only during the query planning and optimization stage, and is not used at any other stage of the execution process. This view that statistics is only helpful to the query optimizer has significant consequences on how the statistical systems in those databases have been implemented. There are variations, but the central theme of statistics collections in database products to this point is to acquire a small but statistically significant sample of data, and computing a simple set of statistics from this sample. And, because sampling eliminates all boundaries between data partitions (either within a node or between multiple nodes), this approach has the circular effect of forcing database systems to view data only in a global manner\footnote{Or, as in the case with Oracle, to draw a strict distinction between global and local statistics}.

We propose an adjustment to this paradigm. Clearly, the query optimizer will need to maintain a global understanding of data, in order to choose the correct plans. However, in addition to that, SpliceMachine will maintain statistics within boundaries (in particular, within the boundaries of regions). This will allow SpliceMachine to make optimization choices which are not available to the query planner, and therefore to improve throughput and reduce latency by well chosen execution strategies. Further, this approach will lead to a better use of resources: we can use statistical information to ensure that SpliceMachine runs more stably and more predictably even under the stress of many different queries.

This document is broken down into five main sections. In the \hyperref[sec:Overview]{\textbf{Overview}}, we will provide a high-level summary of the structure of the statistics module, and how it will be used in SpliceMachine. Then, in the \hyperref[sec:Statistics]{\textbf{Statistics}} module, we will describe the statistics that are to be collected, and how they will be used. The \hyperref[sec:Collection]{\textbf{Collection}} and \hyperref[sec:Storage]{\textbf{Storage}} sections will describe how statistics information will be collected, stored, and accessed in an efficient and low-impact manner. Finally, in \hyperref[sec:OtherDBs]{\textbf{Appendix \ref{sec:OtherDBs}}} we provide context about statistics collection in other databases, and in \hyperref[sec:Algorithms]{\textbf{Appendix \ref{sec:Algorithms}}} we give more details about the selected statistics collection algorithms, and a brief construction of how they work. These appendices provide context as to why certain decisions were made, and how they affect the overall design of SpliceMachine's statistics engine, but do not directly impact the structure of the engine.

\section{Overview}
\label{sec:Overview}
There are three main components of the statistics strategy in SpliceMachine:

\begin{description}
\item[Collection] approaches what statistics are collected and how the collection will occur.
\item[Storage] resolves issues around storing statistical data so that it may be effectively accessed.
\item[Access] resolves issues around accessing statistical data quickly, both for human administrators and internal systems
\end{description}

\subsection{Collection}
Statistics will be collected through the use of a periodically executed maintenance tasks, which can be triggered on a single region in one of two modes. Manual mode (which will be implemented first), will collect statistics when the administrator issues the appropriate request. There will be some variations on those procedures to ensure that statistics can be collected with varying degrees of thoroughness (and with varying performance characteristics); for example, statistics may be collected on individual tables, schemas, and regions.

Automatic mode, by contrast, will attempt to refresh statistics whenever the system can reasonably detect that new statistics be helpful. In particular, whenever a region detects that it has received a significant number of successful writes, it will assume that it needs new statistics, and will submit a task for execution. However, this maintenance task will wait to be executed until the region detects a substantial decrease in it's overall write load (Should such a situation never occur, it will eventually execute anyway).

As automatic mode may choose to use resources at an inopurtune time, it will come with an associated disable call, which an administrator can use to disable automatic collection for high-load databases. It is expected that automatic mode will be a follow-up addition, while manual mode will be implemented first.

\subsection{Storage}
Once collected for a given region, Statistics must be made available for the query optimizers on \emph{all} RegionServers to use. 

The most obvious way to do this is to store the data into an HBase table. In particular, there will be two tables: 

\begin{description}
				\item[\texttt{sys\_table\_statistics}] maintains statistics for the table as a whole, including latency and other physical statistics.
				\item[\texttt{sys\_column\_statistics}] maintains statistics for each individual column
\end{description}

Each region will update specific rows in this table, making the storage format region-specific. This allows individual regions to update themselves without requiring the entire system to update.

These two tables will hold data in binary formats which are efficient to store and can be combined correctly, but will not be human readable in any direct sense. To allow human visibility into the system, we will also create two read-only views:

\begin{description}
				\item[\texttt{sys\_table\_stats}] will be a human-readable view of the table as a whole
				\item[\texttt{sys\_column\_stats}] will be a human-readable view of for each individual column
\end{description}

Only the statistics engine itself will be able to modify these views.

\subsection{Access}
There is a significant downside to this strategy that we must deal with. We note that table access is a relatively expensive operation to conduct remotely, and even more expensive when that data is stored in only a single region (which it is reasonable to expect, since a single cluster will have thousands of regions at most). However, we are helped by the realization that statistics updates will occur relatively rarely in our system\footnote{Rarely in this case can be anywhere between once a month for a reference data set to once every hour for high-volume OLTP tables}. Because of this, we can safely cache statistics data locally on each RegionServer on the cluster.

In order to avoid memory (and CPU) penalties in this cache, caching will have three levels:

\begin{description}
\item[The Local Region Cache] will hold up-to-date statistics information for every region which belongs to the \emph{local} Region Server. This cache is never stale, since statistics data is always local.
\item[The Remote Region Cache] will hold statistics information for regions which belong to \emph{remote} Region Servers.
\item[The Global Table Cache] will hold cached statistics which are \emph{global}, and contain only the statistics which occur after merging all region statistics together.
\end{description}

\subsection{Obtaining Global Statistics}
The query optimizer (and human viewers) need to be able to see statistics information which ignores region boundaries--it is only interested in seeing statistics for tables as a whole instead of specific regions. As a result, we must provide a mechanism for merging together the statistics which were generated by each region in the table; we refer to this process as \emph{merging} the statistics.

This process will scan the relevant HBase tables for all related statistics, and merge those results in memory. As this is a relatively expensive process, we will cache the completed results in the Global Table Cache. 

\section{Statistics}
\label{sec:Statistics}
The strategy that we've outlined so far has expressed itself in the singular "Statistics", but what, exactly, are these statistics that we plan on collecting, and why are they considered useful for us?

Roughly speaking, we would like to collect anything(and everything) that will give us more information about the data that is stored. We are tolerant to some degree of error in our estimates, as long as we can get accurate enough that we make the correct decisions during optimization. 

We can safely group our statistics into two distinct categories: \emph{logical} and \emph{runtime}.  

\subsection{Logical Statistics}
Logical Statistics are what we generally think of when we think of statistics; they involve information that we can use to resolve a (slighty fuzzy) view of the data without requiring reference to the data itself.

We separate our logical statistics into two distinct subcategories. On the global side, we maintain \emph{table}-level statistics, which provide us with information about the table as a whole, ignoring specific columns. For more precise information, we also maintain \emph{column}-level statistics, which provide us with information about the columns themselves.

\subsection{Table-Level Statistics}
Table-level statistics are reasonably simple to maintain and understand, as they deal with rows in the absence of additional column information. 

We maintain the following set of statistics:

\begin{enumerate}
				\item Row Count
				\item Total Size of table in bytes
				\item Mean width of a Row in bytes (including RowKey)
				\item Mean width of a RowKey in bytes
				\item Total Query Count
				\item Region Count
				\item Mean Region Size in bytes
				\item Distribution of Rows by Region as Histogram
				\item Distribution of Query Count by Region as Histogram
				\item Server Count
				\item Mean "Server Size" in bytes
				\item Distribution of Rows by Server as Histogram
				\item Distribution of Query Count by Server as Histogram
\end{enumerate}

Region and Server Counts measure the number of Regions(and the number of Servers, respectively) which are actively involed in managing this table's data. For example, if the table has ten regions scattered over 3 servers, then the Region Count is ten and the Server Count is three.

The additional fields attempt to keep track of the distribution of data on a per-region and per-server basis. This is represented easiest as a histogram distribution of data by server and region. The distribution of rows allows the load balancer and/or administrators to quickly determine which regions contain an excessively large number of rows, while the distribution of query counts will allow the load balancer and administrators to determine "hot" regions and servers--regions and servers which are involved with an excessively large volume of traffic.

As a note, it is not always strictly necessary to keep all this information stored on disk--for example, the region count can be acquired directly, without reference to any tables.

\subsection{Column Statistics}
Column Statistics manage information about individual columns, and are often the more interesting (and more difficult) algorithms to implement. While table-level metrics are easy to acquire\footnote{In fact, one could simply maintain the correct values for all table-level metrics and have a reasonable degree of accuracy}, column statistics are significantly more complex. 

To start with, we will collect the following statistics:

\begin{enumerate}
				\item Cardinality
				\item \emph{Null Fraction} = number of null elements / total number of rows in table
				\item Frequent Elements
				\item Wavelet Representation (if data type is ordered)
				\item Minimum Value (if data type is ordered)
				\item Frequency of Minimum Value
				\item Maximum value (if data type is ordered)
				\item Frequency of Maximum Value
				\item Mean size of a column (in bytes)
\end{enumerate}

These statistics will be stored in an intermediate binary format which is easy to merge together\footnote{in particular, the algorithms which generate statistics are all linear functions}.

\subsubsection{Cardinality}
The first, and simplest, metric that we wish to record is the \emph{cardinality}. Mathematically, \emph{cardinality} is just the number of entries in a set. As we are dealing with multisets(sets which can contain more than one entry with the same value), cardinality is more appropriately stated as the number of \emph{distinct} elements in the data set. 

There are a number of algorithms which can estimate the cardinality of a data set with varying degrees of accuracy. However, the most effective estimation strategy is \hyperref[sec:HyperLogLog]{\emph{HyperLogLog}}\cite{Flajolet07hyperloglog:the}. 

HyperLogLog essentially strikes a balance between memory consumption and accuracy. A more accurate estimate requires more counters,and thus more total memory. However, once a given accuracy is specified, the space requirements are \emph{constant} with respect to the number of rows processed. Additionally, updates to the data structure are constant-time. 

In practice, each counter requires only a byte of space, so the total memory space is the cost of a single object, a single array, and $2^b$ bytes. If $b=14$, this memory cost is $\approx 16$ kilobytes per column(Heule et. al's adjustments can reduce this cost for low cardinalities\cite{HyperLogLogGoogle}). Thus, over the maximum 1024 columns, the total memory footprint due to cardinality checking is $\approx 2^{b+10}$ bytes(for $b=14$, this is $\approx 16$ megabytes).

\subsubsection{Frequent Elements}
The next set of statistics to collect for SpliceMachine is referred to as the \emph{Frequent Elements}\footnote{Also referred to as the \emph{Heavy Hitters}, or the \emph{Iceberg Values}}. Frequent elements are simply the elements which occur most frequently in the data set; collecting them and their exact frequencies allows us to be more accurate for queries in which these frequent elements are involved.

There are fewer algorithms available for solving the Frequent Elements problem, but there are still a reasonably large number to parse through. However, the \hyperref[sec:SpaceSaver]{\emph{SpaceSaver}} algorithm stands out as a particularly effective choice.

The SpaceSaver algorithm keeps an approximate list of heavy hitters using constant space (controlled by the desired accuracy). It does this by keeping not just a fixed-size list of elements, but also a fixed-size list of error estimates, which it can use to eliminate errneously counted heavy hitters\footnote{We elide the details of the algorithm in this section. For those details, see Appendix-\ref{sec:SpaceSaver}}. As a result of the structural designs and a bit of clever data structure organization, SpaceSaver is able to allow constant-time updates with a bounded constant-space memory cost.

Generally speaking, if one wishes to keep the top $N$ most frequent elements, then the SpaceSaver algorithm requires $N$ fields (the item itself), plus $2N$ counters (2 for each item in the data structure. 

\subsubsection{Histograms and Wavelets}
The core of any reasonable statistics structure is the \emph{histogram}. Roughly speaking, a histogram is a summary structure which shows the distribution of ordered data across it's possible domain. Typically, it is used in data visualization tools, but when the need to make visual sense is removed, histograms are allowed to become more exotic in favor of better estimating capacities.

Technically speaking, a Histogram is a mechanism for approximation the distribution of data across its domain of possible values. Unfortunately, Histogram data structures on the whole are generally either non-linear (and therefore cannot be merged between two regions) and expensive to construct, or are highly inaccurate\footnote{in some cases, both}. As a result, if we were to use histograms, we must either sacrifice our region-specific architecture (in favor of a global sampling approach), or sacrifice any reasonable accuracy. While we can tolerate \emph{some} inaccuracy in our approximations, we cannot afford to sacrifice it all.

Fortunately, there is a third choice we can make. \emph{Wavelet transforms} are a method of approximating a function using a linear sum of other functions(analogously to Fourier Transforms, although not precisely the same). They are commonly used in signal processing applications where data is considered to be a stream of incoming values whose contribution to the whole is not predictable.

It is only a slight change to transform our way of thinking from a static \emph{set} of data points which must be described using a histogram, into one where we are a \emph{stream} of changes to a \emph{signal}, where the signal in this case is the \emph{cumulative density function} of the data itself--in other words, the signal is the histogram, and we attempt to approximate that signal using Wavelet transforms.

The elegance of Wavelet transforms are that they are linear (and therefore easy to merge\cite{GilbertWaveletHistograms}), and accurate--much more so than random sampling\cite{MatiasWaveletHistogram}. 

We construct our wavelet representation using the \emph{Haar Wavelet Transform}. In effect, this transform maintains a tree of counters, and each update to that tree updates a subset of those total counters. At each level $0\leq l < \log{N}$ of the tree, there are $2^l$ counters. The exact approach therefore uses a total of $N$ counters, which is prohibitive for large domain sizes (like integers and larger). 

Above a certain level, we switch from an exact approach to one based on the \hyperref[sec:GroupedCountSketch]{\emph{GroupedCountSketch}} data structure. The GroupedCount Sketch uses a fixed amount of memory to store all counters at a specific level, but sacrifices some correctness to do so. This tradeoff is more than acceptable in light of the memory savings gained. This hybrid approach allows us to control the total memory consumed for each histogram on a per-column basis.

Fundamentally, the Haar Transform considers signals by mapping the domain of possible values into the domain of integers--as a result, we must have a way of representing arbitrary byte sequences (such as strings) as scalar values which does not change the sort order. Thankfully, SpliceMachine uses sorted byte array representations of all data types, which means that we can represent an arbitrary sequence of byte arrays as a smaller, arbitrary sequence of 8-byte scalars. Each of these scalars can be treated as a fixed \emph{dimension}, to which we use a multi-dimensional Haar transform (essentially a multi-dimensional tree) to maintain the structure.

Finally, once the transform is complete, we can note that most of the counters we kept will be very close to 0 (i.e. they will have very little impact on the final result). As a result, we can maintain a very accurate representation of the distribution using only a few of the final \emph{Haar coefficients}.


\subsection{Physical Statistics}
In addition to logical statistics, we must collect some basic information about the physical world in which we are operating. Because we are durable, a large component of our cost is the cost of reading data off disk; because we are a clustered environment, the second largest power is the cost to write and read data over the network. Thus, we will need to collect disk and network I/O latency. 

In other systems (notably Oracle\cite{Oracle}), information about hardware is collected at startup; this is a good approach for collecting localized metrics that cannot change (such as number of disks, etc.), but is not applicable to all metrics. Thus, we have two categories: \emph{fixed} and \emph{variable} measurements.

Fixed measurements consist of:

\begin{enumerate}
				\item Number of CPUs
				\item Max Heap Size
				\item Max Block Cache Size
				\item Number of IPC threads
\end{enumerate}

while variable measurements are:
\begin{enumerate}
				\item Local Read Latency
				\item Remote Read Latency
				\item Write network latency (to TEMP)
\end{enumerate}

Fixed measurements are collected once during startup and recorded, while variable measurements need to be periodically refreshed. 

The initial way of collecting variable measurements is to use the logical statistics gathering phase. During the statistics collection phase, take a uniform sample of rows from the table, and write those rows to TEMP. If the table being measured is an index table, use the same sample to perform a simple index lookup on the main table, which will provide index lookup and write latency measures. Local read measures can be acquired merely by recording the read performance of the statistics gathering process.

Remote read latency is more difficult to measure. Initially, it will be recorded during some transaction lookups, although ultimately a more sophisticated method may be used.

When automatic collection is enabled, a good way of obtaining these measurements is similar to that of PostgreSQL\cite{PGCollector} measures real time performance, and is referred to as \emph{query sampling}. When query sampling is enabled, a random sample of queries is taken\footnote{The sampling logic may be selective--for example, it may only only randomly sample from those queries which are expected to take a very short period of time, so that the added cost is not excessive.}. When a query is chosen, it will record the latency measurements that occurred during the execution of that query. This would allow more accurate variable measurements, but would have an adverse impact on performance for selected queries, so it would also require a shut-off valve to disable it when performance is critical.

\section{Collection}
\label{sec:Collection}
Now that we have a clear image of what, precisely, we are collecting within our statistics engine, it behooves us to talk about the collection mechanism itself.

There will ultimately be two modes of collections: \emph{manual} and \emph{automatic}. Manual mode will be implemented first, as it is simpler to implement and test, while automatic mode will be initiated afterwards.

\subsection{Manual Collection Mode}
Manual collection mode consists of a set of stored procedures, which will be used to initiate statistics gathering in a variety of different ways.

First, we will allow granularity of gathering, so that an administrator is not required to initiate collection for the entire database every time. In particular, we will have the following stored procedures:

\begin{description}
				\item[\texttt{COLLECT\_ALL\_STATISTICS()}] Will collect statistics for every table and index in the database. This will likely be excessively expensive.
				\item[\texttt{COLLECT\_SCHEMA\_STATISTICS('SCHEMA')}] Will collect statistics for every table and index in the specified \texttt{SCHEMA}. 
				\item[\texttt{COLLECT\_TABLE\_STATISTICS('SCHEMA','TABLE')}] Will collect statistics for the specified table or index.
				\item[\texttt{COLLECT\_COLUMN\_STATISTICS('SCHEMA','TABLE','COLUMNS')}] Will collect statistics for one or more specified columns in the particular table.
								\item[\texttt{COLLECT\_REGION\_STATISTICS('SCHEMA','TABLE','start key','stop key')}] For primary keyed tables and indices, this will collect statistics for a specified region of a table. This is useful for forcing statistics collection after a manual split, for example.
\end{description}
This will allow administrators some control over the resource usage of the collection tasks. Note, however, that collection always occurs within a maintenance task frame, so the number of available tasks for execution are limited.

\subsection{Automatic Collection Mode}
Ultimately, an automatic collection mode will be an effective tool. When automatic collection is enabled, an individual region will keep watch on how many mutations are received. When a threshold of mutations has been reached, it will schedule a new gathering task for itself, to be executed \emph{after} the rate of mutations received has decreased to a smaller volume. This way, individual regions can proactively adjust their statistics information without interfering excessively with ongoing writes. 

These tasks will be queued in such a way as to guarantee that

\begin{enumerate}
				\item No Region may collect statistics more than a fixed number of times in a given period of time.
				\item No more than one statistics task for a region can be queued at a time--others tasks are merged in to the process.
\end{enumerate}

\subsection{Transactions}
The question arises as to what isolation level the Statistics engine should use during collection tasks. Each level exposes us to a different type of potential error. 

If we choose read uncommitted, then we will see all active and committed records, and thus we will collect statistics in the most optimistic sense (that active transactions will eventually become committed). However, if a large transaction were to be rolled back, then we would incorrectly include those rows in the statistics view, leading us to believe that the table is larger than it truly is.

If we choose read committed or repeatable reads, then we will not see transactions that are currently active, only rows which belong to committed transactions. Thus, if we collected statistics during a large insert or import, we would not have those records, leading us to incorrectly assume that the table is smaller than it truly is.

Our choice is made a bit simpler when we consider how transactional structures work in SpliceMachine. In order for a given row to be filtered transactionally, it must first be read off disk--thus, the most expensive component in the transactional process must occur no matter what. This means that every operation will functionally read every row that was durably written, even if that row belongs to a transaction which has been rolled back. Therefore, we would like to include that read cost in our statistical model, which implies that we want to include rows at any possible stage. However, we would \emph{not} want to include rolled back rows, as they could potentially destroy the predictive abilities of our intermediate estimates. 

Thus, after consideration, it seems clear that we should use a read uncommitted isolation level when collecting statistics.

\section{Storage}
\label{sec:Storage}
We would like our statistics to be distributed in a straightforward and durable manner. The simplest way to do this is using an HBase table for storage. Then, when statistics are needed, they can simply be read.

In order to support the region-level isolation that our statistics engine requires, we will need to also separate writes out by individual regions. 

\subsection{Table Statistics}
Table statistics are stored in the \texttt{sys\_table\_statistics} table, whose schema is as in Table-\ref{table:tableStatistics}; it has an effective primary key of (\texttt{conglom\_id},\texttt{region\_id}).

\begin{table}
				\begin{tabular}{|l|c|p{6cm}|}
								\hline
								\bf{Name}									& \bf{Type}	&	\bf{Description} \\ \hline	
								\texttt{conglom\_id}			&	bigint	&	The conglomerate id for the table \\ \hline
								\texttt{region\_id}				&	varchar	&	the region identifier \\ \hline
								\texttt{num\_rows}				&	bigint	& RowCount \\ \hline
								\texttt{region\_size}			&	bigint	&	Total size of region (in bytes) \\ \hline
								\texttt{mean\_row\_width}	&	integer	&	mean row width (in bytes) \\ \hline
								\texttt{mean\_key\_width}	&	integer	&	mean key width (in bytes) \\ \hline
								\texttt{query\_count}			&	bigint	&	Total number of queries issued \\ \hline
				\end{tabular}
				\caption{\texttt{sys\_table\_statistics} table schema}
				\label{table:tableStatistics}
\end{table}

Note that some table-level statistics (such as server information) are not stored. This is because the location of regions may change frequently, and it is relatively efficient for the system to collect that data on demand.

It is important to note that \texttt{sys\_table\_statistics} has "virtual" columns--columns which can be considered part of the table, but which are not stored directly; this approach makes it difficult for us to provide direct user access through SQL, which is a very powerful visibility feature. In order to allow administrators such direct SQL access, we expose a read-only view call \texttt{sys\_table\_stats}. The full schema for \texttt{sys\_table\_stats} can be found in Table-\ref{table:tableStats}.

\begin{table}
				\begin{tabular}{|l|c|p{6cm}|}
								\hline
								\bf{Name}														& \bf{Type}	&	\bf{Description} \\ \hline	
								\texttt{conglom\_id}								&	bigint		&	The conglomerate id for the table \\ \hline
								\texttt{num\_rows}									&	bigint		&	The number of rows in the table \\ \hline
								\texttt{mean\_row\_width}						&	integer		&	The mean width of a row (in bytes) 	\\ \hline
								\texttt{mean\_key\_width}						&	integer		&	The mean width of a key (in bytes) \\ \hline
								\texttt{mean\_region\_size}					&	bigint		&	The mean size of the region (in bytes) \\ \hline
								\texttt{min\_region\_size}					&	bigint		&	The minimum region size (in bytes) \\ \hline
								\texttt{max\_region\_size}					&	bigint		&	The maximum region size (in bytes) \\ \hline
								\texttt{num\_regions}								&	integer		&	Region count	\\	\hline
								\texttt{mean\_region\_query\_count}	&	bigint		&	The mean number of queries per region \\ \hline
								\texttt{max\_region\_query\_count}	&	bigint		&	The max number of queries per region \\ \hline
								\texttt{min\_region\_query\_count}	&	bigint		&	The min number of queries per region \\ \hline
								\texttt{num\_servers}								&	integer		&	Server count \\ \hline
								\texttt{mean\_server\_size}					&	bigint		&	The mean size per server (in bytes) \\ \hline
								\texttt{min\_server\_size}					&	bigint		&	The minimum size per server (in bytes) \\ \hline
								\texttt{max\_server\_size}					&	bigint		&	The maximum size per server (in bytes) \\ \hline
								\texttt{mean\_server\_query\_count}	&	bigint		&	The mean number of queries per server  \\ \hline
								\texttt{max\_server\_query\_count}	&	bigint		&	The max number of queries per server \\ \hline
								\texttt{min\_server\_query\_count}	&	bigint		&	The min number of queries per server \\ \hline
				\end{tabular}
				\caption{\texttt{sys\_table\_stats} table schema}
				\label{table:tableStats}
\end{table}
\subsection{Column Statistics}
Column Statistics are stored in the \texttt{sys\_column\_statistics} table, which is an internal-only table containing binary representations of column statistics values\footnote{Note that in many cases we store an intermediate binary representation which makes it easy to merge with other regions, but is often not human-readable}. The table has a schema as described in Table-\ref{table:columnStatistics}, and has an effective primary key of 
\linebreak(\texttt{conglomerate\_id},\texttt{column\_num},\texttt{region\_id}).

\begin{table}
				\begin{tabular}{|l|c|p{6cm}|}
								\hline
								\bf{Name}									& \bf{Type}	&	\bf{Description} \\ \hline	
								\texttt{conglomerate\_id}	&	bigint		&	The conglomerate id for the table \\ \hline
								\texttt{region\_id}				&	varchar		&	The region identifier \\ \hline
								\texttt{column\_num}			&	smallint	& The column number \\ \hline
								\texttt{cardinality}			&	binary		&	cardinality estimate \\ \hline
								\texttt{null\_frac}				&	real			&	null fraction \\ \hline
								\texttt{freq\_vals}				&	binary		&	encoded list of most frequent values \\ \hline
								\texttt{freq\_freqs}			&	binary		&	encoded list of frequencies for most common values \\ \hline
								\texttt{distribution}			&	binary		&	binary encoded representation of the distribution \\ \hline
								\texttt{min\_val}					&	column type	&	the minimum column value	\\	\hline
								\texttt{freq\_min\_val}		&	column type	&	The frequency of the minimum column value \\ \hline
								\texttt{max\_val}					&	column type	&	the maximum column value	\\	\hline
								\texttt{freq\_max\_val}		&	column type	&	The frequency of the maximum column value \\ \hline
				\end{tabular}
				\caption{\texttt{sys\_column\_statistics} table schema}
				\label{table:columnStatistics}
\end{table}

In order to expose the binary structures in the column stats, we expose a read-only view \linebreak\texttt{sys\_column\_stats} with the schema as in Table-\ref{table:columnStats}. This view is intended for user exploration and visibility more than for functional usability within the system.

\begin{table}
				\begin{tabular}{|l|c|p{6cm}|}
								\hline
								\bf{Name}									& \bf{Type}	&	\bf{Description} \\ \hline	
								\texttt{conglomerate\_id}	&	bigint		&	The conglomerate id for the table \\ \hline
								\texttt{column\_num}			&	smallint	&	The column number \\ \hline
								\texttt{cardinality}			&	bigint		&	The cardinality estimate \\ \hline
								\texttt{null\_frac}				&	real			& Null fraction \\ \hline
								\texttt{freq\_vals}				&	varchar		&	comma-separated list of frequent values, and their frequencies \\ \hline	
								\texttt{distribution}			&	varchar		&	functional representation of the distribution \\ \hline
								\texttt{min\_val}					&	column type	&	the minimum column value	\\	\hline
								\texttt{freq\_min\_val}		&	column type	&	The frequency of the minimum column value \\ \hline
								\texttt{max\_val}					&	column type	&	the maximum column value	\\	\hline
								\texttt{freq\_max\_val}		&	column type	&	The frequency of the maximum column value \\ \hline
				\end{tabular}
				\caption{\texttt{sys\_column\_stats} table schema}
				\label{table:columnStats}
\end{table}

Note that we may collect statistics for any column, but order statistics (min, max, and distribution) will only be collected and stored for columns which are considered orderable. For example, varchars will keep order statistics, but blobs columns will not. For such columns, order statistics will be \texttt{null}.

\subsection{Internal Caching of Logical Statistics}
Because statistics are rarely changed\footnote{"rarely", in this case, can mean anywhere between months and minutes in between changes,depending on configuration}, and because stale statistics are not usually a significant problem for optimization tools, it is possible to maintain a \emph{Statistics Cache} on each node, which can speed up access to logical statistics in useful ways. This cache will have three levels:

\begin{enumerate}
				\item Local Regions Cache
				\item Remote Regions Cache
				\item Table Cache
\end{enumerate}

All caches except for the Local Regions Cache will be periodically refreshed with a background table scan, which will fetch the latest information for that specific entry. 

\subsubsection{Local Regions Cache}
Local regions will hold their statistics in the Local Region cache, and every time statistics are updated, they are also written to the Local Regions Cache. As a result, this cache is never stale. When accessing statistics for those particular regions, the Local Regions cache is always consulted, thus avoiding any table reads. This is particularly important for use-cases such as dynamic buffer sizing, which will want the most up-to-date statistics information, but only for a specific region.

\subsubsection{Remote Regions Cache}
On the other hand, when a region is not accessible locally, but region-level statistics are still desired, the \emph{Remote Regions Cache} will be consulted. This will maintain a small cache of remote Region statistics, which can be used to avoid a table access during remote region statistics gathering.

As the usefulness of remote regions is not generally significant, the Remote Regions Cache is likely to be very small, and will evict rapidly.

\subsubsection{Table Cache}
When statistics for the entire table are desired in a global, merged sense, a \emph{Table Cache} will be maintained, which will allow the caller to quickly access global statistics for a table without requiring an expensive scan and merge process. This cache will be the primary source of statistics for use in global systems like the query optimizer, and as such will need to be relatively large, in order to maintain an effective set of table statistics for efficient access.

However, the Table Cache will not be useful for region-specific statistics access, as it will not maintain any region-specific information. For that purpose, one will use the Local or Remote Regions Cache.

\subsection{Physical Statistics}
Because Physical statistics are server-specific rather than data-specific, they are expected to change more rapidly, and the value of storing them for a long time is less obvious. Additionally, we can generally assume that all servers in a single cluster have close to the same level of hardware (and thus have close to the same physical statistics). As a result, we can store only a subset of all needed statistics in an in-memory view, and refresh that view periodically. 

When a refresh is desired, we do not need to contact all region servers. Instead, we will contact a random subset of servers for their latest physical statistics information. When physical statistics are desired, we then use the subset as a representative sample of the whole\footnote{e.g. it will use mean values to represent specific region servers}.

However, physical statistics are useful for a large number of monitoring tools, so it will be very useful to expose a read-only view of this data, called \texttt{sys\_physical\_stats}. This view has a schema described in Table-\ref{table:physicalStats}.

\begin{table}
				\begin{tabular}{|l|c|p{6cm}|}
								\hline
								\bf{Name}													& \bf{Type}	&	\bf{Description} \\ \hline	
								\texttt{server\_ip}								&	varchar		&	IP address of server \\ \hline
								\texttt{hostname}									&	varchar		&	Hostname of server \\ \hline
								\texttt{num\_cpus}								&	integer		& Number of CPUs \\ \hline
								\texttt{max\_heap\_size}					&	integer		&	Max heap size (in bytes)\\ \hline
								\texttt{max\_block\_cache\_size}	&	integer		&	Max block cache size (in bytes) \\ \hline
								\texttt{num\_ipc\_threads}				&	integer		&	Number of IPC threads \\ \hline
								\texttt{local\_read\_latency}			&	bigint		&	Local read latency (ns) \\ \hline
								\texttt{remote\_read\_latency}		&	bigint		& Remote read latency (ns) \\ \hline
								\texttt{write\_latency}						&	bigint		&	Remote write latency (ns) \\ \hline
				\end{tabular}
				\caption{\texttt{sys\_physical\_stats} table schema}
				\label{table:physicalStats}
\end{table}

